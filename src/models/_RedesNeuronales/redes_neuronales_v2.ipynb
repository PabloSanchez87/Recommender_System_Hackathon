{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo - fraction = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librer√≠as necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import psutil\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from torch.utils.data import get_worker_info\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "from torch.amp import GradScaler, autocast\n",
    "import json\n",
    "\n",
    "# Configuraci√≥n global de caracter√≠sticas\n",
    "FEATURE_COLS = [\"time_since_last\", \"session_relative_position\", \"session_duration\", \n",
    "                \"R\", \"F\", \"M\", \"device_type\", \"pagetype\", \"discount\", \"cod_section\", \"family\"]\n",
    "FEATURE_DIM = len(FEATURE_COLS)\n",
    "\n",
    "# ‚úÖ 1Ô∏è‚É£ Configuraci√≥n del Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# üî• Funci√≥n para Monitoreo de Memoria\n",
    "def print_memory_usage(epoch=None):\n",
    "    process = psutil.Process(os.getpid()) \n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"üñ•Ô∏è RAM Usage: {mem_info.rss / 1e9:.2f} GB\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üî• GPU Usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"üî• GPU Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    if epoch is not None:\n",
    "        print(f\"üìä Memoria despu√©s del Epoch {epoch}\")\n",
    "\n",
    "# ‚úÖ Implementaci√≥n de Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, pos_weight=self.pos_weight, reduction='none'\n",
    "        )\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        pt = targets * probas + (1 - targets) * (1 - probas)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        loss = self.alpha * focal_term * bce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "# ‚úÖ 2Ô∏è‚É£ Definir Modelo Ajustado con Dropout y Regularizaci√≥n\n",
    "class GRURecommender(nn.Module):\n",
    "    def __init__(self, input_dim=50, feature_dim=FEATURE_DIM, hidden_dim=128, num_layers=2, output_dim=1):\n",
    "        super(GRURecommender, self).__init__()\n",
    "        # Ajustamos el GRU para aceptar la nueva dimensi√≥n de caracter√≠sticas\n",
    "        self.gru = nn.GRU(input_dim + feature_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "        for name, param in self.gru.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, product_embeddings, session_features):\n",
    "        # Normalizar las caracter√≠sticas de entrada\n",
    "        product_embeddings = (product_embeddings - product_embeddings.mean(dim=1, keepdim=True)) / \\\n",
    "                             (product_embeddings.std(dim=1, keepdim=True) + 1e-6)\n",
    "        session_features = (session_features - session_features.mean(dim=1, keepdim=True)) / \\\n",
    "                           (session_features.std(dim=1, keepdim=True) + 1e-6)\n",
    "        x = torch.cat((product_embeddings, session_features), dim=-1)\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "model = GRURecommender().to(device)\n",
    "print(f\"Modelo en: {next(model.parameters()).device}\")\n",
    "\n",
    "# ‚úÖ 3Ô∏è‚É£ Definir P√©rdida y Optimizador con Focal Loss y regularizaci√≥n\n",
    "pos_weight_value = 5.0\n",
    "pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32, device=device)\n",
    "print(f\"Pos_weight calculado: {pos_weight.item()}\")\n",
    "\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0, pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "scaler = torch.GradScaler(\"cuda\")\n",
    "\n",
    "print(f\"Criterion: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "\n",
    "# ‚úÖ 4Ô∏è‚É£ Definir Funciones de Evaluaci√≥n con M√©tricas Adicionales y NDCG\n",
    "def compute_ndcg(actuals, predictions, k=5):\n",
    "    if len(actuals) == 0:\n",
    "        return 0.0\n",
    "    indices = np.argsort(-predictions)[:k]\n",
    "    sorted_rels = actuals[indices]\n",
    "    dcg = sum((2**rel - 1) / np.log2(i+2) for i, rel in enumerate(sorted_rels))\n",
    "    ideal_sorted_rels = np.sort(actuals)[::-1][:k]\n",
    "    idcg = sum((2**rel - 1) / np.log2(i+2) for i, rel in enumerate(ideal_sorted_rels))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())\n",
    "            all_outputs.extend(outputs.cpu().numpy().flatten())\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_outputs = np.array(all_outputs)\n",
    "    try:\n",
    "        auc_roc = roc_auc_score(all_targets, all_outputs)\n",
    "    except ValueError:\n",
    "        auc_roc = float('nan')\n",
    "    average_precision = average_precision_score(all_targets, all_outputs)\n",
    "    y_pred = (all_outputs >= threshold).astype(int)\n",
    "    precision = precision_score(all_targets, y_pred, zero_division=0)\n",
    "    recall = recall_score(all_targets, y_pred, zero_division=0)\n",
    "    f1 = f1_score(all_targets, y_pred, zero_division=0)\n",
    "    mapk_score = calculate_mapk(all_targets, all_outputs, k=5)\n",
    "    ndcg = compute_ndcg(all_targets, all_outputs, k=5)\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, NDCG@5: {ndcg:.4f}\")\n",
    "    return auc_roc, average_precision, mapk_score, f1\n",
    "\n",
    "def calculate_mapk(actuals, predictions, k=5):\n",
    "    indices = np.argsort(-predictions)\n",
    "    actuals_sorted = actuals[indices]\n",
    "    top_k = actuals_sorted[:k]\n",
    "    return np.mean(top_k)\n",
    "\n",
    "# ‚úÖ 5Ô∏è‚É£ Funci√≥n de Entrenamiento con Persistencia del Mejor Modelo\n",
    "def train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, epochs):\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        print_memory_usage(epoch=\"Inicio\")\n",
    "        print(\"Iniciando el bucle de batches...\")\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(product_embeddings, session_features)\n",
    "                loss = criterion(outputs, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            grads_finite = True\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                        grads_finite = False\n",
    "                        print(\"NaN o Inf encontrado en los gradientes. Saltando la actualizaci√≥n.\")\n",
    "                        break\n",
    "            if grads_finite:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                scaler.update()\n",
    "                continue\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "        print_memory_usage(epoch=epoch+1)\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        auc_roc, avg_precision, mapk_score, f1 = evaluate(model, val_loader)\n",
    "        print(f\"Validation AUC-ROC: {auc_roc:.4f}, Average Precision: {avg_precision:.4f}, MAP@5: {mapk_score:.4f}\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            model_filename = f\"best_model_epoch_{epoch+1}_f1_{f1:.4f}.pt\"\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(f\"Nuevo mejor modelo guardado: {model_filename}\")\n",
    "    return model_filename\n",
    "\n",
    "# ‚úÖ 6Ô∏è‚É£ Cargar Dataset con Normalizaci√≥n y Manejo de Outliers\n",
    "MAX_SEQ_LENGTH = 50\n",
    "\n",
    "class IterableSessionDataset(IterableDataset):\n",
    "    def __init__(self, df_path, fraction=1.0, mode='train'):\n",
    "        self.df_path = df_path\n",
    "        self.feature_cols = FEATURE_COLS  # Usamos las caracter√≠sticas ampliadas\n",
    "        self.fraction = fraction\n",
    "        self.mode = mode\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            return self._data_iterator()\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            return self._data_iterator(worker_id, num_workers)\n",
    "\n",
    "    def _data_iterator(self, worker_id=0, num_workers=1):\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "        num_row_groups_train = int(num_row_groups_to_use * 0.8)\n",
    "        num_row_groups_val = num_row_groups_to_use - num_row_groups_train\n",
    "        if self.mode == 'train':\n",
    "            start_idx = 0\n",
    "            end_idx = num_row_groups_train\n",
    "        elif self.mode == 'val':\n",
    "            start_idx = num_row_groups_train\n",
    "            end_idx = num_row_groups_train + num_row_groups_val\n",
    "        elif self.mode == 'test':\n",
    "            start_idx = 0\n",
    "            end_idx = num_row_groups_to_use\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode: must be 'train', 'val', or 'test'.\")\n",
    "        batches_per_worker = (end_idx - start_idx + num_workers - 1) // num_workers if num_workers > 0 else end_idx - start_idx\n",
    "        start = start_idx + worker_id * batches_per_worker\n",
    "        end = min(start + batches_per_worker, end_idx)\n",
    "        for i in range(start, end):\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "            # Preprocesamiento\n",
    "            for col in ['time_since_last', 'session_duration']:\n",
    "                if col in df.columns:\n",
    "                    df[col] = np.log1p(df[col])\n",
    "                else:\n",
    "                    df[col] = 0.0\n",
    "            for col in self.feature_cols:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = 0.0\n",
    "            df[self.feature_cols] = (df[self.feature_cols] - df[self.feature_cols].mean()) / (df[self.feature_cols].std() + 1e-6)\n",
    "            df_iter = df.groupby(\"session_id\")\n",
    "            for session_id, session_data in df_iter:\n",
    "                product_embeddings = session_data[\"embedding_reduced\"].tolist()\n",
    "                partnumbers = session_data[\"partnumber\"].tolist()\n",
    "                if any(embedding is None or np.isnan(embedding).any() for embedding in product_embeddings):\n",
    "                    continue\n",
    "                product_embeddings = torch.tensor(np.array(product_embeddings), dtype=torch.float32)\n",
    "                session_features = torch.tensor(session_data[self.feature_cols].values, dtype=torch.float32)\n",
    "                if self.mode in ['train', 'val']:\n",
    "                    targets = torch.tensor(session_data[\"add_to_cart\"].values, dtype=torch.float32)\n",
    "                    targets = torch.clamp(targets, min=0.0, max=1.0)\n",
    "                    yield product_embeddings, session_features, targets, session_id, partnumbers\n",
    "                elif self.mode == 'test':\n",
    "                    targets = torch.zeros(product_embeddings.shape[0], dtype=torch.float32)\n",
    "                    yield product_embeddings, session_features, targets, session_id, partnumbers\n",
    "\n",
    "# ‚úÖ Muestreo ponderado en la funci√≥n de colaci√≥n\n",
    "def collate_fn(batch):\n",
    "    if len(batch) == 0:\n",
    "        return (torch.zeros(1, MAX_SEQ_LENGTH, 50 + FEATURE_DIM),\n",
    "                torch.zeros(1, MAX_SEQ_LENGTH, FEATURE_DIM),\n",
    "                torch.zeros(1, MAX_SEQ_LENGTH), [0], [0])\n",
    "    mode = 'train' if batch[0][2].sum().item() > 0 else 'test'\n",
    "    if mode == 'train':\n",
    "        positive_samples = []\n",
    "        negative_samples = []\n",
    "        for sample in batch:\n",
    "            if sample[2].sum().item() > 0:\n",
    "                positive_samples.append(sample)\n",
    "            else:\n",
    "                negative_samples.append(sample)\n",
    "        oversample_factor = 2\n",
    "        oversampled_positive = positive_samples * oversample_factor\n",
    "        oversampled_batch = negative_samples + oversampled_positive\n",
    "        np.random.shuffle(oversampled_batch)\n",
    "    else:\n",
    "        oversampled_batch = batch\n",
    "    product_embeddings, session_features, targets, session_ids, partnumbers_list = zip(*oversampled_batch)\n",
    "    max_len = min(max([x.shape[0] for x in product_embeddings]), MAX_SEQ_LENGTH)\n",
    "    padded_embeddings = torch.zeros((len(oversampled_batch), max_len, product_embeddings[0].shape[1]))\n",
    "    padded_features = torch.zeros((len(oversampled_batch), max_len, session_features[0].shape[1]))\n",
    "    padded_targets = torch.zeros((len(oversampled_batch), max_len))\n",
    "    for i in range(len(oversampled_batch)):\n",
    "        seq_len = min(product_embeddings[i].shape[0], max_len)\n",
    "        padded_embeddings[i, :seq_len] = product_embeddings[i][:seq_len]\n",
    "        padded_features[i, :seq_len] = session_features[i][:seq_len]\n",
    "        padded_targets[i, :seq_len] = targets[i][:seq_len]\n",
    "    return padded_embeddings, padded_features, padded_targets, session_ids, partnumbers_list\n",
    "\n",
    "# ‚úÖ 7Ô∏è‚É£ Crear Datasets de Entrenamiento y Validaci√≥n\n",
    "data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "fraction = 1\n",
    "train_dataset = IterableSessionDataset(data_path, fraction=fraction, mode='train')\n",
    "val_dataset = IterableSessionDataset(data_path, fraction=fraction, mode='val')\n",
    "\n",
    "# ‚úÖ 8Ô∏è‚É£ Ejecutar Entrenamiento\n",
    "EPOCHS = 10\n",
    "print(f'--- N√∫mero de epochs: {EPOCHS} ---')\n",
    "best_model_file = train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, EPOCHS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'submission.json' generado con √©xito.\n"
     ]
    }
   ],
   "source": [
    "# üì§ 9Ô∏è‚É£ Inferencia y Generaci√≥n de Predicciones\n",
    "def inference(model, test_dataset, device, top_k=5):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    recommendations = {}\n",
    "    global_top_recommendations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            product_embeddings, session_features, _, session_ids, partnumbers_list = batch\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            for sid, parts, score_seq in zip(session_ids, partnumbers_list, scores):\n",
    "                product_scores = list(zip(parts, score_seq))\n",
    "                product_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                top_recommendations = [int(p) for p, s in product_scores[:top_k]]\n",
    "                top_recommendations = list(dict.fromkeys(top_recommendations))\n",
    "                if len(top_recommendations) < top_k:\n",
    "                    global_top_recommendations.extend([p for p in parts if p not in top_recommendations])\n",
    "                recommendations[int(sid)] = top_recommendations\n",
    "\n",
    "    product_counts = {}\n",
    "    for p in global_top_recommendations:\n",
    "        product_counts[p] = product_counts.get(p, 0) + 1\n",
    "    sorted_global_top = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    global_top = [p for p, count in sorted_global_top]\n",
    "\n",
    "    for sid, recs in recommendations.items():\n",
    "        if len(recs) < top_k:\n",
    "            additional = [p for p in global_top if p not in recs]\n",
    "            recs.extend(additional[:top_k - len(recs)])\n",
    "            recommendations[sid] = recs\n",
    "\n",
    "    output_json = {\n",
    "        \"target\": recommendations\n",
    "    }\n",
    "\n",
    "    with open(\"submission.json\", \"w\") as f:\n",
    "        json.dump(output_json, f, indent=4)\n",
    "\n",
    "    print(\"Archivo 'submission.json' generado con √©xito.\")\n",
    "\n",
    "# Cargar el mejor modelo entrenado\n",
    "model.load_state_dict(torch.load(best_model_file, weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Preparar el dataset de prueba\n",
    "test_data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/test_full.parquet\"\n",
    "test_dataset = IterableSessionDataset(test_data_path, fraction=1.0, mode='test')\n",
    "\n",
    "# Ejecutar la inferencia\n",
    "inference(model, test_dataset, device, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√°s personalizaci√≥n en la inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_product_attributes(file_path):\n",
    "    # Carga los datos de productos desde un archivo (ajusta seg√∫n tu formato)\n",
    "    df = pd.read_parquet(file_path)  # o pd.read_parquet(file_path) si es Parquet\n",
    "    \n",
    "    # Inicializa el diccionario\n",
    "    product_attributes = {}\n",
    "    \n",
    "    # Recorre cada fila del DataFrame para llenar el diccionario\n",
    "    for _, row in df.iterrows():\n",
    "        product_attributes[row['partnumber']] = {\n",
    "            \"cod_section\": row[\"cod_section\"],\n",
    "            \"family\": row[\"family\"],\n",
    "            \"discount\": row[\"discount\"],\n",
    "            # Agrega m√°s atributos si es necesario\n",
    "        }\n",
    "    \n",
    "    return product_attributes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'submission_v2.json' generado con √©xito.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Supongamos que 'product_attributes' ya est√° cargado y disponible globalmente.\n",
    "# Por ejemplo:\n",
    "# product_attributes = load_product_attributes(\"path_to_products_file\")\n",
    "\n",
    "# Cargar el diccionario usando la funci√≥n\n",
    "product_attributes = load_product_attributes(\"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/optimized_products_transformed.parquet\")\n",
    "\n",
    "\n",
    "def inference_v2(model, test_dataset, device, top_k=5):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    recommendations = {}\n",
    "    global_top_recommendations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            product_embeddings, session_features, _, session_ids, partnumbers_list = batch\n",
    "\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "            for sid, parts, score_seq in zip(session_ids, partnumbers_list, scores):\n",
    "                product_scores = list(zip(parts, score_seq))\n",
    "                product_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                top_recommendations = [int(p) for p, s in product_scores[:top_k]]\n",
    "                top_recommendations = list(dict.fromkeys(top_recommendations))\n",
    "\n",
    "                # Recopilar productos similares basados en atributos\n",
    "                if len(top_recommendations) < top_k:\n",
    "                    # Obtener atributos de los productos ya recomendados\n",
    "                    similar_candidates = []\n",
    "                    for rec in top_recommendations:\n",
    "                        rec_attrs = product_attributes.get(rec, {})\n",
    "                        # Buscar otros productos en la misma sesi√≥n con atributos similares\n",
    "                        for p in parts:\n",
    "                            p = int(p)\n",
    "                            if p in top_recommendations:\n",
    "                                continue\n",
    "                            p_attrs = product_attributes.get(p, {})\n",
    "                            # Comparar atributos (por ejemplo, cod_section y family)\n",
    "                            if (p_attrs.get(\"cod_section\") == rec_attrs.get(\"cod_section\") and\n",
    "                                p_attrs.get(\"family\") == rec_attrs.get(\"family\")):\n",
    "                                similar_candidates.append(p)\n",
    "                    # A√±adir candidatos similares si es necesario\n",
    "                    for candidate in similar_candidates:\n",
    "                        if candidate not in top_recommendations and len(top_recommendations) < top_k:\n",
    "                            top_recommendations.append(candidate)\n",
    "\n",
    "                    # Acumular productos para fallback global si a√∫n faltan recomendaciones\n",
    "                    if len(top_recommendations) < top_k:\n",
    "                        global_top_recommendations.extend(\n",
    "                            [p for p in parts if p not in top_recommendations]\n",
    "                        )\n",
    "\n",
    "                recommendations[int(sid)] = top_recommendations\n",
    "\n",
    "    # Fallback global como en la versi√≥n anterior\n",
    "    product_counts = {}\n",
    "    for p in global_top_recommendations:\n",
    "        product_counts[p] = product_counts.get(p, 0) + 1\n",
    "    sorted_global_top = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    global_top = [p for p, count in sorted_global_top]\n",
    "\n",
    "    for sid, recs in recommendations.items():\n",
    "        if len(recs) < top_k:\n",
    "            additional = [p for p in global_top if p not in recs]\n",
    "            recs.extend(additional[:top_k - len(recs)])\n",
    "            recommendations[sid] = recs\n",
    "\n",
    "    output_json = {\n",
    "        \"target\": recommendations\n",
    "    }\n",
    "\n",
    "    with open(\"submission_v2.json\", \"w\") as f:\n",
    "        json.dump(output_json, f, indent=4)\n",
    "\n",
    "    print(\"Archivo 'submission_v2.json' generado con √©xito.\")\n",
    "\n",
    "# Cargar el mejor modelo entrenado\n",
    "model.load_state_dict(torch.load(best_model_file, weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Preparar el dataset de prueba\n",
    "test_data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/test_full.parquet\"\n",
    "test_dataset = IterableSessionDataset(test_data_path, fraction=1.0, mode='test')\n",
    "\n",
    "# Ejecutar la inferencia\n",
    "inference_v2(model, test_dataset, device, top_k=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
