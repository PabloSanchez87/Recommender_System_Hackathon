{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir el Conjunto de Datos Basado en Usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "# df = pd.read_parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2766405\n",
      "44047154\n",
      "15.922163963700182\n"
     ]
    }
   ],
   "source": [
    "# num_pos = df['add_to_cart'].sum()\n",
    "# num_neg = len(df) - num_pos\n",
    "\n",
    "# print(num_pos)\n",
    "# print(num_neg)\n",
    "\n",
    "# pos_weight_value = num_neg / num_pos\n",
    "# print(pos_weight_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto completo\n",
    "import pyarrow.parquet as pq\n",
    "data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "\n",
    "table = pq.ParquetFile(data_path)\n",
    "dfs = [table.read_row_group(i).to_pandas() for i in range(table.num_row_groups)]\n",
    "full_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Obtener user_ids Ãºnicos y barajarlos\n",
    "user_ids = full_df['user_id'].unique()\n",
    "np.random.shuffle(user_ids)\n",
    "\n",
    "# Dividir usuarios en entrenamiento y validaciÃ³n (80-20)\n",
    "split_index = int(0.8 * len(user_ids))\n",
    "train_user_ids = set(user_ids[:split_index])\n",
    "val_user_ids = set(user_ids[split_index:])\n",
    "\n",
    "# Guardar los conjuntos de user_ids\n",
    "with open('train_user_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(train_user_ids, f)\n",
    "\n",
    "with open('val_user_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(val_user_ids, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiendo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE_DIM: 13\n",
      "Dispositivo: cuda\n",
      "Modelo en: cuda:0\n",
      "Nuevo pos_weight: 10.0\n",
      "Criterion: BCEWithLogitsLoss()\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 1e-05\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "Fraction of Dataset: 5.0%\n",
      "\n",
      "--- NÃºmero de epochs: 5 ---\n",
      "--- Epoch 1/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.88 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.39 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.3867\n",
      "ğŸ–¥ï¸ RAM Usage: 8.27 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.39 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 1\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 9301 muestras\n",
      "Clase 1: 108849 muestras\n",
      "F1-score: 0.2668, AUC-ROC: 0.7033\n",
      "Validation AUC-ROC: 0.7033, Average Precision: 0.4817, MAP@5: nan\n",
      "NDCG@5: 0.2077\n",
      "Validation NDCG: 0.2077\n",
      "âœ… Nuevo mejor modelo guardado: best_model_epoch_1_ndcg_0.2077.pt\n",
      "--- Epoch 2/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.31 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.39 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.3603\n",
      "ğŸ–¥ï¸ RAM Usage: 8.32 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.39 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 2\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 74979 muestras\n",
      "Clase 1: 43171 muestras\n",
      "F1-score: 0.4818, AUC-ROC: 0.7339\n",
      "Validation AUC-ROC: 0.7339, Average Precision: 0.4280, MAP@5: nan\n",
      "NDCG@5: 0.2143\n",
      "Validation NDCG: 0.2143\n",
      "âœ… Nuevo mejor modelo guardado: best_model_epoch_2_ndcg_0.2143.pt\n",
      "--- Epoch 3/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.32 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.39 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.3100\n",
      "ğŸ–¥ï¸ RAM Usage: 8.33 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.39 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 3\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 82747 muestras\n",
      "Clase 1: 35403 muestras\n",
      "F1-score: 0.4891, AUC-ROC: 0.7580\n",
      "Validation AUC-ROC: 0.7580, Average Precision: 0.4379, MAP@5: nan\n",
      "NDCG@5: 0.2271\n",
      "Validation NDCG: 0.2271\n",
      "âœ… Nuevo mejor modelo guardado: best_model_epoch_3_ndcg_0.2271.pt\n",
      "--- Epoch 4/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.33 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.39 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.2831\n",
      "ğŸ–¥ï¸ RAM Usage: 8.34 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.39 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 4\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 90835 muestras\n",
      "Clase 1: 27315 muestras\n",
      "F1-score: 0.4830, AUC-ROC: 0.7833\n",
      "Validation AUC-ROC: 0.7833, Average Precision: 0.4748, MAP@5: nan\n",
      "NDCG@5: 0.2388\n",
      "Validation NDCG: 0.2388\n",
      "âœ… Nuevo mejor modelo guardado: best_model_epoch_4_ndcg_0.2388.pt\n",
      "--- Epoch 5/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.34 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.39 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.2636\n",
      "ğŸ–¥ï¸ RAM Usage: 8.35 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.39 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 5\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 94457 muestras\n",
      "Clase 1: 23693 muestras\n",
      "F1-score: 0.4720, AUC-ROC: 0.7958\n",
      "Validation AUC-ROC: 0.7958, Average Precision: 0.4886, MAP@5: nan\n",
      "NDCG@5: 0.2424\n",
      "Validation NDCG: 0.2424\n",
      "âœ… Nuevo mejor modelo guardado: best_model_epoch_5_ndcg_0.2424.pt\n"
     ]
    }
   ],
   "source": [
    "# Importar las librerÃ­as necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import psutil\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from torch.utils.data import get_worker_info\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import json\n",
    "from sklearn.metrics import ndcg_score\n",
    "import pickle\n",
    "\n",
    "# ConfiguraciÃ³n global de caracterÃ­sticas\n",
    "FEATURE_COLS = [\"time_since_last\", \"session_relative_position\", \"session_duration\",\n",
    "                \"R\", \"F\", \"M\", \"device_type\", \"pagetype\", \"discount\", \"cod_section\", \"family\",\n",
    "                \"is_new_user\", \"has_session_history\"]\n",
    "\n",
    "FEATURE_DIM = len(FEATURE_COLS)\n",
    "print(f\"FEATURE_DIM: {FEATURE_DIM}\")\n",
    "\n",
    "\n",
    "# âœ… 1ï¸âƒ£ ConfiguraciÃ³n del Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# ğŸ”¥ FunciÃ³n para Monitoreo de Memoria\n",
    "def print_memory_usage(epoch=None):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"ğŸ–¥ï¸ RAM Usage: {mem_info.rss / 1e9:.2f} GB\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ”¥ GPU Usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"ğŸ”¥ GPU Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    if epoch is not None:\n",
    "        print(f\"ğŸ“Š Memoria despuÃ©s del Epoch {epoch}\")\n",
    "\n",
    "# âœ… ImplementaciÃ³n de Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = torch.tensor(1.0, dtype=torch.float32, device=device) if pos_weight is None else pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # âœ… Clamping para evitar valores extremos\n",
    "        inputs = torch.clamp(inputs, min=-10, max=10)\n",
    "\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, pos_weight=self.pos_weight, reduction='none'\n",
    "        )\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        pt = targets * probas + (1 - targets) * (1 - probas)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        loss = self.alpha * focal_term * bce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# âœ… 2ï¸âƒ£ Definir Modelo Ajustado con Dropout y RegularizaciÃ³n\n",
    "# âœ… Modelo GRU con AtenciÃ³n\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attn_weights = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, gru_output):\n",
    "        \"\"\"\n",
    "        gru_output: Tensor de tamaÃ±o (batch, seq_len, hidden_dim)\n",
    "        Devuelve:\n",
    "        - Scores de cada producto en la sesiÃ³n (batch, seq_len)\n",
    "        \"\"\"\n",
    "        attn_scores = self.attn_weights(gru_output).squeeze(-1)  # (batch, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)  # Normalizamos\n",
    "        return attn_weights\n",
    "\n",
    "class GRURecommender(nn.Module):\n",
    "    def __init__(self, input_dim=50, feature_dim=FEATURE_DIM, hidden_dim=128, num_layers=2, output_dim=1):\n",
    "        super(GRURecommender, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim + feature_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.attention = AttentionLayer(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # InicializaciÃ³n de pesos\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.GRU):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, product_embeddings, session_features):\n",
    "            x = torch.cat((product_embeddings, session_features), dim=-1)\n",
    "            gru_output, _ = self.gru(x)\n",
    "            # Eliminar clamping\n",
    "            # gru_output = torch.clamp(gru_output, min=-10, max=10)\n",
    "            attn_weights = self.attention(gru_output)\n",
    "            weighted_output = gru_output * attn_weights.unsqueeze(-1)\n",
    "            scores = self.fc(self.dropout(weighted_output)).squeeze(-1)\n",
    "            # Eliminar clamping\n",
    "            # return torch.clamp(scores, min=-10, max=10)\n",
    "            return scores\n",
    "\n",
    "model = GRURecommender(input_dim=50, feature_dim=FEATURE_DIM, hidden_dim=128, num_layers=2, output_dim=1).to(device)\n",
    "print(f\"Modelo en: {next(model.parameters()).device}\")\n",
    "\n",
    "# âœ… 3ï¸âƒ£ Definir PÃ©rdida y Optimizador con Focal Loss y regularizaciÃ³n\n",
    "# pos_weight_value = 1\n",
    "# pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32, device=device)\n",
    "\n",
    "# num_pos = df['add_to_cart'].sum()\n",
    "# num_neg = len(df) - num_pos\n",
    "# pos_weight_value = num_neg / num_pos\n",
    "pos_weight_value = 10\n",
    "pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32, device=device)\n",
    "print(f\"Nuevo pos_weight: {pos_weight}\")\n",
    "\n",
    "# criterion = FocalLoss(alpha=0.5, gamma=2.5, pos_weight=pos_weight)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "# scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "print(f\"Criterion: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "\n",
    "# âœ… 4ï¸âƒ£ Definir Funciones de EvaluaciÃ³n con MÃ©tricas Adicionales\n",
    "def evaluate(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device)\n",
    "            session_features = session_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(product_embeddings, session_features)  # (batch, seq_len)\n",
    "            outputs = torch.sigmoid(outputs)  # Convertimos logits a probabilidades\n",
    "\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())\n",
    "            all_outputs.extend(outputs.cpu().numpy().flatten())\n",
    "\n",
    "    # âœ… Convertir listas a arrays de NumPy\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_outputs = np.array(all_outputs)\n",
    "\n",
    "    # âœ… Filtrar NaN antes de calcular mÃ©tricas\n",
    "    mask = ~np.isnan(all_targets) & ~np.isnan(all_outputs)\n",
    "    all_targets = all_targets[mask]\n",
    "    all_outputs = all_outputs[mask]\n",
    "\n",
    "    if len(all_targets) == 0:\n",
    "        print(\"âš ï¸ No hay datos vÃ¡lidos para calcular mÃ©tricas.\")\n",
    "        return float('nan'), float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    # Ahora calculamos las predicciones y la distribuciÃ³n\n",
    "    predicted_classes = (all_outputs >= threshold).astype(int)\n",
    "    unique_classes, counts = np.unique(predicted_classes, return_counts=True)\n",
    "    print(\"DistribuciÃ³n de las predicciones del modelo:\")\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        print(f\"Clase {cls}: {count} muestras\")\n",
    "\n",
    "    # Calculamos las mÃ©tricas\n",
    "    auc_roc = roc_auc_score(all_targets, all_outputs)\n",
    "    average_precision = average_precision_score(all_targets, all_outputs)\n",
    "    f1 = f1_score(all_targets, predicted_classes, zero_division=0)\n",
    "\n",
    "    print(f\"F1-score: {f1:.4f}, AUC-ROC: {auc_roc:.4f}\")\n",
    "    return auc_roc, average_precision, float('nan'), f1\n",
    "\n",
    "def evaluate_ndcg(model, data_loader, k=5):\n",
    "    model.eval()\n",
    "    ndcg_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            product_embeddings, session_features, targets, session_ids, partnumbers_list = batch\n",
    "            product_embeddings = product_embeddings.to(device)\n",
    "            session_features = session_features.to(device)\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "\n",
    "            for output, target in zip(outputs, targets):\n",
    "                # AsegÃºrate de que output y target estÃ¡n alineados y son arrays numpy\n",
    "                ndcg = ndcg_score([target], [output], k=k)\n",
    "                ndcg_scores.append(ndcg)\n",
    "\n",
    "    avg_ndcg = np.mean(ndcg_scores)\n",
    "    print(f\"NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "    return avg_ndcg\n",
    "\n",
    "\n",
    "# âœ… 5ï¸âƒ£ FunciÃ³n de Entrenamiento con Persistencia del Mejor Modelo\n",
    "def train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, epochs):\n",
    "    best_ndcg = 0.0\n",
    "    # Eliminamos el escalador de gradientes ya que no usamos AMP\n",
    "    # scaler = torch.cuda.amp.GradScaler('cuda')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,  # No es necesario shuffle con IterableDataset\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        print_memory_usage(epoch=\"Inicio\")\n",
    "        print(\"Iniciando el bucle de batches...\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            # ğŸš¨ Verificar si hay NaN o Inf en las entradas\n",
    "            if torch.isnan(product_embeddings).any() or torch.isinf(product_embeddings).any():\n",
    "                print(f\"âš ï¸ Batch {batch_idx}: `NaN` o `Inf` en `product_embeddings`. Saltando batch...\")\n",
    "                continue\n",
    "            if torch.isnan(session_features).any() or torch.isinf(session_features).any():\n",
    "                print(f\"âš ï¸ Batch {batch_idx}: `NaN` o `Inf` en `session_features`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Sin AMP ni autocast\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "\n",
    "            # ğŸš¨ Verificar valores extremos en `outputs`\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(f\"âš ï¸ Batch {batch_idx}: `NaN` o `Inf` en `outputs`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            # Eliminamos el clamping para evitar interferir con los gradientes\n",
    "            # outputs = torch.clamp(outputs, min=-5, max=5)\n",
    "\n",
    "            targets = targets[:, :outputs.shape[1]]\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # âœ… Verificar si la pÃ©rdida es infinita o NaN antes de `backward()`\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(f\"âš ï¸ Batch {batch_idx}: `NaN` o `Inf` en `loss`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            # RetropropagaciÃ³n sin AMP\n",
    "            loss.backward()\n",
    "\n",
    "            # Verificar si los gradientes son finitos\n",
    "            grads_finite = True\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                    grads_finite = False\n",
    "                    print(f\"âš ï¸ Batch {batch_idx}: NaN o Inf en gradientes en {name}. Saltando actualizaciÃ³n.\")\n",
    "                    break\n",
    "\n",
    "            if grads_finite:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "        print_memory_usage(epoch=epoch+1)\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        auc_roc, avg_precision, mapk_score, f1 = evaluate(model, val_loader)\n",
    "        print(f\"Validation AUC-ROC: {auc_roc:.4f}, Average Precision: {avg_precision:.4f}, MAP@5: {mapk_score:.4f}\")\n",
    "        val_ndcg = evaluate_ndcg(model, val_loader)\n",
    "        print(f\"Validation NDCG: {val_ndcg:.4f}\")\n",
    "        \n",
    "        if val_ndcg > best_ndcg:\n",
    "            best_ndcg = val_ndcg\n",
    "            model_filename = f\"best_model_epoch_{epoch+1}_ndcg_{val_ndcg:.4f}.pt\"\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(f\"âœ… Nuevo mejor modelo guardado: {model_filename}\")\n",
    "        else:\n",
    "            print(f\"No hay mejora en NDCG@5: {val_ndcg:.4f} <= {best_ndcg:.4f}\")\n",
    "\n",
    "    return model_filename\n",
    "\n",
    "\n",
    "# âœ… 6ï¸âƒ£ Cargar Dataset con NormalizaciÃ³n y Manejo de Outliers\n",
    "MAX_SEQ_LENGTH = 50\n",
    "\n",
    "class IterableSessionDataset(IterableDataset):\n",
    "    def __init__(self, df_path, fraction=1.0, mode='train', balance=True, user_ids=None, train_user_ids=None):\n",
    "        self.df_path = df_path\n",
    "        self.feature_cols = FEATURE_COLS\n",
    "        self.fraction = fraction\n",
    "        self.mode = mode\n",
    "        self.balance = balance\n",
    "        self.user_ids = user_ids\n",
    "        self.train_user_ids = train_user_ids\n",
    "\n",
    "        # Cargar el DataFrame completo para calcular estadÃ­sticas globales\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        num_row_groups = table.metadata.num_row_groups\n",
    "        dfs = [table.read_row_group(i).to_pandas() for i in range(num_row_groups)]\n",
    "        full_df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Verificar que train_user_ids estÃ¡ disponible\n",
    "        if self.train_user_ids is None:\n",
    "            raise ValueError(\"train_user_ids no estÃ¡ definido. Debes proporcionarlo al inicializar.\")\n",
    "        \n",
    "        # Calcular 'is_new_user' en full_df\n",
    "        full_df['is_new_user'] = full_df['user_id'].apply(lambda x: 0 if x in self.train_user_ids else 1)\n",
    "        \n",
    "        # Calcular 'has_session_history' en full_df\n",
    "        full_df['has_session_history'] = full_df.groupby('session_id').cumcount().apply(lambda x: 0 if x == 0 else 1)\n",
    "        \n",
    "        self.global_feature_means = full_df[self.feature_cols].mean()\n",
    "        self.global_feature_stds = full_df[self.feature_cols].std().replace(0, 1e-6)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            return self._data_iterator()\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            return self._data_iterator(worker_id, num_workers)\n",
    "        \n",
    "        \n",
    "    def _data_iterator(self, worker_id=0, num_workers=1):\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            row_groups = range(0, int(num_row_groups_to_use * 0.8))\n",
    "        else:\n",
    "            row_groups = range(int(num_row_groups_to_use * 0.8), num_row_groups_to_use)\n",
    "\n",
    "        for i in row_groups:\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "            \n",
    "            df_majority = df[df.add_to_cart == 0]\n",
    "            df_minority = df[df.add_to_cart == 1]\n",
    "            df_minority_oversampled = df_minority.sample(len(df_majority), replace=True, random_state=42)\n",
    "            df_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "            df = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            \n",
    "            # Filtrar por user_ids\n",
    "            if self.user_ids is not None:\n",
    "                df = df[df['user_id'].isin(self.user_ids)]\n",
    "                        \n",
    "            # AÃ±adir columna 'is_new_user'\n",
    "            df['is_new_user'] = df['user_id'].apply(lambda x: 0 if x in self.train_user_ids else 1)\n",
    "            # AÃ±adir columna 'has_session_history'\n",
    "            df['has_session_history'] = df.groupby('session_id').cumcount().apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "            # Aplicar transformaciones logarÃ­tmicas\n",
    "            for col in ['time_since_last', 'session_duration']:\n",
    "                df[col] = np.log1p(df[col])\n",
    "\n",
    "            # NormalizaciÃ³n global segura\n",
    "            df[self.feature_cols] = (df[self.feature_cols] - self.global_feature_means) / self.global_feature_stds\n",
    "\n",
    "            # Reemplazar posibles NaN resultantes de la normalizaciÃ³n\n",
    "            df[self.feature_cols] = df[self.feature_cols].fillna(0.0)\n",
    "\n",
    "            # Continuar con el resto del procesamiento...\n",
    "            # if self.balance:\n",
    "            #     df_1 = df[df[\"add_to_cart\"] == 1]\n",
    "            #     df_0 = df[df[\"add_to_cart\"] == 0]\n",
    "            #     if len(df_1) > 0 and len(df_0) > 0:\n",
    "            #         if len(df_1) > len(df_0):\n",
    "            #             df_0 = df_0.sample(n=len(df_1), replace=True, random_state=42)\n",
    "            #         else:\n",
    "            #             df_1 = df_1.sample(n=len(df_0), replace=True, random_state=42)\n",
    "            #    df = pd.concat([df_0, df_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            for session_id, session_data in df.groupby(\"session_id\"):\n",
    "                product_embeddings = session_data[\"embedding_reduced\"].tolist()\n",
    "                partnumbers = session_data[\"partnumber\"].tolist()\n",
    "\n",
    "                # Verificar si hay NaN en embeddings\n",
    "                if any(embedding is None or np.isnan(embedding).any() for embedding in product_embeddings):\n",
    "                    continue\n",
    "\n",
    "                product_embeddings = torch.tensor(np.array(product_embeddings), dtype=torch.float32)\n",
    "\n",
    "                session_features = torch.tensor(session_data[self.feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "                # Verificar si hay NaN en features\n",
    "                if torch.isnan(session_features).any():\n",
    "                    continue\n",
    "\n",
    "                targets = torch.tensor(session_data[\"add_to_cart\"].values, dtype=torch.float32)\n",
    "                yield product_embeddings, session_features, targets, session_id, partnumbers\n",
    "                \n",
    "                \n",
    "# âœ… FunciÃ³n de ColaciÃ³n\n",
    "def collate_fn(batch):\n",
    "    product_embeddings, session_features, targets, session_ids, partnumbers_list = zip(*batch)\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), torch.empty(0), torch.empty(0), [], []\n",
    "\n",
    "    max_len = min(max([x.shape[0] for x in product_embeddings]), MAX_SEQ_LENGTH)\n",
    "\n",
    "    padded_embeddings = torch.zeros((len(batch), max_len, product_embeddings[0].shape[1]))\n",
    "    padded_features = torch.zeros((len(batch), max_len, session_features[0].shape[1]))\n",
    "    padded_targets = torch.zeros((len(batch), max_len))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        seq_len = min(product_embeddings[i].shape[0], max_len)\n",
    "        padded_embeddings[i, :seq_len] = product_embeddings[i][:seq_len]\n",
    "        padded_features[i, :seq_len] = session_features[i][:seq_len]\n",
    "        padded_targets[i, :seq_len] = targets[i][:seq_len]\n",
    "\n",
    "    return padded_embeddings, padded_features, padded_targets, session_ids, partnumbers_list\n",
    "\n",
    "\n",
    "# âœ… 7ï¸âƒ£ Crear Datasets de Entrenamiento y ValidaciÃ³n\n",
    "data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "fraction = 0.05\n",
    "with open('train_user_ids.pkl', 'rb') as f:\n",
    "    train_user_ids = pickle.load(f)\n",
    "\n",
    "with open('val_user_ids.pkl', 'rb') as f:\n",
    "    val_user_ids = pickle.load(f)\n",
    "\n",
    "train_dataset = IterableSessionDataset(\n",
    "    data_path, fraction=fraction, mode='train', balance=True, user_ids=train_user_ids, train_user_ids=train_user_ids\n",
    ")\n",
    "val_dataset = IterableSessionDataset(\n",
    "    data_path, fraction=fraction, mode='val', balance=False, user_ids=val_user_ids, train_user_ids=train_user_ids\n",
    ")\n",
    "# train_dataset = IterableSessionDataset(data_path, fraction=fraction, mode='train')\n",
    "# val_dataset = IterableSessionDataset(data_path, fraction=fraction, mode='val')\n",
    "\n",
    "\n",
    "# âœ… 8ï¸âƒ£ Ejecutar Entrenamiento\n",
    "EPOCHS = 5\n",
    "print(f'\\nFraction of Dataset: {fraction*100}%\\n')\n",
    "print(f'--- NÃºmero de epochs: {EPOCHS} ---')\n",
    "best_model_file = train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE_DIM: 13\n",
      "Dispositivo: cuda\n",
      "\n",
      "==== Fold 1/5 ====\n",
      "--- Epoch 1/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.08 GB\n",
      "ğŸ”¥ GPU Usage: 0.00 GB\n",
      "ğŸ”¥ GPU Cached: 0.00 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.3782\n",
      "ğŸ–¥ï¸ RAM Usage: 8.66 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 1\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 12898 muestras\n",
      "Clase 1: 76602 muestras\n",
      "F1-score: 0.2446, AUC-ROC: 0.6320\n",
      "Validation AUC-ROC: 0.6320, Average Precision: 0.4517, MAP@5: nan\n",
      "NDCG@5: 0.2051\n",
      "Validation NDCG: 0.2051\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_1_ndcg_0.2051.pt\n",
      "--- Epoch 2/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.72 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.3472\n",
      "ğŸ–¥ï¸ RAM Usage: 8.72 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 2\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 54742 muestras\n",
      "Clase 1: 34758 muestras\n",
      "F1-score: 0.4746, AUC-ROC: 0.7374\n",
      "Validation AUC-ROC: 0.7374, Average Precision: 0.4333, MAP@5: nan\n",
      "NDCG@5: 0.2373\n",
      "Validation NDCG: 0.2373\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_2_ndcg_0.2373.pt\n",
      "--- Epoch 3/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.76 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.3003\n",
      "ğŸ–¥ï¸ RAM Usage: 8.76 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 3\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 57913 muestras\n",
      "Clase 1: 31587 muestras\n",
      "F1-score: 0.4898, AUC-ROC: 0.7643\n",
      "Validation AUC-ROC: 0.7643, Average Precision: 0.4496, MAP@5: nan\n",
      "NDCG@5: 0.2421\n",
      "Validation NDCG: 0.2421\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_3_ndcg_0.2421.pt\n",
      "--- Epoch 4/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.76 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.2763\n",
      "ğŸ–¥ï¸ RAM Usage: 8.78 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 4\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 60707 muestras\n",
      "Clase 1: 28793 muestras\n",
      "F1-score: 0.4970, AUC-ROC: 0.7939\n",
      "Validation AUC-ROC: 0.7939, Average Precision: 0.4930, MAP@5: nan\n",
      "NDCG@5: 0.2449\n",
      "Validation NDCG: 0.2449\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_4_ndcg_0.2449.pt\n",
      "--- Epoch 5/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.80 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.2558\n",
      "ğŸ–¥ï¸ RAM Usage: 8.80 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 5\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 61415 muestras\n",
      "Clase 1: 28085 muestras\n",
      "F1-score: 0.4907, AUC-ROC: 0.8072\n",
      "Validation AUC-ROC: 0.8072, Average Precision: 0.5140, MAP@5: nan\n",
      "NDCG@5: 0.2462\n",
      "Validation NDCG: 0.2462\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_5_ndcg_0.2462.pt\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 61415 muestras\n",
      "Clase 1: 28085 muestras\n",
      "F1-score: 0.4907, AUC-ROC: 0.8072\n",
      "NDCG@5: 0.2462\n",
      "\n",
      "==== Fold 2/5 ====\n",
      "--- Epoch 1/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.80 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.3779\n",
      "ğŸ–¥ï¸ RAM Usage: 8.10 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 1\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 7165 muestras\n",
      "Clase 1: 87335 muestras\n",
      "F1-score: 0.2697, AUC-ROC: 0.6957\n",
      "Validation AUC-ROC: 0.6957, Average Precision: 0.4276, MAP@5: nan\n",
      "NDCG@5: 0.2087\n",
      "Validation NDCG: 0.2087\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_1_ndcg_0.2087.pt\n",
      "--- Epoch 2/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.12 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.3487\n",
      "ğŸ–¥ï¸ RAM Usage: 8.15 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 2\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 51566 muestras\n",
      "Clase 1: 42934 muestras\n",
      "F1-score: 0.4488, AUC-ROC: 0.7305\n",
      "Validation AUC-ROC: 0.7305, Average Precision: 0.4013, MAP@5: nan\n",
      "NDCG@5: 0.2440\n",
      "Validation NDCG: 0.2440\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_2_ndcg_0.2440.pt\n",
      "--- Epoch 3/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.17 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.3035\n",
      "ğŸ–¥ï¸ RAM Usage: 8.17 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 3\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 56879 muestras\n",
      "Clase 1: 37621 muestras\n",
      "F1-score: 0.4748, AUC-ROC: 0.7580\n",
      "Validation AUC-ROC: 0.7580, Average Precision: 0.4213, MAP@5: nan\n",
      "NDCG@5: 0.2501\n",
      "Validation NDCG: 0.2501\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_3_ndcg_0.2501.pt\n",
      "--- Epoch 4/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.17 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.2811\n",
      "ğŸ–¥ï¸ RAM Usage: 8.17 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 4\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 61022 muestras\n",
      "Clase 1: 33478 muestras\n",
      "F1-score: 0.4937, AUC-ROC: 0.7897\n",
      "Validation AUC-ROC: 0.7897, Average Precision: 0.4670, MAP@5: nan\n",
      "NDCG@5: 0.2580\n",
      "Validation NDCG: 0.2580\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_4_ndcg_0.2580.pt\n",
      "--- Epoch 5/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.17 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.2643\n",
      "ğŸ–¥ï¸ RAM Usage: 8.18 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 5\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 62425 muestras\n",
      "Clase 1: 32075 muestras\n",
      "F1-score: 0.4953, AUC-ROC: 0.8058\n",
      "Validation AUC-ROC: 0.8058, Average Precision: 0.4918, MAP@5: nan\n",
      "NDCG@5: 0.2583\n",
      "Validation NDCG: 0.2583\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_5_ndcg_0.2583.pt\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 62425 muestras\n",
      "Clase 1: 32075 muestras\n",
      "F1-score: 0.4953, AUC-ROC: 0.8058\n",
      "NDCG@5: 0.2583\n",
      "\n",
      "==== Fold 3/5 ====\n",
      "--- Epoch 1/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.61 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.7969\n",
      "ğŸ–¥ï¸ RAM Usage: 7.82 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 1\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 554032 muestras\n",
      "Clase 1: 4040968 muestras\n",
      "F1-score: 0.1416, AUC-ROC: 0.6079\n",
      "Validation AUC-ROC: 0.6079, Average Precision: 0.3660, MAP@5: nan\n",
      "NDCG@5: 0.1158\n",
      "Validation NDCG: 0.1158\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_1_ndcg_0.1158.pt\n",
      "--- Epoch 2/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.87 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.7947\n",
      "ğŸ–¥ï¸ RAM Usage: 7.87 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 2\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 429943 muestras\n",
      "Clase 1: 4165057 muestras\n",
      "F1-score: 0.1490, AUC-ROC: 0.6353\n",
      "Validation AUC-ROC: 0.6353, Average Precision: 0.3922, MAP@5: nan\n",
      "NDCG@5: 0.1171\n",
      "Validation NDCG: 0.1171\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_2_ndcg_0.1171.pt\n",
      "--- Epoch 3/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.83 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.7930\n",
      "ğŸ–¥ï¸ RAM Usage: 7.83 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 3\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 384864 muestras\n",
      "Clase 1: 4210136 muestras\n",
      "F1-score: 0.1522, AUC-ROC: 0.6473\n",
      "Validation AUC-ROC: 0.6473, Average Precision: 0.4033, MAP@5: nan\n",
      "NDCG@5: 0.1175\n",
      "Validation NDCG: 0.1175\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_3_ndcg_0.1175.pt\n",
      "--- Epoch 4/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.86 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.7918\n",
      "ğŸ–¥ï¸ RAM Usage: 7.86 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 4\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 346899 muestras\n",
      "Clase 1: 4248101 muestras\n",
      "F1-score: 0.1552, AUC-ROC: 0.6585\n",
      "Validation AUC-ROC: 0.6585, Average Precision: 0.4130, MAP@5: nan\n",
      "NDCG@5: 0.1180\n",
      "Validation NDCG: 0.1180\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_4_ndcg_0.1180.pt\n",
      "--- Epoch 5/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.87 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.7910\n",
      "ğŸ–¥ï¸ RAM Usage: 7.89 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 5\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 330405 muestras\n",
      "Clase 1: 4264595 muestras\n",
      "F1-score: 0.1566, AUC-ROC: 0.6635\n",
      "Validation AUC-ROC: 0.6635, Average Precision: 0.4172, MAP@5: nan\n",
      "NDCG@5: 0.1182\n",
      "Validation NDCG: 0.1182\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_5_ndcg_0.1182.pt\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 330405 muestras\n",
      "Clase 1: 4264595 muestras\n",
      "F1-score: 0.1566, AUC-ROC: 0.6635\n",
      "NDCG@5: 0.1182\n",
      "\n",
      "==== Fold 4/5 ====\n",
      "--- Epoch 1/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.86 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.3789\n",
      "ğŸ–¥ï¸ RAM Usage: 8.15 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 1\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 4330 muestras\n",
      "Clase 1: 86070 muestras\n",
      "F1-score: 0.2963, AUC-ROC: 0.8054\n",
      "Validation AUC-ROC: 0.8054, Average Precision: 0.4877, MAP@5: nan\n",
      "NDCG@5: 0.2250\n",
      "Validation NDCG: 0.2250\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_1_ndcg_0.2250.pt\n",
      "--- Epoch 2/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.18 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.3530\n",
      "ğŸ–¥ï¸ RAM Usage: 8.15 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 2\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 51930 muestras\n",
      "Clase 1: 38470 muestras\n",
      "F1-score: 0.5009, AUC-ROC: 0.7853\n",
      "Validation AUC-ROC: 0.7853, Average Precision: 0.4372, MAP@5: nan\n",
      "NDCG@5: 0.2459\n",
      "Validation NDCG: 0.2459\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_2_ndcg_0.2459.pt\n",
      "--- Epoch 3/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.16 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.3033\n",
      "ğŸ–¥ï¸ RAM Usage: 8.17 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 3\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 54456 muestras\n",
      "Clase 1: 35944 muestras\n",
      "F1-score: 0.4989, AUC-ROC: 0.8090\n",
      "Validation AUC-ROC: 0.8090, Average Precision: 0.4548, MAP@5: nan\n",
      "NDCG@5: 0.2483\n",
      "Validation NDCG: 0.2483\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_3_ndcg_0.2483.pt\n",
      "--- Epoch 4/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.17 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.2719\n",
      "ğŸ–¥ï¸ RAM Usage: 8.18 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 4\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 55633 muestras\n",
      "Clase 1: 34767 muestras\n",
      "F1-score: 0.4904, AUC-ROC: 0.8317\n",
      "Validation AUC-ROC: 0.8317, Average Precision: 0.5014, MAP@5: nan\n",
      "NDCG@5: 0.2535\n",
      "Validation NDCG: 0.2535\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_4_ndcg_0.2535.pt\n",
      "--- Epoch 5/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 8.14 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.2444\n",
      "ğŸ–¥ï¸ RAM Usage: 8.15 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 5\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 55440 muestras\n",
      "Clase 1: 34960 muestras\n",
      "F1-score: 0.4810, AUC-ROC: 0.8332\n",
      "Validation AUC-ROC: 0.8332, Average Precision: 0.5139, MAP@5: nan\n",
      "NDCG@5: 0.2546\n",
      "Validation NDCG: 0.2546\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_5_ndcg_0.2546.pt\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 55440 muestras\n",
      "Clase 1: 34960 muestras\n",
      "F1-score: 0.4810, AUC-ROC: 0.8332\n",
      "NDCG@5: 0.2546\n",
      "\n",
      "==== Fold 5/5 ====\n",
      "--- Epoch 1/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.73 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.3777\n",
      "ğŸ–¥ï¸ RAM Usage: 7.84 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 1\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 9014 muestras\n",
      "Clase 1: 79786 muestras\n",
      "F1-score: 0.2694, AUC-ROC: 0.7209\n",
      "Validation AUC-ROC: 0.7209, Average Precision: 0.4651, MAP@5: nan\n",
      "NDCG@5: 0.1931\n",
      "Validation NDCG: 0.1931\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_1_ndcg_0.1931.pt\n",
      "--- Epoch 2/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.89 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.3540\n",
      "ğŸ–¥ï¸ RAM Usage: 7.90 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 2\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 51837 muestras\n",
      "Clase 1: 36963 muestras\n",
      "F1-score: 0.4912, AUC-ROC: 0.7725\n",
      "Validation AUC-ROC: 0.7725, Average Precision: 0.4401, MAP@5: nan\n",
      "NDCG@5: 0.2306\n",
      "Validation NDCG: 0.2306\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_2_ndcg_0.2306.pt\n",
      "--- Epoch 3/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.79 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.3126\n",
      "ğŸ–¥ï¸ RAM Usage: 7.80 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 3\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 54464 muestras\n",
      "Clase 1: 34336 muestras\n",
      "F1-score: 0.5074, AUC-ROC: 0.7987\n",
      "Validation AUC-ROC: 0.7987, Average Precision: 0.4524, MAP@5: nan\n",
      "NDCG@5: 0.2386\n",
      "Validation NDCG: 0.2386\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_3_ndcg_0.2386.pt\n",
      "--- Epoch 4/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.80 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.2905\n",
      "ğŸ–¥ï¸ RAM Usage: 7.80 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 4\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 56640 muestras\n",
      "Clase 1: 32160 muestras\n",
      "F1-score: 0.5187, AUC-ROC: 0.8240\n",
      "Validation AUC-ROC: 0.8240, Average Precision: 0.4889, MAP@5: nan\n",
      "NDCG@5: 0.2417\n",
      "Validation NDCG: 0.2417\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_4_ndcg_0.2417.pt\n",
      "--- Epoch 5/5 ---\n",
      "ğŸ–¥ï¸ RAM Usage: 7.82 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.2728\n",
      "ğŸ–¥ï¸ RAM Usage: 7.83 GB\n",
      "ğŸ”¥ GPU Usage: 0.02 GB\n",
      "ğŸ”¥ GPU Cached: 0.38 GB\n",
      "ğŸ“Š Memoria despuÃ©s del Epoch 5\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 57197 muestras\n",
      "Clase 1: 31603 muestras\n",
      "F1-score: 0.5183, AUC-ROC: 0.8363\n",
      "Validation AUC-ROC: 0.8363, Average Precision: 0.5131, MAP@5: nan\n",
      "NDCG@5: 0.2432\n",
      "Validation NDCG: 0.2432\n",
      "âœ… Nuevo mejor modelo guardado: CV_best_model_epoch_5_ndcg_0.2432.pt\n",
      "DistribuciÃ³n de las predicciones del modelo:\n",
      "Clase 0: 57197 muestras\n",
      "Clase 1: 31603 muestras\n",
      "F1-score: 0.5183, AUC-ROC: 0.8363\n",
      "NDCG@5: 0.2432\n",
      "\n",
      "Promedio de mÃ©tricas en 5-fold cross-validation:\n",
      "AUC-ROC: 0.7892, Average Precision: 0.4900, MAP@5: nan, F1-score: 0.4284, NDCG@5: 0.2241\n"
     ]
    }
   ],
   "source": [
    "# Importar las librerÃ­as necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import psutil\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from torch.utils.data import get_worker_info\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import json\n",
    "from sklearn.metrics import ndcg_score\n",
    "import pickle\n",
    "\n",
    "# ConfiguraciÃ³n global de caracterÃ­sticas\n",
    "FEATURE_COLS = [\"time_since_last\", \"session_relative_position\", \"session_duration\",\n",
    "                \"R\", \"F\", \"M\", \"device_type\", \"pagetype\", \"discount\", \"cod_section\", \"family\",\n",
    "                \"is_new_user\", \"has_session_history\"]\n",
    "\n",
    "FEATURE_DIM = len(FEATURE_COLS)\n",
    "print(f\"FEATURE_DIM: {FEATURE_DIM}\")\n",
    "\n",
    "# âœ… 1ï¸âƒ£ ConfiguraciÃ³n del Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# ğŸ”¥ FunciÃ³n para Monitoreo de Memoria\n",
    "def print_memory_usage(epoch=None):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"ğŸ–¥ï¸ RAM Usage: {mem_info.rss / 1e9:.2f} GB\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ”¥ GPU Usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"ğŸ”¥ GPU Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    if epoch is not None:\n",
    "        print(f\"ğŸ“Š Memoria despuÃ©s del Epoch {epoch}\")\n",
    "\n",
    "# âœ… ImplementaciÃ³n de Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = torch.tensor(1.0, dtype=torch.float32, device=device) if pos_weight is None else pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # âœ… Clamping para evitar valores extremos\n",
    "        inputs = torch.clamp(inputs, min=-10, max=10)\n",
    "\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, pos_weight=self.pos_weight, reduction='none'\n",
    "        )\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        pt = targets * probas + (1 - targets) * (1 - probas)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        loss = self.alpha * focal_term * bce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# âœ… 2ï¸âƒ£ Definir Modelo Ajustado con Dropout y RegularizaciÃ³n\n",
    "# âœ… Modelo GRU con AtenciÃ³n\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attn_weights = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, gru_output):\n",
    "        \"\"\"\n",
    "        gru_output: Tensor de tamaÃ±o (batch, seq_len, hidden_dim)\n",
    "        Devuelve:\n",
    "        - Scores de cada producto en la sesiÃ³n (batch, seq_len)\n",
    "        \"\"\"\n",
    "        attn_scores = self.attn_weights(gru_output).squeeze(-1)  # (batch, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)  # Normalizamos\n",
    "        return attn_weights\n",
    "\n",
    "class GRURecommender(nn.Module):\n",
    "    def __init__(self, input_dim=50, feature_dim=FEATURE_DIM, hidden_dim=128, num_layers=2, output_dim=1):\n",
    "        super(GRURecommender, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim + feature_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.attention = AttentionLayer(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # InicializaciÃ³n de pesos\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.GRU):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, product_embeddings, session_features):\n",
    "        x = torch.cat((product_embeddings, session_features), dim=-1)\n",
    "        gru_output, _ = self.gru(x)\n",
    "        attn_weights = self.attention(gru_output)\n",
    "        weighted_output = gru_output * attn_weights.unsqueeze(-1)\n",
    "        scores = self.fc(self.dropout(weighted_output)).squeeze(-1)\n",
    "        return scores\n",
    "\n",
    "# âœ… 3ï¸âƒ£ ConfiguraciÃ³n de pÃ©rdida, optimizador y scheduler se harÃ¡ dentro de cada fold mÃ¡s adelante\n",
    "\n",
    "# âœ… 4ï¸âƒ£ Definir Funciones de EvaluaciÃ³n con MÃ©tricas Adicionales\n",
    "def evaluate(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device)\n",
    "            session_features = session_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(product_embeddings, session_features)  # (batch, seq_len)\n",
    "            outputs = torch.sigmoid(outputs)  # Convertimos logits a probabilidades\n",
    "\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())\n",
    "            all_outputs.extend(outputs.cpu().numpy().flatten())\n",
    "\n",
    "    # âœ… Convertir listas a arrays de NumPy\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_outputs = np.array(all_outputs)\n",
    "\n",
    "    # âœ… Filtrar NaN antes de calcular mÃ©tricas\n",
    "    mask = ~np.isnan(all_targets) & ~np.isnan(all_outputs)\n",
    "    all_targets = all_targets[mask]\n",
    "    all_outputs = all_outputs[mask]\n",
    "\n",
    "    if len(all_targets) == 0:\n",
    "        print(\"âš ï¸ No hay datos vÃ¡lidos para calcular mÃ©tricas.\")\n",
    "        return float('nan'), float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    # Ahora calculamos las predicciones y la distribuciÃ³n\n",
    "    predicted_classes = (all_outputs >= threshold).astype(int)\n",
    "    unique_classes, counts = np.unique(predicted_classes, return_counts=True)\n",
    "    print(\"DistribuciÃ³n de las predicciones del modelo:\")\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        print(f\"Clase {cls}: {count} muestras\")\n",
    "\n",
    "    # Calculamos las mÃ©tricas\n",
    "    auc_roc = roc_auc_score(all_targets, all_outputs)\n",
    "    average_precision = average_precision_score(all_targets, all_outputs)\n",
    "    f1 = f1_score(all_targets, predicted_classes, zero_division=0)\n",
    "\n",
    "    print(f\"F1-score: {f1:.4f}, AUC-ROC: {auc_roc:.4f}\")\n",
    "    return auc_roc, average_precision, float('nan'), f1\n",
    "\n",
    "def evaluate_ndcg(model, data_loader, k=5):\n",
    "    model.eval()\n",
    "    ndcg_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            product_embeddings, session_features, targets, session_ids, partnumbers_list = batch\n",
    "            product_embeddings = product_embeddings.to(device)\n",
    "            session_features = session_features.to(device)\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "\n",
    "            for output, target in zip(outputs, targets):\n",
    "                # AsegÃºrate de que output y target estÃ¡n alineados y son arrays numpy\n",
    "                ndcg = ndcg_score([target], [output], k=k)\n",
    "                ndcg_scores.append(ndcg)\n",
    "\n",
    "    avg_ndcg = np.mean(ndcg_scores)\n",
    "    print(f\"NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "    return avg_ndcg\n",
    "\n",
    "# âœ… 5ï¸âƒ£ FunciÃ³n de Entrenamiento con Persistencia del Mejor Modelo\n",
    "def train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, epochs):\n",
    "    best_ndcg = 0.0\n",
    "    model_filename = None  # Para almacenar el nombre del mejor modelo de este fold\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,  # No es necesario shuffle con IterableDataset\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "\n",
    "        print_memory_usage(epoch=\"Inicio\")\n",
    "        print(\"Iniciando el bucle de batches...\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            # ğŸš¨ Verificar si hay NaN o Inf en las entradas\n",
    "            if torch.isnan(product_embeddings).any() or torch.isinf(product_embeddings).any():\n",
    "                print(f\"âš ï¸ Batch {batch_idx}: `NaN` o `Inf` en `product_embeddings`. Saltando batch...\")\n",
    "                continue\n",
    "            if torch.isnan(session_features).any() or torch.isinf(session_features).any():\n",
    "                print(f\"âš ï¸ Batch {batch_idx}: `NaN` o `Inf` en `session_features`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "\n",
    "            # ğŸš¨ Verificar valores extremos en `outputs`\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(f\"âš ï¸ Batch {batch_idx}: `NaN` o `Inf` en `outputs`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            targets = targets[:, :outputs.shape[1]]\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # âœ… Verificar si la pÃ©rdida es infinita o NaN antes de `backward()`\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(f\"âš ï¸ Batch {batch_idx}: `NaN` o `Inf` en `loss`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Verificar si los gradientes son finitos\n",
    "            grads_finite = True\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                    grads_finite = False\n",
    "                    print(f\"âš ï¸ Batch {batch_idx}: NaN o Inf en gradientes en {name}. Saltando actualizaciÃ³n.\")\n",
    "                    break\n",
    "\n",
    "            if grads_finite:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "        print_memory_usage(epoch=epoch+1)\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        auc_roc, avg_precision, mapk_score, f1 = evaluate(model, val_loader)\n",
    "        print(f\"Validation AUC-ROC: {auc_roc:.4f}, Average Precision: {avg_precision:.4f}, MAP@5: {mapk_score:.4f}\")\n",
    "        val_ndcg = evaluate_ndcg(model, val_loader)\n",
    "        print(f\"Validation NDCG: {val_ndcg:.4f}\")\n",
    "\n",
    "        if val_ndcg > best_ndcg:\n",
    "            best_ndcg = val_ndcg\n",
    "            model_filename = f\"CV_best_model_epoch_{epoch+1}_ndcg_{val_ndcg:.4f}.pt\"\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(f\"âœ… Nuevo mejor modelo guardado: {model_filename}\")\n",
    "        else:\n",
    "            print(f\"No hay mejora en NDCG@5: {val_ndcg:.4f} <= {best_ndcg:.4f}\")\n",
    "\n",
    "    return model_filename\n",
    "\n",
    "# âœ… 6ï¸âƒ£ Cargar Dataset con NormalizaciÃ³n y Manejo de Outliers\n",
    "MAX_SEQ_LENGTH = 50\n",
    "\n",
    "class IterableSessionDataset(IterableDataset):\n",
    "    def __init__(self, df_path, fraction=1.0, mode='train', balance=True, user_ids=None, train_user_ids=None):\n",
    "        self.df_path = df_path\n",
    "        self.feature_cols = FEATURE_COLS\n",
    "        self.fraction = fraction\n",
    "        self.mode = mode\n",
    "        self.balance = balance\n",
    "        self.user_ids = user_ids\n",
    "        self.train_user_ids = train_user_ids\n",
    "\n",
    "        # Cargar el DataFrame completo para calcular estadÃ­sticas globales de entrenamiento\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        num_row_groups = table.metadata.num_row_groups\n",
    "        dfs = [table.read_row_group(i).to_pandas() for i in range(num_row_groups)]\n",
    "        full_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Verificar que train_user_ids estÃ¡ disponible\n",
    "        if self.train_user_ids is None:\n",
    "            raise ValueError(\"train_user_ids no estÃ¡ definido. Debes proporcionarlo al inicializar.\")\n",
    "\n",
    "        # Calcular 'is_new_user' en full_df\n",
    "        full_df['is_new_user'] = full_df['user_id'].apply(lambda x: 0 if x in self.train_user_ids else 1)\n",
    "\n",
    "        # Calcular 'has_session_history' en full_df\n",
    "        full_df['has_session_history'] = full_df.groupby('session_id').cumcount().apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "        self.global_feature_means = full_df[self.feature_cols].mean()\n",
    "        self.global_feature_stds = full_df[self.feature_cols].std().replace(0, 1e-6)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            return self._data_iterator()\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            return self._data_iterator(worker_id, num_workers)\n",
    "\n",
    "    def _data_iterator(self, worker_id=0, num_workers=1):\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            row_groups = range(0, int(num_row_groups_to_use * 0.8))\n",
    "        else:\n",
    "            row_groups = range(int(num_row_groups_to_use * 0.8), num_row_groups_to_use)\n",
    "\n",
    "        for i in row_groups:\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "\n",
    "            # Balanceo del dataset\n",
    "            df_majority = df[df.add_to_cart == 0]\n",
    "            df_minority = df[df.add_to_cart == 1]\n",
    "            if len(df_minority) > 0:\n",
    "                df_minority_oversampled = df_minority.sample(len(df_majority), replace=True, random_state=42)\n",
    "                df_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "                df = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            # Filtrar por user_ids si se proporcionan\n",
    "            if self.user_ids is not None:\n",
    "                df = df[df['user_id'].isin(self.user_ids)]\n",
    "\n",
    "            # AÃ±adir columna 'is_new_user'\n",
    "            df['is_new_user'] = df['user_id'].apply(lambda x: 0 if x in self.train_user_ids else 1)\n",
    "            # AÃ±adir columna 'has_session_history'\n",
    "            df['has_session_history'] = df.groupby('session_id').cumcount().apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "            # Aplicar transformaciones logarÃ­tmicas\n",
    "            for col in ['time_since_last', 'session_duration']:\n",
    "                df[col] = np.log1p(df[col])\n",
    "\n",
    "            # NormalizaciÃ³n global segura\n",
    "            df[self.feature_cols] = (df[self.feature_cols] - self.global_feature_means) / self.global_feature_stds\n",
    "\n",
    "            # Reemplazar posibles NaN resultantes de la normalizaciÃ³n\n",
    "            df[self.feature_cols] = df[self.feature_cols].fillna(0.0)\n",
    "\n",
    "            for session_id, session_data in df.groupby(\"session_id\"):\n",
    "                product_embeddings = session_data[\"embedding_reduced\"].tolist()\n",
    "                partnumbers = session_data[\"partnumber\"].tolist()\n",
    "\n",
    "                # Verificar si hay NaN en embeddings\n",
    "                if any(embedding is None or np.isnan(embedding).any() for embedding in product_embeddings):\n",
    "                    continue\n",
    "\n",
    "                product_embeddings = torch.tensor(np.array(product_embeddings), dtype=torch.float32)\n",
    "\n",
    "                session_features = torch.tensor(session_data[self.feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "                # Verificar si hay NaN en features\n",
    "                if torch.isnan(session_features).any():\n",
    "                    continue\n",
    "\n",
    "                targets = torch.tensor(session_data[\"add_to_cart\"].values, dtype=torch.float32)\n",
    "                yield product_embeddings, session_features, targets, session_id, partnumbers\n",
    "\n",
    "\n",
    "# âœ… FunciÃ³n de ColaciÃ³n\n",
    "def collate_fn(batch):\n",
    "    product_embeddings, session_features, targets, session_ids, partnumbers_list = zip(*batch)\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), torch.empty(0), torch.empty(0), [], []\n",
    "\n",
    "    max_len = min(max([x.shape[0] for x in product_embeddings]), MAX_SEQ_LENGTH)\n",
    "\n",
    "    padded_embeddings = torch.zeros((len(batch), max_len, product_embeddings[0].shape[1]))\n",
    "    padded_features = torch.zeros((len(batch), max_len, session_features[0].shape[1]))\n",
    "    padded_targets = torch.zeros((len(batch), max_len))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        seq_len = min(product_embeddings[i].shape[0], max_len)\n",
    "        padded_embeddings[i, :seq_len] = product_embeddings[i][:seq_len]\n",
    "        padded_features[i, :seq_len] = session_features[i][:seq_len]\n",
    "        padded_targets[i, :seq_len] = targets[i][:seq_len]\n",
    "\n",
    "    return padded_embeddings, padded_features, padded_targets, session_ids, partnumbers_list\n",
    "\n",
    "# âœ… 7ï¸âƒ£ Crear Datasets y preparaciÃ³n para validaciÃ³n cruzada\n",
    "data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "fraction = 0.05\n",
    "\n",
    "with open('train_user_ids.pkl', 'rb') as f:\n",
    "    train_user_ids = pickle.load(f)\n",
    "\n",
    "with open('val_user_ids.pkl', 'rb') as f:\n",
    "    val_user_ids = pickle.load(f)\n",
    "\n",
    "# âœ… 8ï¸âƒ£ ImplementaciÃ³n de K-Fold Cross-Validation\n",
    "EPOCHS = 5\n",
    "k = 5  # NÃºmero de folds para cross-validation\n",
    "\n",
    "# Mezclar y dividir train_user_ids en k folds\n",
    "train_user_ids = list(train_user_ids)  # Convertir set a lista\n",
    "np.random.shuffle(train_user_ids)\n",
    "folds = np.array_split(train_user_ids, k)\n",
    "\n",
    "all_fold_metrics = []\n",
    "\n",
    "for fold in range(k):\n",
    "    print(f\"\\n==== Fold {fold+1}/{k} ====\")\n",
    "\n",
    "    # Definir IDs de usuario para este fold\n",
    "    val_fold_ids = folds[fold]\n",
    "    # Combinar los folds restantes para formar el conjunto de entrenamiento\n",
    "    train_fold_ids = np.concatenate([folds[i] for i in range(k) if i != fold])\n",
    "    \n",
    "    # Crear datasets para este fold\n",
    "    train_dataset = IterableSessionDataset(\n",
    "        data_path, fraction=fraction, mode='train', balance=True,\n",
    "        user_ids=train_fold_ids, train_user_ids=train_fold_ids\n",
    "    )\n",
    "    val_dataset = IterableSessionDataset(\n",
    "        data_path, fraction=fraction, mode='val', balance=False,\n",
    "        user_ids=val_fold_ids, train_user_ids=train_fold_ids\n",
    "    )\n",
    "    \n",
    "    # Re-inicializar modelo, criterio, optimizador y scheduler para cada fold\n",
    "    model = GRURecommender(input_dim=50, feature_dim=FEATURE_DIM, hidden_dim=128, num_layers=2, output_dim=1).to(device)\n",
    "    pos_weight_value = 10\n",
    "    pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32, device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "    \n",
    "    # Entrenar el modelo para este fold\n",
    "    best_model_file = train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, EPOCHS)\n",
    "    \n",
    "    # Crear DataLoader para evaluaciÃ³n en el fold actual\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Evaluar el rendimiento en el conjunto de validaciÃ³n para este fold\n",
    "    auc_roc, avg_precision, mapk_score, f1 = evaluate(model, val_loader)\n",
    "    val_ndcg = evaluate_ndcg(model, val_loader)\n",
    "    \n",
    "    # Almacenar mÃ©tricas del fold actual\n",
    "    all_fold_metrics.append((auc_roc, avg_precision, mapk_score, f1, val_ndcg))\n",
    "\n",
    "# Convertir resultados a array de NumPy para calcular promedios\n",
    "all_fold_metrics = np.array(all_fold_metrics)\n",
    "avg_metrics = np.mean(all_fold_metrics, axis=0)\n",
    "\n",
    "print(f\"\\nPromedio de mÃ©tricas en {k}-fold cross-validation:\")\n",
    "print(f\"AUC-ROC: {avg_metrics[0]:.4f}, Average Precision: {avg_metrics[1]:.4f}, \"\n",
    "      f\"MAP@5: {avg_metrics[2]:.4f}, F1-score: {avg_metrics[3]:.4f}, NDCG@5: {avg_metrics[4]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_product_attributes(products_file_path):\n",
    "    df_products = pd.read_parquet(products_file_path)\n",
    "    product_attributes = {}\n",
    "    for idx, row in df_products.iterrows():\n",
    "        partnumber = int(row['partnumber'])\n",
    "        product_attributes[partnumber] = {\n",
    "            'cod_section': row.get('cod_section', None),\n",
    "            'family': row.get('family', None),\n",
    "            # AÃ±adir mÃ¡s atributos si es necesario\n",
    "        }\n",
    "    return product_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableSessionDataset(IterableDataset):\n",
    "    def __init__(self, df_path, fraction=1.0, mode='train', balance=True):\n",
    "        self.df_path = df_path\n",
    "        self.feature_cols = FEATURE_COLS\n",
    "        self.fraction = fraction\n",
    "        self.mode = mode\n",
    "        self.balance = balance  # Solo aplica para entrenamiento\n",
    "        # Cargar estadÃ­sticas globales para normalizaciÃ³n\n",
    "        self.global_feature_means = None\n",
    "        self.global_feature_stds = None\n",
    "        self._compute_global_statistics()\n",
    "\n",
    "    def _compute_global_statistics(self):\n",
    "        # Cargar una muestra del dataset para calcular la media y desviaciÃ³n estÃ¡ndar\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        dfs = []\n",
    "        for i in range(num_row_groups_to_use):\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "            dfs.append(df)\n",
    "\n",
    "        full_df = pd.concat(dfs)\n",
    "        # Determinar columnas disponibles entre las caracterÃ­sticas esperadas\n",
    "        available_cols = [col for col in self.feature_cols if col in full_df.columns]\n",
    "        self.global_feature_means = full_df[available_cols].mean()\n",
    "        self.global_feature_stds = full_df[available_cols].std().replace(0, 1e-6)\n",
    "\n",
    "        # Asignar valores por defecto para columnas faltantes\n",
    "        for col in self.feature_cols:\n",
    "            if col not in available_cols:\n",
    "                self.global_feature_means[col] = 0.0\n",
    "                self.global_feature_stds[col] = 1.0\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Devuelve un iterador sobre los datos. \"\"\"\n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            return self._data_iterator()\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            return self._data_iterator(worker_id, num_workers)\n",
    "\n",
    "    def _data_iterator(self, worker_id=0, num_workers=1):\n",
    "        \"\"\" Iterador interno que carga los datos desde Parquet. \"\"\"\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            row_groups = range(0, int(num_row_groups_to_use * 0.8))\n",
    "        elif self.mode == 'val':\n",
    "            row_groups = range(int(num_row_groups_to_use * 0.8), num_row_groups_to_use)\n",
    "        elif self.mode == 'test':\n",
    "            row_groups = range(0, num_row_groups_to_use)\n",
    "        else:\n",
    "            raise ValueError(\"Modo invÃ¡lido. Use 'train', 'val' o 'test'.\")\n",
    "\n",
    "        for i in row_groups:\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "\n",
    "            # Asegurar que todas las columnas de FEATURE_COLS estÃ©n presentes\n",
    "            for col in self.feature_cols:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = 0.0\n",
    "\n",
    "            # Aplicar transformaciones logarÃ­tmicas\n",
    "            for col in ['time_since_last', 'session_duration']:\n",
    "                df[col] = np.log1p(df[col])\n",
    "\n",
    "            # NormalizaciÃ³n global segura\n",
    "            df[self.feature_cols] = (df[self.feature_cols] - self.global_feature_means) / self.global_feature_stds\n",
    "            df[self.feature_cols] = df[self.feature_cols].fillna(0.0)\n",
    "\n",
    "            # Para el modo 'train' y 'val', podemos balancear los datos\n",
    "            if self.mode in ['train', 'val'] and self.balance:\n",
    "                df_1 = df[df[\"add_to_cart\"] == 1]\n",
    "                df_0 = df[df[\"add_to_cart\"] == 0]\n",
    "                if len(df_1) > 0 and len(df_0) > 0:\n",
    "                    if len(df_1) > len(df_0):\n",
    "                        df_0 = df_0.sample(n=len(df_1), replace=True, random_state=42)\n",
    "                    else:\n",
    "                        df_1 = df_1.sample(n=len(df_0), replace=True, random_state=42)\n",
    "                    df = pd.concat([df_0, df_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            for session_id, session_data in df.groupby(\"session_id\"):\n",
    "                product_embeddings = session_data[\"embedding_reduced\"].tolist()\n",
    "                partnumbers = session_data[\"partnumber\"].tolist()\n",
    "\n",
    "                # Verificar si hay NaN en embeddings\n",
    "                if any(embedding is None or np.isnan(embedding).any() for embedding in product_embeddings):\n",
    "                    continue\n",
    "\n",
    "                product_embeddings = torch.tensor(np.array(product_embeddings), dtype=torch.float32)\n",
    "                session_features = torch.tensor(session_data[self.feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "                # Verificar si hay NaN en features\n",
    "                if torch.isnan(session_features).any():\n",
    "                    continue\n",
    "\n",
    "                if self.mode in ['train', 'val']:\n",
    "                    targets = torch.tensor(session_data[\"add_to_cart\"].values, dtype=torch.float32)\n",
    "                else:\n",
    "                    targets = None  # No hay targets en el conjunto de prueba\n",
    "\n",
    "                yield product_embeddings, session_features, targets, session_id, partnumbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    product_embeddings, session_features, targets, session_ids, partnumbers_list = zip(*batch)\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), torch.empty(0), None, [], []\n",
    "\n",
    "    max_len = min(max([x.shape[0] for x in product_embeddings]), MAX_SEQ_LENGTH)\n",
    "\n",
    "    padded_embeddings = torch.zeros((len(batch), max_len, product_embeddings[0].shape[1]))\n",
    "    padded_features = torch.zeros((len(batch), max_len, session_features[0].shape[1]))\n",
    "\n",
    "    if targets[0] is not None:\n",
    "        padded_targets = torch.zeros((len(batch), max_len))\n",
    "    else:\n",
    "        padded_targets = None  # No hay targets en el conjunto de prueba\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        seq_len = min(product_embeddings[i].shape[0], max_len)\n",
    "        padded_embeddings[i, :seq_len] = product_embeddings[i][:seq_len]\n",
    "        padded_features[i, :seq_len] = session_features[i][:seq_len]\n",
    "        if targets[0] is not None:\n",
    "            padded_targets[i, :seq_len] = targets[i][:seq_len]\n",
    "\n",
    "    return padded_embeddings, padded_features, padded_targets, session_ids, partnumbers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los atributos de los productos\n",
    "product_attributes = load_product_attributes(\"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/optimized_products_transformed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_v2(model, test_dataset, device, top_k=5):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    recommendations = {}\n",
    "    global_top_recommendations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            product_embeddings, session_features, _, session_ids, partnumbers_list = batch\n",
    "\n",
    "            if product_embeddings.shape[0] == 0:\n",
    "                continue  # Saltar batch vacÃ­o\n",
    "\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(product_embeddings, session_features)  # (batch, seq_len)\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()  # Convertimos logits a probabilidades\n",
    "\n",
    "            for sid, parts, score_seq in zip(session_ids, partnumbers_list, scores):\n",
    "                # Asegurar que `parts` y `score_seq` tienen la misma longitud\n",
    "                if len(parts) != len(score_seq):\n",
    "                    min_len = min(len(parts), len(score_seq))\n",
    "                    parts = parts[:min_len]\n",
    "                    score_seq = score_seq[:min_len]\n",
    "\n",
    "                product_scores = list(zip(parts, score_seq))\n",
    "                product_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                top_recommendations = [int(p) for p, s in product_scores[:top_k]]\n",
    "                top_recommendations = list(dict.fromkeys(top_recommendations))\n",
    "\n",
    "                # Recopilar productos similares basados en atributos\n",
    "                if len(top_recommendations) < top_k:\n",
    "                    # Obtener atributos de los productos ya recomendados\n",
    "                    similar_candidates = []\n",
    "                    for rec in top_recommendations:\n",
    "                        rec_attrs = product_attributes.get(rec, {})\n",
    "                        # Buscar otros productos con atributos similares\n",
    "                        for p in parts:\n",
    "                            p = int(p)\n",
    "                            if p in top_recommendations:\n",
    "                                continue\n",
    "                            p_attrs = product_attributes.get(p, {})\n",
    "                            # Comparar atributos (por ejemplo, cod_section y family)\n",
    "                            if (p_attrs.get(\"cod_section\") == rec_attrs.get(\"cod_section\") and\n",
    "                                p_attrs.get(\"family\") == rec_attrs.get(\"family\")):\n",
    "                                similar_candidates.append(p)\n",
    "                    # AÃ±adir candidatos similares si es necesario\n",
    "                    for candidate in similar_candidates:\n",
    "                        if candidate not in top_recommendations and len(top_recommendations) < top_k:\n",
    "                            top_recommendations.append(candidate)\n",
    "\n",
    "                    # Acumular productos para fallback global si aÃºn faltan recomendaciones\n",
    "                    if len(top_recommendations) < top_k:\n",
    "                        global_top_recommendations.extend(\n",
    "                            [p for p in parts if p not in top_recommendations]\n",
    "                        )\n",
    "\n",
    "                recommendations[int(sid)] = top_recommendations\n",
    "\n",
    "    # Fallback global\n",
    "    product_counts = {}\n",
    "    for p in global_top_recommendations:\n",
    "        product_counts[p] = product_counts.get(p, 0) + 1\n",
    "    sorted_global_top = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    global_top = [p for p, count in sorted_global_top]\n",
    "\n",
    "    for sid, recs in recommendations.items():\n",
    "        if len(recs) < top_k:\n",
    "            additional = [p for p in global_top if p not in recs]\n",
    "            recs.extend(additional[:top_k - len(recs)])\n",
    "            recommendations[sid] = recs\n",
    "\n",
    "    output_json = {\n",
    "        \"target\": recommendations\n",
    "    }\n",
    "\n",
    "    with open(\"submission_v3.json\", \"w\") as f:\n",
    "        json.dump(output_json, f, indent=4)\n",
    "\n",
    "    print(\"Archivo 'submission_v3.json' generado con Ã©xito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'submission_v3.json' generado con Ã©xito.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el mejor modelo entrenado\n",
    "best_model_file = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/new_model/CV_best_model_epoch_5_ndcg_0.2583.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_file, weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Preparar el dataset de prueba\n",
    "test_data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_test_full.parquet\"\n",
    "test_dataset = IterableSessionDataset(test_data_path, fraction=1.0, mode='test', balance=False)\n",
    "\n",
    "# Ejecutar la inferencia\n",
    "inference_v2(model, test_dataset, device, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
