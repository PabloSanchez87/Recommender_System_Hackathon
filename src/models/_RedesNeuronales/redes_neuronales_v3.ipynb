{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir el Conjunto de Datos Basado en Usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "# df = pd.read_parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2766405\n",
      "44047154\n",
      "15.922163963700182\n"
     ]
    }
   ],
   "source": [
    "# num_pos = df['add_to_cart'].sum()\n",
    "# num_neg = len(df) - num_pos\n",
    "\n",
    "# print(num_pos)\n",
    "# print(num_neg)\n",
    "\n",
    "# pos_weight_value = num_neg / num_pos\n",
    "# print(pos_weight_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto completo\n",
    "import pyarrow.parquet as pq\n",
    "data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "\n",
    "table = pq.ParquetFile(data_path)\n",
    "dfs = [table.read_row_group(i).to_pandas() for i in range(table.num_row_groups)]\n",
    "full_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Obtener user_ids √∫nicos y barajarlos\n",
    "user_ids = full_df['user_id'].unique()\n",
    "np.random.shuffle(user_ids)\n",
    "\n",
    "# Dividir usuarios en entrenamiento y validaci√≥n (80-20)\n",
    "split_index = int(0.8 * len(user_ids))\n",
    "train_user_ids = set(user_ids[:split_index])\n",
    "val_user_ids = set(user_ids[split_index:])\n",
    "\n",
    "# Guardar los conjuntos de user_ids\n",
    "with open('train_user_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(train_user_ids, f)\n",
    "\n",
    "with open('val_user_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(val_user_ids, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiendo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE_DIM: 13\n",
      "Dispositivo: cuda\n",
      "Modelo en: cuda:0\n",
      "Nuevo pos_weight: 10.0\n",
      "Criterion: BCEWithLogitsLoss()\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 1e-05\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "Fraction of Dataset: 5.0%\n",
      "\n",
      "--- N√∫mero de epochs: 5 ---\n",
      "--- Epoch 1/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.88 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.39 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.3867\n",
      "üñ•Ô∏è RAM Usage: 8.27 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.39 GB\n",
      "üìä Memoria despu√©s del Epoch 1\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 9301 muestras\n",
      "Clase 1: 108849 muestras\n",
      "F1-score: 0.2668, AUC-ROC: 0.7033\n",
      "Validation AUC-ROC: 0.7033, Average Precision: 0.4817, MAP@5: nan\n",
      "NDCG@5: 0.2077\n",
      "Validation NDCG: 0.2077\n",
      "‚úÖ Nuevo mejor modelo guardado: best_model_epoch_1_ndcg_0.2077.pt\n",
      "--- Epoch 2/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.31 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.39 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.3603\n",
      "üñ•Ô∏è RAM Usage: 8.32 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.39 GB\n",
      "üìä Memoria despu√©s del Epoch 2\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 74979 muestras\n",
      "Clase 1: 43171 muestras\n",
      "F1-score: 0.4818, AUC-ROC: 0.7339\n",
      "Validation AUC-ROC: 0.7339, Average Precision: 0.4280, MAP@5: nan\n",
      "NDCG@5: 0.2143\n",
      "Validation NDCG: 0.2143\n",
      "‚úÖ Nuevo mejor modelo guardado: best_model_epoch_2_ndcg_0.2143.pt\n",
      "--- Epoch 3/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.32 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.39 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.3100\n",
      "üñ•Ô∏è RAM Usage: 8.33 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.39 GB\n",
      "üìä Memoria despu√©s del Epoch 3\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 82747 muestras\n",
      "Clase 1: 35403 muestras\n",
      "F1-score: 0.4891, AUC-ROC: 0.7580\n",
      "Validation AUC-ROC: 0.7580, Average Precision: 0.4379, MAP@5: nan\n",
      "NDCG@5: 0.2271\n",
      "Validation NDCG: 0.2271\n",
      "‚úÖ Nuevo mejor modelo guardado: best_model_epoch_3_ndcg_0.2271.pt\n",
      "--- Epoch 4/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.33 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.39 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.2831\n",
      "üñ•Ô∏è RAM Usage: 8.34 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.39 GB\n",
      "üìä Memoria despu√©s del Epoch 4\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 90835 muestras\n",
      "Clase 1: 27315 muestras\n",
      "F1-score: 0.4830, AUC-ROC: 0.7833\n",
      "Validation AUC-ROC: 0.7833, Average Precision: 0.4748, MAP@5: nan\n",
      "NDCG@5: 0.2388\n",
      "Validation NDCG: 0.2388\n",
      "‚úÖ Nuevo mejor modelo guardado: best_model_epoch_4_ndcg_0.2388.pt\n",
      "--- Epoch 5/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.34 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.39 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.2636\n",
      "üñ•Ô∏è RAM Usage: 8.35 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.39 GB\n",
      "üìä Memoria despu√©s del Epoch 5\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 94457 muestras\n",
      "Clase 1: 23693 muestras\n",
      "F1-score: 0.4720, AUC-ROC: 0.7958\n",
      "Validation AUC-ROC: 0.7958, Average Precision: 0.4886, MAP@5: nan\n",
      "NDCG@5: 0.2424\n",
      "Validation NDCG: 0.2424\n",
      "‚úÖ Nuevo mejor modelo guardado: best_model_epoch_5_ndcg_0.2424.pt\n"
     ]
    }
   ],
   "source": [
    "# Importar las librer√≠as necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import psutil\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from torch.utils.data import get_worker_info\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import json\n",
    "from sklearn.metrics import ndcg_score\n",
    "import pickle\n",
    "\n",
    "# Configuraci√≥n global de caracter√≠sticas\n",
    "FEATURE_COLS = [\"time_since_last\", \"session_relative_position\", \"session_duration\",\n",
    "                \"R\", \"F\", \"M\", \"device_type\", \"pagetype\", \"discount\", \"cod_section\", \"family\",\n",
    "                \"is_new_user\", \"has_session_history\"]\n",
    "\n",
    "FEATURE_DIM = len(FEATURE_COLS)\n",
    "print(f\"FEATURE_DIM: {FEATURE_DIM}\")\n",
    "\n",
    "\n",
    "# ‚úÖ 1Ô∏è‚É£ Configuraci√≥n del Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# üî• Funci√≥n para Monitoreo de Memoria\n",
    "def print_memory_usage(epoch=None):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"üñ•Ô∏è RAM Usage: {mem_info.rss / 1e9:.2f} GB\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üî• GPU Usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"üî• GPU Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    if epoch is not None:\n",
    "        print(f\"üìä Memoria despu√©s del Epoch {epoch}\")\n",
    "\n",
    "# ‚úÖ Implementaci√≥n de Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = torch.tensor(1.0, dtype=torch.float32, device=device) if pos_weight is None else pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # ‚úÖ Clamping para evitar valores extremos\n",
    "        inputs = torch.clamp(inputs, min=-10, max=10)\n",
    "\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, pos_weight=self.pos_weight, reduction='none'\n",
    "        )\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        pt = targets * probas + (1 - targets) * (1 - probas)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        loss = self.alpha * focal_term * bce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# ‚úÖ 2Ô∏è‚É£ Definir Modelo Ajustado con Dropout y Regularizaci√≥n\n",
    "# ‚úÖ Modelo GRU con Atenci√≥n\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attn_weights = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, gru_output):\n",
    "        \"\"\"\n",
    "        gru_output: Tensor de tama√±o (batch, seq_len, hidden_dim)\n",
    "        Devuelve:\n",
    "        - Scores de cada producto en la sesi√≥n (batch, seq_len)\n",
    "        \"\"\"\n",
    "        attn_scores = self.attn_weights(gru_output).squeeze(-1)  # (batch, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)  # Normalizamos\n",
    "        return attn_weights\n",
    "\n",
    "class GRURecommender(nn.Module):\n",
    "    def __init__(self, input_dim=50, feature_dim=FEATURE_DIM, hidden_dim=128, num_layers=2, output_dim=1):\n",
    "        super(GRURecommender, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim + feature_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.attention = AttentionLayer(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Inicializaci√≥n de pesos\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.GRU):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, product_embeddings, session_features):\n",
    "            x = torch.cat((product_embeddings, session_features), dim=-1)\n",
    "            gru_output, _ = self.gru(x)\n",
    "            # Eliminar clamping\n",
    "            # gru_output = torch.clamp(gru_output, min=-10, max=10)\n",
    "            attn_weights = self.attention(gru_output)\n",
    "            weighted_output = gru_output * attn_weights.unsqueeze(-1)\n",
    "            scores = self.fc(self.dropout(weighted_output)).squeeze(-1)\n",
    "            # Eliminar clamping\n",
    "            # return torch.clamp(scores, min=-10, max=10)\n",
    "            return scores\n",
    "\n",
    "model = GRURecommender(input_dim=50, feature_dim=FEATURE_DIM, hidden_dim=128, num_layers=2, output_dim=1).to(device)\n",
    "print(f\"Modelo en: {next(model.parameters()).device}\")\n",
    "\n",
    "# ‚úÖ 3Ô∏è‚É£ Definir P√©rdida y Optimizador con Focal Loss y regularizaci√≥n\n",
    "# pos_weight_value = 1\n",
    "# pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32, device=device)\n",
    "\n",
    "# num_pos = df['add_to_cart'].sum()\n",
    "# num_neg = len(df) - num_pos\n",
    "# pos_weight_value = num_neg / num_pos\n",
    "pos_weight_value = 10\n",
    "pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32, device=device)\n",
    "print(f\"Nuevo pos_weight: {pos_weight}\")\n",
    "\n",
    "# criterion = FocalLoss(alpha=0.5, gamma=2.5, pos_weight=pos_weight)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "# scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "print(f\"Criterion: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "\n",
    "# ‚úÖ 4Ô∏è‚É£ Definir Funciones de Evaluaci√≥n con M√©tricas Adicionales\n",
    "def evaluate(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device)\n",
    "            session_features = session_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(product_embeddings, session_features)  # (batch, seq_len)\n",
    "            outputs = torch.sigmoid(outputs)  # Convertimos logits a probabilidades\n",
    "\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())\n",
    "            all_outputs.extend(outputs.cpu().numpy().flatten())\n",
    "\n",
    "    # ‚úÖ Convertir listas a arrays de NumPy\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_outputs = np.array(all_outputs)\n",
    "\n",
    "    # ‚úÖ Filtrar NaN antes de calcular m√©tricas\n",
    "    mask = ~np.isnan(all_targets) & ~np.isnan(all_outputs)\n",
    "    all_targets = all_targets[mask]\n",
    "    all_outputs = all_outputs[mask]\n",
    "\n",
    "    if len(all_targets) == 0:\n",
    "        print(\"‚ö†Ô∏è No hay datos v√°lidos para calcular m√©tricas.\")\n",
    "        return float('nan'), float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    # Ahora calculamos las predicciones y la distribuci√≥n\n",
    "    predicted_classes = (all_outputs >= threshold).astype(int)\n",
    "    unique_classes, counts = np.unique(predicted_classes, return_counts=True)\n",
    "    print(\"Distribuci√≥n de las predicciones del modelo:\")\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        print(f\"Clase {cls}: {count} muestras\")\n",
    "\n",
    "    # Calculamos las m√©tricas\n",
    "    auc_roc = roc_auc_score(all_targets, all_outputs)\n",
    "    average_precision = average_precision_score(all_targets, all_outputs)\n",
    "    f1 = f1_score(all_targets, predicted_classes, zero_division=0)\n",
    "\n",
    "    print(f\"F1-score: {f1:.4f}, AUC-ROC: {auc_roc:.4f}\")\n",
    "    return auc_roc, average_precision, float('nan'), f1\n",
    "\n",
    "def evaluate_ndcg(model, data_loader, k=5):\n",
    "    model.eval()\n",
    "    ndcg_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            product_embeddings, session_features, targets, session_ids, partnumbers_list = batch\n",
    "            product_embeddings = product_embeddings.to(device)\n",
    "            session_features = session_features.to(device)\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "\n",
    "            for output, target in zip(outputs, targets):\n",
    "                # Aseg√∫rate de que output y target est√°n alineados y son arrays numpy\n",
    "                ndcg = ndcg_score([target], [output], k=k)\n",
    "                ndcg_scores.append(ndcg)\n",
    "\n",
    "    avg_ndcg = np.mean(ndcg_scores)\n",
    "    print(f\"NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "    return avg_ndcg\n",
    "\n",
    "\n",
    "# ‚úÖ 5Ô∏è‚É£ Funci√≥n de Entrenamiento con Persistencia del Mejor Modelo\n",
    "def train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, epochs):\n",
    "    best_ndcg = 0.0\n",
    "    # Eliminamos el escalador de gradientes ya que no usamos AMP\n",
    "    # scaler = torch.cuda.amp.GradScaler('cuda')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,  # No es necesario shuffle con IterableDataset\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        print_memory_usage(epoch=\"Inicio\")\n",
    "        print(\"Iniciando el bucle de batches...\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            # üö® Verificar si hay NaN o Inf en las entradas\n",
    "            if torch.isnan(product_embeddings).any() or torch.isinf(product_embeddings).any():\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` o `Inf` en `product_embeddings`. Saltando batch...\")\n",
    "                continue\n",
    "            if torch.isnan(session_features).any() or torch.isinf(session_features).any():\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` o `Inf` en `session_features`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Sin AMP ni autocast\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "\n",
    "            # üö® Verificar valores extremos en `outputs`\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` o `Inf` en `outputs`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            # Eliminamos el clamping para evitar interferir con los gradientes\n",
    "            # outputs = torch.clamp(outputs, min=-5, max=5)\n",
    "\n",
    "            targets = targets[:, :outputs.shape[1]]\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # ‚úÖ Verificar si la p√©rdida es infinita o NaN antes de `backward()`\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` o `Inf` en `loss`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            # Retropropagaci√≥n sin AMP\n",
    "            loss.backward()\n",
    "\n",
    "            # Verificar si los gradientes son finitos\n",
    "            grads_finite = True\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                    grads_finite = False\n",
    "                    print(f\"‚ö†Ô∏è Batch {batch_idx}: NaN o Inf en gradientes en {name}. Saltando actualizaci√≥n.\")\n",
    "                    break\n",
    "\n",
    "            if grads_finite:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "        print_memory_usage(epoch=epoch+1)\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        auc_roc, avg_precision, mapk_score, f1 = evaluate(model, val_loader)\n",
    "        print(f\"Validation AUC-ROC: {auc_roc:.4f}, Average Precision: {avg_precision:.4f}, MAP@5: {mapk_score:.4f}\")\n",
    "        val_ndcg = evaluate_ndcg(model, val_loader)\n",
    "        print(f\"Validation NDCG: {val_ndcg:.4f}\")\n",
    "        \n",
    "        if val_ndcg > best_ndcg:\n",
    "            best_ndcg = val_ndcg\n",
    "            model_filename = f\"best_model_epoch_{epoch+1}_ndcg_{val_ndcg:.4f}.pt\"\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(f\"‚úÖ Nuevo mejor modelo guardado: {model_filename}\")\n",
    "        else:\n",
    "            print(f\"No hay mejora en NDCG@5: {val_ndcg:.4f} <= {best_ndcg:.4f}\")\n",
    "\n",
    "    return model_filename\n",
    "\n",
    "\n",
    "# ‚úÖ 6Ô∏è‚É£ Cargar Dataset con Normalizaci√≥n y Manejo de Outliers\n",
    "MAX_SEQ_LENGTH = 50\n",
    "\n",
    "class IterableSessionDataset(IterableDataset):\n",
    "    def __init__(self, df_path, fraction=1.0, mode='train', balance=True, user_ids=None, train_user_ids=None):\n",
    "        self.df_path = df_path\n",
    "        self.feature_cols = FEATURE_COLS\n",
    "        self.fraction = fraction\n",
    "        self.mode = mode\n",
    "        self.balance = balance\n",
    "        self.user_ids = user_ids\n",
    "        self.train_user_ids = train_user_ids\n",
    "\n",
    "        # Cargar el DataFrame completo para calcular estad√≠sticas globales\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        num_row_groups = table.metadata.num_row_groups\n",
    "        dfs = [table.read_row_group(i).to_pandas() for i in range(num_row_groups)]\n",
    "        full_df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Verificar que train_user_ids est√° disponible\n",
    "        if self.train_user_ids is None:\n",
    "            raise ValueError(\"train_user_ids no est√° definido. Debes proporcionarlo al inicializar.\")\n",
    "        \n",
    "        # Calcular 'is_new_user' en full_df\n",
    "        full_df['is_new_user'] = full_df['user_id'].apply(lambda x: 0 if x in self.train_user_ids else 1)\n",
    "        \n",
    "        # Calcular 'has_session_history' en full_df\n",
    "        full_df['has_session_history'] = full_df.groupby('session_id').cumcount().apply(lambda x: 0 if x == 0 else 1)\n",
    "        \n",
    "        self.global_feature_means = full_df[self.feature_cols].mean()\n",
    "        self.global_feature_stds = full_df[self.feature_cols].std().replace(0, 1e-6)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            return self._data_iterator()\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            return self._data_iterator(worker_id, num_workers)\n",
    "        \n",
    "        \n",
    "    def _data_iterator(self, worker_id=0, num_workers=1):\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            row_groups = range(0, int(num_row_groups_to_use * 0.8))\n",
    "        else:\n",
    "            row_groups = range(int(num_row_groups_to_use * 0.8), num_row_groups_to_use)\n",
    "\n",
    "        for i in row_groups:\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "            \n",
    "            df_majority = df[df.add_to_cart == 0]\n",
    "            df_minority = df[df.add_to_cart == 1]\n",
    "            df_minority_oversampled = df_minority.sample(len(df_majority), replace=True, random_state=42)\n",
    "            df_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "            df = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            \n",
    "            # Filtrar por user_ids\n",
    "            if self.user_ids is not None:\n",
    "                df = df[df['user_id'].isin(self.user_ids)]\n",
    "                        \n",
    "            # A√±adir columna 'is_new_user'\n",
    "            df['is_new_user'] = df['user_id'].apply(lambda x: 0 if x in self.train_user_ids else 1)\n",
    "            # A√±adir columna 'has_session_history'\n",
    "            df['has_session_history'] = df.groupby('session_id').cumcount().apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "            # Aplicar transformaciones logar√≠tmicas\n",
    "            for col in ['time_since_last', 'session_duration']:\n",
    "                df[col] = np.log1p(df[col])\n",
    "\n",
    "            # Normalizaci√≥n global segura\n",
    "            df[self.feature_cols] = (df[self.feature_cols] - self.global_feature_means) / self.global_feature_stds\n",
    "\n",
    "            # Reemplazar posibles NaN resultantes de la normalizaci√≥n\n",
    "            df[self.feature_cols] = df[self.feature_cols].fillna(0.0)\n",
    "\n",
    "            # Continuar con el resto del procesamiento...\n",
    "            # if self.balance:\n",
    "            #     df_1 = df[df[\"add_to_cart\"] == 1]\n",
    "            #     df_0 = df[df[\"add_to_cart\"] == 0]\n",
    "            #     if len(df_1) > 0 and len(df_0) > 0:\n",
    "            #         if len(df_1) > len(df_0):\n",
    "            #             df_0 = df_0.sample(n=len(df_1), replace=True, random_state=42)\n",
    "            #         else:\n",
    "            #             df_1 = df_1.sample(n=len(df_0), replace=True, random_state=42)\n",
    "            #    df = pd.concat([df_0, df_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            for session_id, session_data in df.groupby(\"session_id\"):\n",
    "                product_embeddings = session_data[\"embedding_reduced\"].tolist()\n",
    "                partnumbers = session_data[\"partnumber\"].tolist()\n",
    "\n",
    "                # Verificar si hay NaN en embeddings\n",
    "                if any(embedding is None or np.isnan(embedding).any() for embedding in product_embeddings):\n",
    "                    continue\n",
    "\n",
    "                product_embeddings = torch.tensor(np.array(product_embeddings), dtype=torch.float32)\n",
    "\n",
    "                session_features = torch.tensor(session_data[self.feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "                # Verificar si hay NaN en features\n",
    "                if torch.isnan(session_features).any():\n",
    "                    continue\n",
    "\n",
    "                targets = torch.tensor(session_data[\"add_to_cart\"].values, dtype=torch.float32)\n",
    "                yield product_embeddings, session_features, targets, session_id, partnumbers\n",
    "                \n",
    "                \n",
    "# ‚úÖ Funci√≥n de Colaci√≥n\n",
    "def collate_fn(batch):\n",
    "    product_embeddings, session_features, targets, session_ids, partnumbers_list = zip(*batch)\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), torch.empty(0), torch.empty(0), [], []\n",
    "\n",
    "    max_len = min(max([x.shape[0] for x in product_embeddings]), MAX_SEQ_LENGTH)\n",
    "\n",
    "    padded_embeddings = torch.zeros((len(batch), max_len, product_embeddings[0].shape[1]))\n",
    "    padded_features = torch.zeros((len(batch), max_len, session_features[0].shape[1]))\n",
    "    padded_targets = torch.zeros((len(batch), max_len))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        seq_len = min(product_embeddings[i].shape[0], max_len)\n",
    "        padded_embeddings[i, :seq_len] = product_embeddings[i][:seq_len]\n",
    "        padded_features[i, :seq_len] = session_features[i][:seq_len]\n",
    "        padded_targets[i, :seq_len] = targets[i][:seq_len]\n",
    "\n",
    "    return padded_embeddings, padded_features, padded_targets, session_ids, partnumbers_list\n",
    "\n",
    "\n",
    "# ‚úÖ 7Ô∏è‚É£ Crear Datasets de Entrenamiento y Validaci√≥n\n",
    "data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "fraction = 0.05\n",
    "with open('train_user_ids.pkl', 'rb') as f:\n",
    "    train_user_ids = pickle.load(f)\n",
    "\n",
    "with open('val_user_ids.pkl', 'rb') as f:\n",
    "    val_user_ids = pickle.load(f)\n",
    "\n",
    "train_dataset = IterableSessionDataset(\n",
    "    data_path, fraction=fraction, mode='train', balance=True, user_ids=train_user_ids, train_user_ids=train_user_ids\n",
    ")\n",
    "val_dataset = IterableSessionDataset(\n",
    "    data_path, fraction=fraction, mode='val', balance=False, user_ids=val_user_ids, train_user_ids=train_user_ids\n",
    ")\n",
    "# train_dataset = IterableSessionDataset(data_path, fraction=fraction, mode='train')\n",
    "# val_dataset = IterableSessionDataset(data_path, fraction=fraction, mode='val')\n",
    "\n",
    "\n",
    "# ‚úÖ 8Ô∏è‚É£ Ejecutar Entrenamiento\n",
    "EPOCHS = 5\n",
    "print(f'\\nFraction of Dataset: {fraction*100}%\\n')\n",
    "print(f'--- N√∫mero de epochs: {EPOCHS} ---')\n",
    "best_model_file = train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE_DIM: 13\n",
      "Dispositivo: cuda\n",
      "\n",
      "==== Fold 1/5 ====\n",
      "--- Epoch 1/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.08 GB\n",
      "üî• GPU Usage: 0.00 GB\n",
      "üî• GPU Cached: 0.00 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.3782\n",
      "üñ•Ô∏è RAM Usage: 8.66 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 1\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 12898 muestras\n",
      "Clase 1: 76602 muestras\n",
      "F1-score: 0.2446, AUC-ROC: 0.6320\n",
      "Validation AUC-ROC: 0.6320, Average Precision: 0.4517, MAP@5: nan\n",
      "NDCG@5: 0.2051\n",
      "Validation NDCG: 0.2051\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_1_ndcg_0.2051.pt\n",
      "--- Epoch 2/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.72 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.3472\n",
      "üñ•Ô∏è RAM Usage: 8.72 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 2\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 54742 muestras\n",
      "Clase 1: 34758 muestras\n",
      "F1-score: 0.4746, AUC-ROC: 0.7374\n",
      "Validation AUC-ROC: 0.7374, Average Precision: 0.4333, MAP@5: nan\n",
      "NDCG@5: 0.2373\n",
      "Validation NDCG: 0.2373\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_2_ndcg_0.2373.pt\n",
      "--- Epoch 3/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.76 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.3003\n",
      "üñ•Ô∏è RAM Usage: 8.76 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 3\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 57913 muestras\n",
      "Clase 1: 31587 muestras\n",
      "F1-score: 0.4898, AUC-ROC: 0.7643\n",
      "Validation AUC-ROC: 0.7643, Average Precision: 0.4496, MAP@5: nan\n",
      "NDCG@5: 0.2421\n",
      "Validation NDCG: 0.2421\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_3_ndcg_0.2421.pt\n",
      "--- Epoch 4/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.76 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.2763\n",
      "üñ•Ô∏è RAM Usage: 8.78 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 4\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 60707 muestras\n",
      "Clase 1: 28793 muestras\n",
      "F1-score: 0.4970, AUC-ROC: 0.7939\n",
      "Validation AUC-ROC: 0.7939, Average Precision: 0.4930, MAP@5: nan\n",
      "NDCG@5: 0.2449\n",
      "Validation NDCG: 0.2449\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_4_ndcg_0.2449.pt\n",
      "--- Epoch 5/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.80 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.2558\n",
      "üñ•Ô∏è RAM Usage: 8.80 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 5\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 61415 muestras\n",
      "Clase 1: 28085 muestras\n",
      "F1-score: 0.4907, AUC-ROC: 0.8072\n",
      "Validation AUC-ROC: 0.8072, Average Precision: 0.5140, MAP@5: nan\n",
      "NDCG@5: 0.2462\n",
      "Validation NDCG: 0.2462\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_5_ndcg_0.2462.pt\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 61415 muestras\n",
      "Clase 1: 28085 muestras\n",
      "F1-score: 0.4907, AUC-ROC: 0.8072\n",
      "NDCG@5: 0.2462\n",
      "\n",
      "==== Fold 2/5 ====\n",
      "--- Epoch 1/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.80 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.3779\n",
      "üñ•Ô∏è RAM Usage: 8.10 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 1\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 7165 muestras\n",
      "Clase 1: 87335 muestras\n",
      "F1-score: 0.2697, AUC-ROC: 0.6957\n",
      "Validation AUC-ROC: 0.6957, Average Precision: 0.4276, MAP@5: nan\n",
      "NDCG@5: 0.2087\n",
      "Validation NDCG: 0.2087\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_1_ndcg_0.2087.pt\n",
      "--- Epoch 2/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.12 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.3487\n",
      "üñ•Ô∏è RAM Usage: 8.15 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 2\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 51566 muestras\n",
      "Clase 1: 42934 muestras\n",
      "F1-score: 0.4488, AUC-ROC: 0.7305\n",
      "Validation AUC-ROC: 0.7305, Average Precision: 0.4013, MAP@5: nan\n",
      "NDCG@5: 0.2440\n",
      "Validation NDCG: 0.2440\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_2_ndcg_0.2440.pt\n",
      "--- Epoch 3/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.17 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.3035\n",
      "üñ•Ô∏è RAM Usage: 8.17 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 3\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 56879 muestras\n",
      "Clase 1: 37621 muestras\n",
      "F1-score: 0.4748, AUC-ROC: 0.7580\n",
      "Validation AUC-ROC: 0.7580, Average Precision: 0.4213, MAP@5: nan\n",
      "NDCG@5: 0.2501\n",
      "Validation NDCG: 0.2501\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_3_ndcg_0.2501.pt\n",
      "--- Epoch 4/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.17 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.2811\n",
      "üñ•Ô∏è RAM Usage: 8.17 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 4\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 61022 muestras\n",
      "Clase 1: 33478 muestras\n",
      "F1-score: 0.4937, AUC-ROC: 0.7897\n",
      "Validation AUC-ROC: 0.7897, Average Precision: 0.4670, MAP@5: nan\n",
      "NDCG@5: 0.2580\n",
      "Validation NDCG: 0.2580\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_4_ndcg_0.2580.pt\n",
      "--- Epoch 5/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.17 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.2643\n",
      "üñ•Ô∏è RAM Usage: 8.18 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 5\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 62425 muestras\n",
      "Clase 1: 32075 muestras\n",
      "F1-score: 0.4953, AUC-ROC: 0.8058\n",
      "Validation AUC-ROC: 0.8058, Average Precision: 0.4918, MAP@5: nan\n",
      "NDCG@5: 0.2583\n",
      "Validation NDCG: 0.2583\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_5_ndcg_0.2583.pt\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 62425 muestras\n",
      "Clase 1: 32075 muestras\n",
      "F1-score: 0.4953, AUC-ROC: 0.8058\n",
      "NDCG@5: 0.2583\n",
      "\n",
      "==== Fold 3/5 ====\n",
      "--- Epoch 1/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.61 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.7969\n",
      "üñ•Ô∏è RAM Usage: 7.82 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 1\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 554032 muestras\n",
      "Clase 1: 4040968 muestras\n",
      "F1-score: 0.1416, AUC-ROC: 0.6079\n",
      "Validation AUC-ROC: 0.6079, Average Precision: 0.3660, MAP@5: nan\n",
      "NDCG@5: 0.1158\n",
      "Validation NDCG: 0.1158\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_1_ndcg_0.1158.pt\n",
      "--- Epoch 2/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.87 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.7947\n",
      "üñ•Ô∏è RAM Usage: 7.87 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 2\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 429943 muestras\n",
      "Clase 1: 4165057 muestras\n",
      "F1-score: 0.1490, AUC-ROC: 0.6353\n",
      "Validation AUC-ROC: 0.6353, Average Precision: 0.3922, MAP@5: nan\n",
      "NDCG@5: 0.1171\n",
      "Validation NDCG: 0.1171\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_2_ndcg_0.1171.pt\n",
      "--- Epoch 3/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.83 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.7930\n",
      "üñ•Ô∏è RAM Usage: 7.83 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 3\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 384864 muestras\n",
      "Clase 1: 4210136 muestras\n",
      "F1-score: 0.1522, AUC-ROC: 0.6473\n",
      "Validation AUC-ROC: 0.6473, Average Precision: 0.4033, MAP@5: nan\n",
      "NDCG@5: 0.1175\n",
      "Validation NDCG: 0.1175\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_3_ndcg_0.1175.pt\n",
      "--- Epoch 4/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.86 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.7918\n",
      "üñ•Ô∏è RAM Usage: 7.86 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 4\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 346899 muestras\n",
      "Clase 1: 4248101 muestras\n",
      "F1-score: 0.1552, AUC-ROC: 0.6585\n",
      "Validation AUC-ROC: 0.6585, Average Precision: 0.4130, MAP@5: nan\n",
      "NDCG@5: 0.1180\n",
      "Validation NDCG: 0.1180\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_4_ndcg_0.1180.pt\n",
      "--- Epoch 5/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.87 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.7910\n",
      "üñ•Ô∏è RAM Usage: 7.89 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 5\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 330405 muestras\n",
      "Clase 1: 4264595 muestras\n",
      "F1-score: 0.1566, AUC-ROC: 0.6635\n",
      "Validation AUC-ROC: 0.6635, Average Precision: 0.4172, MAP@5: nan\n",
      "NDCG@5: 0.1182\n",
      "Validation NDCG: 0.1182\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_5_ndcg_0.1182.pt\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 330405 muestras\n",
      "Clase 1: 4264595 muestras\n",
      "F1-score: 0.1566, AUC-ROC: 0.6635\n",
      "NDCG@5: 0.1182\n",
      "\n",
      "==== Fold 4/5 ====\n",
      "--- Epoch 1/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.86 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.3789\n",
      "üñ•Ô∏è RAM Usage: 8.15 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 1\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 4330 muestras\n",
      "Clase 1: 86070 muestras\n",
      "F1-score: 0.2963, AUC-ROC: 0.8054\n",
      "Validation AUC-ROC: 0.8054, Average Precision: 0.4877, MAP@5: nan\n",
      "NDCG@5: 0.2250\n",
      "Validation NDCG: 0.2250\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_1_ndcg_0.2250.pt\n",
      "--- Epoch 2/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.18 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.3530\n",
      "üñ•Ô∏è RAM Usage: 8.15 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 2\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 51930 muestras\n",
      "Clase 1: 38470 muestras\n",
      "F1-score: 0.5009, AUC-ROC: 0.7853\n",
      "Validation AUC-ROC: 0.7853, Average Precision: 0.4372, MAP@5: nan\n",
      "NDCG@5: 0.2459\n",
      "Validation NDCG: 0.2459\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_2_ndcg_0.2459.pt\n",
      "--- Epoch 3/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.16 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.3033\n",
      "üñ•Ô∏è RAM Usage: 8.17 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 3\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 54456 muestras\n",
      "Clase 1: 35944 muestras\n",
      "F1-score: 0.4989, AUC-ROC: 0.8090\n",
      "Validation AUC-ROC: 0.8090, Average Precision: 0.4548, MAP@5: nan\n",
      "NDCG@5: 0.2483\n",
      "Validation NDCG: 0.2483\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_3_ndcg_0.2483.pt\n",
      "--- Epoch 4/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.17 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.2719\n",
      "üñ•Ô∏è RAM Usage: 8.18 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 4\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 55633 muestras\n",
      "Clase 1: 34767 muestras\n",
      "F1-score: 0.4904, AUC-ROC: 0.8317\n",
      "Validation AUC-ROC: 0.8317, Average Precision: 0.5014, MAP@5: nan\n",
      "NDCG@5: 0.2535\n",
      "Validation NDCG: 0.2535\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_4_ndcg_0.2535.pt\n",
      "--- Epoch 5/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.14 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.2444\n",
      "üñ•Ô∏è RAM Usage: 8.15 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 5\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 55440 muestras\n",
      "Clase 1: 34960 muestras\n",
      "F1-score: 0.4810, AUC-ROC: 0.8332\n",
      "Validation AUC-ROC: 0.8332, Average Precision: 0.5139, MAP@5: nan\n",
      "NDCG@5: 0.2546\n",
      "Validation NDCG: 0.2546\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_5_ndcg_0.2546.pt\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 55440 muestras\n",
      "Clase 1: 34960 muestras\n",
      "F1-score: 0.4810, AUC-ROC: 0.8332\n",
      "NDCG@5: 0.2546\n",
      "\n",
      "==== Fold 5/5 ====\n",
      "--- Epoch 1/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.73 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 1.3777\n",
      "üñ•Ô∏è RAM Usage: 7.84 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 1\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 9014 muestras\n",
      "Clase 1: 79786 muestras\n",
      "F1-score: 0.2694, AUC-ROC: 0.7209\n",
      "Validation AUC-ROC: 0.7209, Average Precision: 0.4651, MAP@5: nan\n",
      "NDCG@5: 0.1931\n",
      "Validation NDCG: 0.1931\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_1_ndcg_0.1931.pt\n",
      "--- Epoch 2/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.89 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 1.3540\n",
      "üñ•Ô∏è RAM Usage: 7.90 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 2\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 51837 muestras\n",
      "Clase 1: 36963 muestras\n",
      "F1-score: 0.4912, AUC-ROC: 0.7725\n",
      "Validation AUC-ROC: 0.7725, Average Precision: 0.4401, MAP@5: nan\n",
      "NDCG@5: 0.2306\n",
      "Validation NDCG: 0.2306\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_2_ndcg_0.2306.pt\n",
      "--- Epoch 3/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.79 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 1.3126\n",
      "üñ•Ô∏è RAM Usage: 7.80 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 3\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 54464 muestras\n",
      "Clase 1: 34336 muestras\n",
      "F1-score: 0.5074, AUC-ROC: 0.7987\n",
      "Validation AUC-ROC: 0.7987, Average Precision: 0.4524, MAP@5: nan\n",
      "NDCG@5: 0.2386\n",
      "Validation NDCG: 0.2386\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_3_ndcg_0.2386.pt\n",
      "--- Epoch 4/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.80 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [4/5], Loss: 1.2905\n",
      "üñ•Ô∏è RAM Usage: 7.80 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 4\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 56640 muestras\n",
      "Clase 1: 32160 muestras\n",
      "F1-score: 0.5187, AUC-ROC: 0.8240\n",
      "Validation AUC-ROC: 0.8240, Average Precision: 0.4889, MAP@5: nan\n",
      "NDCG@5: 0.2417\n",
      "Validation NDCG: 0.2417\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_4_ndcg_0.2417.pt\n",
      "--- Epoch 5/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.82 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [5/5], Loss: 1.2728\n",
      "üñ•Ô∏è RAM Usage: 7.83 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.38 GB\n",
      "üìä Memoria despu√©s del Epoch 5\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 57197 muestras\n",
      "Clase 1: 31603 muestras\n",
      "F1-score: 0.5183, AUC-ROC: 0.8363\n",
      "Validation AUC-ROC: 0.8363, Average Precision: 0.5131, MAP@5: nan\n",
      "NDCG@5: 0.2432\n",
      "Validation NDCG: 0.2432\n",
      "‚úÖ Nuevo mejor modelo guardado: CV_best_model_epoch_5_ndcg_0.2432.pt\n",
      "Distribuci√≥n de las predicciones del modelo:\n",
      "Clase 0: 57197 muestras\n",
      "Clase 1: 31603 muestras\n",
      "F1-score: 0.5183, AUC-ROC: 0.8363\n",
      "NDCG@5: 0.2432\n",
      "\n",
      "Promedio de m√©tricas en 5-fold cross-validation:\n",
      "AUC-ROC: 0.7892, Average Precision: 0.4900, MAP@5: nan, F1-score: 0.4284, NDCG@5: 0.2241\n"
     ]
    }
   ],
   "source": [
    "# Importar las librer√≠as necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import psutil\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from torch.utils.data import get_worker_info\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import json\n",
    "from sklearn.metrics import ndcg_score\n",
    "import pickle\n",
    "\n",
    "# Configuraci√≥n global de caracter√≠sticas\n",
    "FEATURE_COLS = [\"time_since_last\", \"session_relative_position\", \"session_duration\",\n",
    "                \"R\", \"F\", \"M\", \"device_type\", \"pagetype\", \"discount\", \"cod_section\", \"family\",\n",
    "                \"is_new_user\", \"has_session_history\"]\n",
    "\n",
    "FEATURE_DIM = len(FEATURE_COLS)\n",
    "print(f\"FEATURE_DIM: {FEATURE_DIM}\")\n",
    "\n",
    "# ‚úÖ 1Ô∏è‚É£ Configuraci√≥n del Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# üî• Funci√≥n para Monitoreo de Memoria\n",
    "def print_memory_usage(epoch=None):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"üñ•Ô∏è RAM Usage: {mem_info.rss / 1e9:.2f} GB\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üî• GPU Usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"üî• GPU Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    if epoch is not None:\n",
    "        print(f\"üìä Memoria despu√©s del Epoch {epoch}\")\n",
    "\n",
    "# ‚úÖ Implementaci√≥n de Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = torch.tensor(1.0, dtype=torch.float32, device=device) if pos_weight is None else pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # ‚úÖ Clamping para evitar valores extremos\n",
    "        inputs = torch.clamp(inputs, min=-10, max=10)\n",
    "\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, pos_weight=self.pos_weight, reduction='none'\n",
    "        )\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        pt = targets * probas + (1 - targets) * (1 - probas)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        loss = self.alpha * focal_term * bce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# ‚úÖ 2Ô∏è‚É£ Definir Modelo Ajustado con Dropout y Regularizaci√≥n\n",
    "# ‚úÖ Modelo GRU con Atenci√≥n\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attn_weights = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, gru_output):\n",
    "        \"\"\"\n",
    "        gru_output: Tensor de tama√±o (batch, seq_len, hidden_dim)\n",
    "        Devuelve:\n",
    "        - Scores de cada producto en la sesi√≥n (batch, seq_len)\n",
    "        \"\"\"\n",
    "        attn_scores = self.attn_weights(gru_output).squeeze(-1)  # (batch, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)  # Normalizamos\n",
    "        return attn_weights\n",
    "\n",
    "class GRURecommender(nn.Module):\n",
    "    def __init__(self, input_dim=50, feature_dim=FEATURE_DIM, hidden_dim=128, num_layers=2, output_dim=1):\n",
    "        super(GRURecommender, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim + feature_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.attention = AttentionLayer(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Inicializaci√≥n de pesos\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.GRU):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, product_embeddings, session_features):\n",
    "        x = torch.cat((product_embeddings, session_features), dim=-1)\n",
    "        gru_output, _ = self.gru(x)\n",
    "        attn_weights = self.attention(gru_output)\n",
    "        weighted_output = gru_output * attn_weights.unsqueeze(-1)\n",
    "        scores = self.fc(self.dropout(weighted_output)).squeeze(-1)\n",
    "        return scores\n",
    "\n",
    "# ‚úÖ 3Ô∏è‚É£ Configuraci√≥n de p√©rdida, optimizador y scheduler se har√° dentro de cada fold m√°s adelante\n",
    "\n",
    "# ‚úÖ 4Ô∏è‚É£ Definir Funciones de Evaluaci√≥n con M√©tricas Adicionales\n",
    "def evaluate(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device)\n",
    "            session_features = session_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(product_embeddings, session_features)  # (batch, seq_len)\n",
    "            outputs = torch.sigmoid(outputs)  # Convertimos logits a probabilidades\n",
    "\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())\n",
    "            all_outputs.extend(outputs.cpu().numpy().flatten())\n",
    "\n",
    "    # ‚úÖ Convertir listas a arrays de NumPy\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_outputs = np.array(all_outputs)\n",
    "\n",
    "    # ‚úÖ Filtrar NaN antes de calcular m√©tricas\n",
    "    mask = ~np.isnan(all_targets) & ~np.isnan(all_outputs)\n",
    "    all_targets = all_targets[mask]\n",
    "    all_outputs = all_outputs[mask]\n",
    "\n",
    "    if len(all_targets) == 0:\n",
    "        print(\"‚ö†Ô∏è No hay datos v√°lidos para calcular m√©tricas.\")\n",
    "        return float('nan'), float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    # Ahora calculamos las predicciones y la distribuci√≥n\n",
    "    predicted_classes = (all_outputs >= threshold).astype(int)\n",
    "    unique_classes, counts = np.unique(predicted_classes, return_counts=True)\n",
    "    print(\"Distribuci√≥n de las predicciones del modelo:\")\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        print(f\"Clase {cls}: {count} muestras\")\n",
    "\n",
    "    # Calculamos las m√©tricas\n",
    "    auc_roc = roc_auc_score(all_targets, all_outputs)\n",
    "    average_precision = average_precision_score(all_targets, all_outputs)\n",
    "    f1 = f1_score(all_targets, predicted_classes, zero_division=0)\n",
    "\n",
    "    print(f\"F1-score: {f1:.4f}, AUC-ROC: {auc_roc:.4f}\")\n",
    "    return auc_roc, average_precision, float('nan'), f1\n",
    "\n",
    "def evaluate_ndcg(model, data_loader, k=5):\n",
    "    model.eval()\n",
    "    ndcg_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            product_embeddings, session_features, targets, session_ids, partnumbers_list = batch\n",
    "            product_embeddings = product_embeddings.to(device)\n",
    "            session_features = session_features.to(device)\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "\n",
    "            for output, target in zip(outputs, targets):\n",
    "                # Aseg√∫rate de que output y target est√°n alineados y son arrays numpy\n",
    "                ndcg = ndcg_score([target], [output], k=k)\n",
    "                ndcg_scores.append(ndcg)\n",
    "\n",
    "    avg_ndcg = np.mean(ndcg_scores)\n",
    "    print(f\"NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "    return avg_ndcg\n",
    "\n",
    "# ‚úÖ 5Ô∏è‚É£ Funci√≥n de Entrenamiento con Persistencia del Mejor Modelo\n",
    "def train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, epochs):\n",
    "    best_ndcg = 0.0\n",
    "    model_filename = None  # Para almacenar el nombre del mejor modelo de este fold\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,  # No es necesario shuffle con IterableDataset\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "\n",
    "        print_memory_usage(epoch=\"Inicio\")\n",
    "        print(\"Iniciando el bucle de batches...\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            # üö® Verificar si hay NaN o Inf en las entradas\n",
    "            if torch.isnan(product_embeddings).any() or torch.isinf(product_embeddings).any():\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` o `Inf` en `product_embeddings`. Saltando batch...\")\n",
    "                continue\n",
    "            if torch.isnan(session_features).any() or torch.isinf(session_features).any():\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` o `Inf` en `session_features`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(product_embeddings, session_features)\n",
    "\n",
    "            # üö® Verificar valores extremos en `outputs`\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` o `Inf` en `outputs`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            targets = targets[:, :outputs.shape[1]]\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # ‚úÖ Verificar si la p√©rdida es infinita o NaN antes de `backward()`\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` o `Inf` en `loss`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Verificar si los gradientes son finitos\n",
    "            grads_finite = True\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                    grads_finite = False\n",
    "                    print(f\"‚ö†Ô∏è Batch {batch_idx}: NaN o Inf en gradientes en {name}. Saltando actualizaci√≥n.\")\n",
    "                    break\n",
    "\n",
    "            if grads_finite:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "        print_memory_usage(epoch=epoch+1)\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        auc_roc, avg_precision, mapk_score, f1 = evaluate(model, val_loader)\n",
    "        print(f\"Validation AUC-ROC: {auc_roc:.4f}, Average Precision: {avg_precision:.4f}, MAP@5: {mapk_score:.4f}\")\n",
    "        val_ndcg = evaluate_ndcg(model, val_loader)\n",
    "        print(f\"Validation NDCG: {val_ndcg:.4f}\")\n",
    "\n",
    "        if val_ndcg > best_ndcg:\n",
    "            best_ndcg = val_ndcg\n",
    "            model_filename = f\"CV_best_model_epoch_{epoch+1}_ndcg_{val_ndcg:.4f}.pt\"\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(f\"‚úÖ Nuevo mejor modelo guardado: {model_filename}\")\n",
    "        else:\n",
    "            print(f\"No hay mejora en NDCG@5: {val_ndcg:.4f} <= {best_ndcg:.4f}\")\n",
    "\n",
    "    return model_filename\n",
    "\n",
    "# ‚úÖ 6Ô∏è‚É£ Cargar Dataset con Normalizaci√≥n y Manejo de Outliers\n",
    "MAX_SEQ_LENGTH = 50\n",
    "\n",
    "class IterableSessionDataset(IterableDataset):\n",
    "    def __init__(self, df_path, fraction=1.0, mode='train', balance=True, user_ids=None, train_user_ids=None):\n",
    "        self.df_path = df_path\n",
    "        self.feature_cols = FEATURE_COLS\n",
    "        self.fraction = fraction\n",
    "        self.mode = mode\n",
    "        self.balance = balance\n",
    "        self.user_ids = user_ids\n",
    "        self.train_user_ids = train_user_ids\n",
    "\n",
    "        # Cargar el DataFrame completo para calcular estad√≠sticas globales de entrenamiento\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        num_row_groups = table.metadata.num_row_groups\n",
    "        dfs = [table.read_row_group(i).to_pandas() for i in range(num_row_groups)]\n",
    "        full_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Verificar que train_user_ids est√° disponible\n",
    "        if self.train_user_ids is None:\n",
    "            raise ValueError(\"train_user_ids no est√° definido. Debes proporcionarlo al inicializar.\")\n",
    "\n",
    "        # Calcular 'is_new_user' en full_df\n",
    "        full_df['is_new_user'] = full_df['user_id'].apply(lambda x: 0 if x in self.train_user_ids else 1)\n",
    "\n",
    "        # Calcular 'has_session_history' en full_df\n",
    "        full_df['has_session_history'] = full_df.groupby('session_id').cumcount().apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "        self.global_feature_means = full_df[self.feature_cols].mean()\n",
    "        self.global_feature_stds = full_df[self.feature_cols].std().replace(0, 1e-6)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            return self._data_iterator()\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            return self._data_iterator(worker_id, num_workers)\n",
    "\n",
    "    def _data_iterator(self, worker_id=0, num_workers=1):\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            row_groups = range(0, int(num_row_groups_to_use * 0.8))\n",
    "        else:\n",
    "            row_groups = range(int(num_row_groups_to_use * 0.8), num_row_groups_to_use)\n",
    "\n",
    "        for i in row_groups:\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "\n",
    "            # Balanceo del dataset\n",
    "            df_majority = df[df.add_to_cart == 0]\n",
    "            df_minority = df[df.add_to_cart == 1]\n",
    "            if len(df_minority) > 0:\n",
    "                df_minority_oversampled = df_minority.sample(len(df_majority), replace=True, random_state=42)\n",
    "                df_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "                df = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            # Filtrar por user_ids si se proporcionan\n",
    "            if self.user_ids is not None:\n",
    "                df = df[df['user_id'].isin(self.user_ids)]\n",
    "\n",
    "            # A√±adir columna 'is_new_user'\n",
    "            df['is_new_user'] = df['user_id'].apply(lambda x: 0 if x in self.train_user_ids else 1)\n",
    "            # A√±adir columna 'has_session_history'\n",
    "            df['has_session_history'] = df.groupby('session_id').cumcount().apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "            # Aplicar transformaciones logar√≠tmicas\n",
    "            for col in ['time_since_last', 'session_duration']:\n",
    "                df[col] = np.log1p(df[col])\n",
    "\n",
    "            # Normalizaci√≥n global segura\n",
    "            df[self.feature_cols] = (df[self.feature_cols] - self.global_feature_means) / self.global_feature_stds\n",
    "\n",
    "            # Reemplazar posibles NaN resultantes de la normalizaci√≥n\n",
    "            df[self.feature_cols] = df[self.feature_cols].fillna(0.0)\n",
    "\n",
    "            for session_id, session_data in df.groupby(\"session_id\"):\n",
    "                product_embeddings = session_data[\"embedding_reduced\"].tolist()\n",
    "                partnumbers = session_data[\"partnumber\"].tolist()\n",
    "\n",
    "                # Verificar si hay NaN en embeddings\n",
    "                if any(embedding is None or np.isnan(embedding).any() for embedding in product_embeddings):\n",
    "                    continue\n",
    "\n",
    "                product_embeddings = torch.tensor(np.array(product_embeddings), dtype=torch.float32)\n",
    "\n",
    "                session_features = torch.tensor(session_data[self.feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "                # Verificar si hay NaN en features\n",
    "                if torch.isnan(session_features).any():\n",
    "                    continue\n",
    "\n",
    "                targets = torch.tensor(session_data[\"add_to_cart\"].values, dtype=torch.float32)\n",
    "                yield product_embeddings, session_features, targets, session_id, partnumbers\n",
    "\n",
    "\n",
    "# ‚úÖ Funci√≥n de Colaci√≥n\n",
    "def collate_fn(batch):\n",
    "    product_embeddings, session_features, targets, session_ids, partnumbers_list = zip(*batch)\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), torch.empty(0), torch.empty(0), [], []\n",
    "\n",
    "    max_len = min(max([x.shape[0] for x in product_embeddings]), MAX_SEQ_LENGTH)\n",
    "\n",
    "    padded_embeddings = torch.zeros((len(batch), max_len, product_embeddings[0].shape[1]))\n",
    "    padded_features = torch.zeros((len(batch), max_len, session_features[0].shape[1]))\n",
    "    padded_targets = torch.zeros((len(batch), max_len))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        seq_len = min(product_embeddings[i].shape[0], max_len)\n",
    "        padded_embeddings[i, :seq_len] = product_embeddings[i][:seq_len]\n",
    "        padded_features[i, :seq_len] = session_features[i][:seq_len]\n",
    "        padded_targets[i, :seq_len] = targets[i][:seq_len]\n",
    "\n",
    "    return padded_embeddings, padded_features, padded_targets, session_ids, partnumbers_list\n",
    "\n",
    "# ‚úÖ 7Ô∏è‚É£ Crear Datasets y preparaci√≥n para validaci√≥n cruzada\n",
    "data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "fraction = 0.05\n",
    "\n",
    "with open('train_user_ids.pkl', 'rb') as f:\n",
    "    train_user_ids = pickle.load(f)\n",
    "\n",
    "with open('val_user_ids.pkl', 'rb') as f:\n",
    "    val_user_ids = pickle.load(f)\n",
    "\n",
    "# ‚úÖ 8Ô∏è‚É£ Implementaci√≥n de K-Fold Cross-Validation\n",
    "EPOCHS = 5\n",
    "k = 5  # N√∫mero de folds para cross-validation\n",
    "\n",
    "# Mezclar y dividir train_user_ids en k folds\n",
    "train_user_ids = list(train_user_ids)  # Convertir set a lista\n",
    "np.random.shuffle(train_user_ids)\n",
    "folds = np.array_split(train_user_ids, k)\n",
    "\n",
    "all_fold_metrics = []\n",
    "\n",
    "for fold in range(k):\n",
    "    print(f\"\\n==== Fold {fold+1}/{k} ====\")\n",
    "\n",
    "    # Definir IDs de usuario para este fold\n",
    "    val_fold_ids = folds[fold]\n",
    "    # Combinar los folds restantes para formar el conjunto de entrenamiento\n",
    "    train_fold_ids = np.concatenate([folds[i] for i in range(k) if i != fold])\n",
    "    \n",
    "    # Crear datasets para este fold\n",
    "    train_dataset = IterableSessionDataset(\n",
    "        data_path, fraction=fraction, mode='train', balance=True,\n",
    "        user_ids=train_fold_ids, train_user_ids=train_fold_ids\n",
    "    )\n",
    "    val_dataset = IterableSessionDataset(\n",
    "        data_path, fraction=fraction, mode='val', balance=False,\n",
    "        user_ids=val_fold_ids, train_user_ids=train_fold_ids\n",
    "    )\n",
    "    \n",
    "    # Re-inicializar modelo, criterio, optimizador y scheduler para cada fold\n",
    "    model = GRURecommender(input_dim=50, feature_dim=FEATURE_DIM, hidden_dim=128, num_layers=2, output_dim=1).to(device)\n",
    "    pos_weight_value = 10\n",
    "    pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32, device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "    \n",
    "    # Entrenar el modelo para este fold\n",
    "    best_model_file = train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, EPOCHS)\n",
    "    \n",
    "    # Crear DataLoader para evaluaci√≥n en el fold actual\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Evaluar el rendimiento en el conjunto de validaci√≥n para este fold\n",
    "    auc_roc, avg_precision, mapk_score, f1 = evaluate(model, val_loader)\n",
    "    val_ndcg = evaluate_ndcg(model, val_loader)\n",
    "    \n",
    "    # Almacenar m√©tricas del fold actual\n",
    "    all_fold_metrics.append((auc_roc, avg_precision, mapk_score, f1, val_ndcg))\n",
    "\n",
    "# Convertir resultados a array de NumPy para calcular promedios\n",
    "all_fold_metrics = np.array(all_fold_metrics)\n",
    "avg_metrics = np.mean(all_fold_metrics, axis=0)\n",
    "\n",
    "print(f\"\\nPromedio de m√©tricas en {k}-fold cross-validation:\")\n",
    "print(f\"AUC-ROC: {avg_metrics[0]:.4f}, Average Precision: {avg_metrics[1]:.4f}, \"\n",
    "      f\"MAP@5: {avg_metrics[2]:.4f}, F1-score: {avg_metrics[3]:.4f}, NDCG@5: {avg_metrics[4]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_product_attributes(products_file_path):\n",
    "    df_products = pd.read_parquet(products_file_path)\n",
    "    product_attributes = {}\n",
    "    for idx, row in df_products.iterrows():\n",
    "        partnumber = int(row['partnumber'])\n",
    "        product_attributes[partnumber] = {\n",
    "            'cod_section': row.get('cod_section', None),\n",
    "            'family': row.get('family', None),\n",
    "            # A√±adir m√°s atributos si es necesario\n",
    "        }\n",
    "    return product_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableSessionDataset(IterableDataset):\n",
    "    def __init__(self, df_path, fraction=1.0, mode='train', balance=True):\n",
    "        self.df_path = df_path\n",
    "        self.feature_cols = FEATURE_COLS\n",
    "        self.fraction = fraction\n",
    "        self.mode = mode\n",
    "        self.balance = balance  # Solo aplica para entrenamiento\n",
    "        # Cargar estad√≠sticas globales para normalizaci√≥n\n",
    "        self.global_feature_means = None\n",
    "        self.global_feature_stds = None\n",
    "        self._compute_global_statistics()\n",
    "\n",
    "    def _compute_global_statistics(self):\n",
    "        # Cargar una muestra del dataset para calcular la media y desviaci√≥n est√°ndar\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        dfs = []\n",
    "        for i in range(num_row_groups_to_use):\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "            dfs.append(df)\n",
    "\n",
    "        full_df = pd.concat(dfs)\n",
    "        # Determinar columnas disponibles entre las caracter√≠sticas esperadas\n",
    "        available_cols = [col for col in self.feature_cols if col in full_df.columns]\n",
    "        self.global_feature_means = full_df[available_cols].mean()\n",
    "        self.global_feature_stds = full_df[available_cols].std().replace(0, 1e-6)\n",
    "\n",
    "        # Asignar valores por defecto para columnas faltantes\n",
    "        for col in self.feature_cols:\n",
    "            if col not in available_cols:\n",
    "                self.global_feature_means[col] = 0.0\n",
    "                self.global_feature_stds[col] = 1.0\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Devuelve un iterador sobre los datos. \"\"\"\n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            return self._data_iterator()\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            return self._data_iterator(worker_id, num_workers)\n",
    "\n",
    "    def _data_iterator(self, worker_id=0, num_workers=1):\n",
    "        \"\"\" Iterador interno que carga los datos desde Parquet. \"\"\"\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            row_groups = range(0, int(num_row_groups_to_use * 0.8))\n",
    "        elif self.mode == 'val':\n",
    "            row_groups = range(int(num_row_groups_to_use * 0.8), num_row_groups_to_use)\n",
    "        elif self.mode == 'test':\n",
    "            row_groups = range(0, num_row_groups_to_use)\n",
    "        else:\n",
    "            raise ValueError(\"Modo inv√°lido. Use 'train', 'val' o 'test'.\")\n",
    "\n",
    "        for i in row_groups:\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "\n",
    "            # Asegurar que todas las columnas de FEATURE_COLS est√©n presentes\n",
    "            for col in self.feature_cols:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = 0.0\n",
    "\n",
    "            # Aplicar transformaciones logar√≠tmicas\n",
    "            for col in ['time_since_last', 'session_duration']:\n",
    "                df[col] = np.log1p(df[col])\n",
    "\n",
    "            # Normalizaci√≥n global segura\n",
    "            df[self.feature_cols] = (df[self.feature_cols] - self.global_feature_means) / self.global_feature_stds\n",
    "            df[self.feature_cols] = df[self.feature_cols].fillna(0.0)\n",
    "\n",
    "            # Para el modo 'train' y 'val', podemos balancear los datos\n",
    "            if self.mode in ['train', 'val'] and self.balance:\n",
    "                df_1 = df[df[\"add_to_cart\"] == 1]\n",
    "                df_0 = df[df[\"add_to_cart\"] == 0]\n",
    "                if len(df_1) > 0 and len(df_0) > 0:\n",
    "                    if len(df_1) > len(df_0):\n",
    "                        df_0 = df_0.sample(n=len(df_1), replace=True, random_state=42)\n",
    "                    else:\n",
    "                        df_1 = df_1.sample(n=len(df_0), replace=True, random_state=42)\n",
    "                    df = pd.concat([df_0, df_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            for session_id, session_data in df.groupby(\"session_id\"):\n",
    "                product_embeddings = session_data[\"embedding_reduced\"].tolist()\n",
    "                partnumbers = session_data[\"partnumber\"].tolist()\n",
    "\n",
    "                # Verificar si hay NaN en embeddings\n",
    "                if any(embedding is None or np.isnan(embedding).any() for embedding in product_embeddings):\n",
    "                    continue\n",
    "\n",
    "                product_embeddings = torch.tensor(np.array(product_embeddings), dtype=torch.float32)\n",
    "                session_features = torch.tensor(session_data[self.feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "                # Verificar si hay NaN en features\n",
    "                if torch.isnan(session_features).any():\n",
    "                    continue\n",
    "\n",
    "                if self.mode in ['train', 'val']:\n",
    "                    targets = torch.tensor(session_data[\"add_to_cart\"].values, dtype=torch.float32)\n",
    "                else:\n",
    "                    targets = None  # No hay targets en el conjunto de prueba\n",
    "\n",
    "                yield product_embeddings, session_features, targets, session_id, partnumbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    product_embeddings, session_features, targets, session_ids, partnumbers_list = zip(*batch)\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), torch.empty(0), None, [], []\n",
    "\n",
    "    max_len = min(max([x.shape[0] for x in product_embeddings]), MAX_SEQ_LENGTH)\n",
    "\n",
    "    padded_embeddings = torch.zeros((len(batch), max_len, product_embeddings[0].shape[1]))\n",
    "    padded_features = torch.zeros((len(batch), max_len, session_features[0].shape[1]))\n",
    "\n",
    "    if targets[0] is not None:\n",
    "        padded_targets = torch.zeros((len(batch), max_len))\n",
    "    else:\n",
    "        padded_targets = None  # No hay targets en el conjunto de prueba\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        seq_len = min(product_embeddings[i].shape[0], max_len)\n",
    "        padded_embeddings[i, :seq_len] = product_embeddings[i][:seq_len]\n",
    "        padded_features[i, :seq_len] = session_features[i][:seq_len]\n",
    "        if targets[0] is not None:\n",
    "            padded_targets[i, :seq_len] = targets[i][:seq_len]\n",
    "\n",
    "    return padded_embeddings, padded_features, padded_targets, session_ids, partnumbers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los atributos de los productos\n",
    "product_attributes = load_product_attributes(\"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/optimized_products_transformed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_v2(model, test_dataset, device, top_k=5):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    recommendations = {}\n",
    "    global_top_recommendations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            product_embeddings, session_features, _, session_ids, partnumbers_list = batch\n",
    "\n",
    "            if product_embeddings.shape[0] == 0:\n",
    "                continue  # Saltar batch vac√≠o\n",
    "\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(product_embeddings, session_features)  # (batch, seq_len)\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()  # Convertimos logits a probabilidades\n",
    "\n",
    "            for sid, parts, score_seq in zip(session_ids, partnumbers_list, scores):\n",
    "                # Asegurar que `parts` y `score_seq` tienen la misma longitud\n",
    "                if len(parts) != len(score_seq):\n",
    "                    min_len = min(len(parts), len(score_seq))\n",
    "                    parts = parts[:min_len]\n",
    "                    score_seq = score_seq[:min_len]\n",
    "\n",
    "                product_scores = list(zip(parts, score_seq))\n",
    "                product_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                top_recommendations = [int(p) for p, s in product_scores[:top_k]]\n",
    "                top_recommendations = list(dict.fromkeys(top_recommendations))\n",
    "\n",
    "                # Recopilar productos similares basados en atributos\n",
    "                if len(top_recommendations) < top_k:\n",
    "                    # Obtener atributos de los productos ya recomendados\n",
    "                    similar_candidates = []\n",
    "                    for rec in top_recommendations:\n",
    "                        rec_attrs = product_attributes.get(rec, {})\n",
    "                        # Buscar otros productos con atributos similares\n",
    "                        for p in parts:\n",
    "                            p = int(p)\n",
    "                            if p in top_recommendations:\n",
    "                                continue\n",
    "                            p_attrs = product_attributes.get(p, {})\n",
    "                            # Comparar atributos (por ejemplo, cod_section y family)\n",
    "                            if (p_attrs.get(\"cod_section\") == rec_attrs.get(\"cod_section\") and\n",
    "                                p_attrs.get(\"family\") == rec_attrs.get(\"family\")):\n",
    "                                similar_candidates.append(p)\n",
    "                    # A√±adir candidatos similares si es necesario\n",
    "                    for candidate in similar_candidates:\n",
    "                        if candidate not in top_recommendations and len(top_recommendations) < top_k:\n",
    "                            top_recommendations.append(candidate)\n",
    "\n",
    "                    # Acumular productos para fallback global si a√∫n faltan recomendaciones\n",
    "                    if len(top_recommendations) < top_k:\n",
    "                        global_top_recommendations.extend(\n",
    "                            [p for p in parts if p not in top_recommendations]\n",
    "                        )\n",
    "\n",
    "                recommendations[int(sid)] = top_recommendations\n",
    "\n",
    "    # Fallback global\n",
    "    product_counts = {}\n",
    "    for p in global_top_recommendations:\n",
    "        product_counts[p] = product_counts.get(p, 0) + 1\n",
    "    sorted_global_top = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    global_top = [p for p, count in sorted_global_top]\n",
    "\n",
    "    for sid, recs in recommendations.items():\n",
    "        if len(recs) < top_k:\n",
    "            additional = [p for p in global_top if p not in recs]\n",
    "            recs.extend(additional[:top_k - len(recs)])\n",
    "            recommendations[sid] = recs\n",
    "\n",
    "    output_json = {\n",
    "        \"target\": recommendations\n",
    "    }\n",
    "\n",
    "    with open(\"submission_v3.json\", \"w\") as f:\n",
    "        json.dump(output_json, f, indent=4)\n",
    "\n",
    "    print(\"Archivo 'submission_v3.json' generado con √©xito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'submission_v3.json' generado con √©xito.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el mejor modelo entrenado\n",
    "best_model_file = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/new_model/CV_best_model_epoch_5_ndcg_0.2583.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_file, weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Preparar el dataset de prueba\n",
    "test_data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_test_full.parquet\"\n",
    "test_dataset = IterableSessionDataset(test_data_path, fraction=1.0, mode='test', balance=False)\n",
    "\n",
    "# Ejecutar la inferencia\n",
    "inference_v2(model, test_dataset, device, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
