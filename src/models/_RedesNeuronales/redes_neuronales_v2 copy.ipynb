{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo - fraction = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n",
      "Modelo en: cuda:0\n",
      "Nuevo pos_weight: 15.666666984558105\n",
      "Criterion: FocalLoss()\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0001\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "Fraction of Dataset: 20.0%\n",
      "\n",
      "--- N√∫mero de epochs: 5 ---\n",
      "--- Epoch 1/5 ---\n",
      "üñ•Ô∏è RAM Usage: 7.87 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.32 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [1/5], Loss: 0.0417\n",
      "üñ•Ô∏è RAM Usage: 8.31 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.32 GB\n",
      "üìä Memoria despu√©s del Epoch 1\n",
      "F1-score: 0.7401, AUC-ROC: 0.9787\n",
      "Validation AUC-ROC: 0.9787, Average Precision: 0.8253, MAP@5: nan\n",
      "‚úÖ Nuevo mejor modelo guardado: best_model_epoch_1_f1_0.7401.pt\n",
      "--- Epoch 2/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.33 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.32 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [2/5], Loss: 0.0223\n",
      "üñ•Ô∏è RAM Usage: 8.32 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.32 GB\n",
      "üìä Memoria despu√©s del Epoch 2\n",
      "F1-score: 0.7752, AUC-ROC: 0.9872\n",
      "Validation AUC-ROC: 0.9872, Average Precision: 0.8939, MAP@5: nan\n",
      "‚úÖ Nuevo mejor modelo guardado: best_model_epoch_2_f1_0.7752.pt\n",
      "--- Epoch 3/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.31 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.32 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "Epoch [3/5], Loss: 0.0187\n",
      "üñ•Ô∏è RAM Usage: 8.34 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.32 GB\n",
      "üìä Memoria despu√©s del Epoch 3\n",
      "F1-score: 0.7774, AUC-ROC: 0.9885\n",
      "Validation AUC-ROC: 0.9885, Average Precision: 0.9043, MAP@5: nan\n",
      "‚úÖ Nuevo mejor modelo guardado: best_model_epoch_3_f1_0.7774.pt\n",
      "--- Epoch 4/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.35 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.32 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "‚ö†Ô∏è Batch 1695: NaN o Inf en gradientes. Saltando actualizaci√≥n.\n",
      "Epoch [4/5], Loss: 0.0177\n",
      "üñ•Ô∏è RAM Usage: 8.22 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.32 GB\n",
      "üìä Memoria despu√©s del Epoch 4\n",
      "F1-score: 0.7816, AUC-ROC: 0.9892\n",
      "Validation AUC-ROC: 0.9892, Average Precision: 0.9095, MAP@5: nan\n",
      "‚úÖ Nuevo mejor modelo guardado: best_model_epoch_4_f1_0.7816.pt\n",
      "--- Epoch 5/5 ---\n",
      "üñ•Ô∏è RAM Usage: 8.08 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.32 GB\n",
      "üìä Memoria despu√©s del Epoch Inicio\n",
      "Iniciando el bucle de batches...\n",
      "‚ö†Ô∏è Batch 1304: NaN o Inf en gradientes. Saltando actualizaci√≥n.\n",
      "Epoch [5/5], Loss: 0.0172\n",
      "üñ•Ô∏è RAM Usage: 8.10 GB\n",
      "üî• GPU Usage: 0.02 GB\n",
      "üî• GPU Cached: 0.32 GB\n",
      "üìä Memoria despu√©s del Epoch 5\n",
      "F1-score: 0.7831, AUC-ROC: 0.9896\n",
      "Validation AUC-ROC: 0.9896, Average Precision: 0.9131, MAP@5: nan\n",
      "‚úÖ Nuevo mejor modelo guardado: best_model_epoch_5_f1_0.7831.pt\n"
     ]
    }
   ],
   "source": [
    "# Importar las librer√≠as necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import psutil\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from torch.utils.data import get_worker_info\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import json\n",
    "\n",
    "# Configuraci√≥n global de caracter√≠sticas\n",
    "FEATURE_COLS = [\"time_since_last\", \"session_relative_position\", \"session_duration\",\n",
    "                \"R\", \"F\", \"M\", \"device_type\", \"pagetype\", \"discount\", \"cod_section\", \"family\"]\n",
    "FEATURE_DIM = len(FEATURE_COLS)\n",
    "\n",
    "# ‚úÖ 1Ô∏è‚É£ Configuraci√≥n del Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# üî• Funci√≥n para Monitoreo de Memoria\n",
    "def print_memory_usage(epoch=None):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"üñ•Ô∏è RAM Usage: {mem_info.rss / 1e9:.2f} GB\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üî• GPU Usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"üî• GPU Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    if epoch is not None:\n",
    "        print(f\"üìä Memoria despu√©s del Epoch {epoch}\")\n",
    "\n",
    "# ‚úÖ Implementaci√≥n de Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = torch.tensor(1.0, dtype=torch.float32, device=device) if pos_weight is None else pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # ‚úÖ Clamping para evitar valores extremos\n",
    "        inputs = torch.clamp(inputs, min=-10, max=10)\n",
    "\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, pos_weight=self.pos_weight, reduction='none'\n",
    "        )\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        pt = targets * probas + (1 - targets) * (1 - probas)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        loss = self.alpha * focal_term * bce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# ‚úÖ 2Ô∏è‚É£ Definir Modelo Ajustado con Dropout y Regularizaci√≥n\n",
    "# ‚úÖ Modelo GRU con Atenci√≥n\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attn_weights = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, gru_output):\n",
    "        \"\"\"\n",
    "        gru_output: Tensor de tama√±o (batch, seq_len, hidden_dim)\n",
    "        Devuelve:\n",
    "        - Scores de cada producto en la sesi√≥n (batch, seq_len)\n",
    "        \"\"\"\n",
    "        attn_scores = self.attn_weights(gru_output).squeeze(-1)  # (batch, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)  # Normalizamos\n",
    "        return attn_weights\n",
    "\n",
    "class GRURecommender(nn.Module):\n",
    "    def __init__(self, input_dim=50, feature_dim=FEATURE_DIM, hidden_dim=128, num_layers=2, output_dim=1):\n",
    "        super(GRURecommender, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim + feature_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.attention = AttentionLayer(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Inicializaci√≥n de pesos\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.GRU):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, product_embeddings, session_features):\n",
    "        x = torch.cat((product_embeddings, session_features), dim=-1)\n",
    "        gru_output, _ = self.gru(x)\n",
    "\n",
    "        # Clamping para evitar explosi√≥n de gradientes\n",
    "        gru_output = torch.clamp(gru_output, min=-10, max=10)\n",
    "\n",
    "        attn_weights = self.attention(gru_output)\n",
    "        weighted_output = gru_output * attn_weights.unsqueeze(-1)\n",
    "\n",
    "        scores = self.fc(self.dropout(weighted_output)).squeeze(-1)\n",
    "        return torch.clamp(scores, min=-10, max=10)\n",
    "\n",
    "model = GRURecommender().to(device)\n",
    "print(f\"Modelo en: {next(model.parameters()).device}\")\n",
    "\n",
    "# ‚úÖ 3Ô∏è‚É£ Definir P√©rdida y Optimizador con Focal Loss y regularizaci√≥n\n",
    "pos_weight_value = (0.94 / 0.06)  # Invertir la proporci√≥n\n",
    "pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32, device=device)\n",
    "print(f\"Nuevo pos_weight: {pos_weight}\")\n",
    "\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0, pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "torch.amp.GradScaler('cuda')\n",
    "\n",
    "print(f\"Criterion: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "\n",
    "# ‚úÖ 4Ô∏è‚É£ Definir Funciones de Evaluaci√≥n con M√©tricas Adicionales\n",
    "def evaluate(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device)\n",
    "            session_features = session_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(product_embeddings, session_features)  # (batch, seq_len)\n",
    "            outputs = torch.sigmoid(outputs)  # Convertimos logits a probabilidades\n",
    "\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())\n",
    "            all_outputs.extend(outputs.cpu().numpy().flatten())\n",
    "\n",
    "    # ‚úÖ Convertir listas a arrays de NumPy\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_outputs = np.array(all_outputs)\n",
    "\n",
    "    # ‚úÖ Filtrar NaN antes de calcular m√©tricas\n",
    "    mask = ~np.isnan(all_targets) & ~np.isnan(all_outputs)\n",
    "    all_targets = all_targets[mask]\n",
    "    all_outputs = all_outputs[mask]\n",
    "\n",
    "    if len(all_targets) == 0:\n",
    "        print(\"‚ö†Ô∏è No hay datos v√°lidos para calcular m√©tricas.\")\n",
    "        return float('nan'), float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    auc_roc = roc_auc_score(all_targets, all_outputs)\n",
    "    average_precision = average_precision_score(all_targets, all_outputs)\n",
    "    f1 = f1_score(all_targets, (all_outputs >= threshold).astype(int), zero_division=0)\n",
    "\n",
    "    print(f\"F1-score: {f1:.4f}, AUC-ROC: {auc_roc:.4f}\")\n",
    "    return auc_roc, average_precision, float('nan'), f1\n",
    "\n",
    "# ‚úÖ 5Ô∏è‚É£ Funci√≥n de Entrenamiento con Persistencia del Mejor Modelo\n",
    "def train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, epochs):\n",
    "    best_f1 = 0.0\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,  # No es necesario shuffle con IterableDataset\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        print_memory_usage(epoch=\"Inicio\")\n",
    "        print(\"Iniciando el bucle de batches...\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            product_embeddings, session_features, targets, _, _ = batch\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            # üö® Verificar si hay NaN en las entradas\n",
    "            if torch.isnan(product_embeddings).any() or torch.isnan(session_features).any():\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` en `product_embeddings` o `session_features`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(product_embeddings, session_features)\n",
    "\n",
    "                # üö® Verificar valores extremos en `outputs`\n",
    "                if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                    print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` o `inf` en `outputs`. Saltando batch...\")\n",
    "                    print(f\"üìä Rango de outputs: min={outputs.min().item()}, max={outputs.max().item()}\")\n",
    "                    continue\n",
    "\n",
    "                # ‚úÖ Clamping para evitar valores extremos\n",
    "                outputs = torch.clamp(outputs, min=-10, max=10)\n",
    "\n",
    "                targets = targets[:, :outputs.shape[1]]\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            # ‚úÖ Verificar si la p√©rdida es infinita o NaN antes de `backward()`\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx}: `NaN` o `inf` en `loss`. Saltando batch...\")\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            grads_finite = True\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                    grads_finite = False\n",
    "                    print(f\"‚ö†Ô∏è Batch {batch_idx}: NaN o Inf en gradientes. Saltando actualizaci√≥n.\")\n",
    "                    break\n",
    "\n",
    "            if grads_finite:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                scaler.update()\n",
    "                continue\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "        print_memory_usage(epoch=epoch+1)\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        auc_roc, avg_precision, mapk_score, f1 = evaluate(model, val_loader)\n",
    "        print(f\"Validation AUC-ROC: {auc_roc:.4f}, Average Precision: {avg_precision:.4f}, MAP@5: {mapk_score:.4f}\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            model_filename = f\"best_model_epoch_{epoch+1}_f1_{f1:.4f}.pt\"\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(f\"‚úÖ Nuevo mejor modelo guardado: {model_filename}\")\n",
    "\n",
    "    return model_filename\n",
    "\n",
    "# ‚úÖ 6Ô∏è‚É£ Cargar Dataset con Normalizaci√≥n y Manejo de Outliers\n",
    "MAX_SEQ_LENGTH = 50\n",
    "\n",
    "class IterableSessionDataset(IterableDataset):\n",
    "    def __init__(self, df_path, fraction=1.0, mode='train', balance=True):\n",
    "        self.df_path = df_path\n",
    "        self.feature_cols = FEATURE_COLS\n",
    "        self.fraction = fraction\n",
    "        self.mode = mode\n",
    "        self.balance = balance\n",
    "\n",
    "        # Cargar el DataFrame completo para calcular estad√≠sticas globales\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        num_row_groups = table.metadata.num_row_groups\n",
    "        dfs = [table.read_row_group(i).to_pandas() for i in range(num_row_groups)]\n",
    "        full_df = pd.concat(dfs)\n",
    "        self.global_feature_means = full_df[self.feature_cols].mean()\n",
    "        self.global_feature_stds = full_df[self.feature_cols].std().replace(0, 1e-6)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            return self._data_iterator()\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            return self._data_iterator(worker_id, num_workers)\n",
    "\n",
    "    def _data_iterator(self, worker_id=0, num_workers=1):\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            row_groups = range(0, int(num_row_groups_to_use * 0.8))\n",
    "        else:\n",
    "            row_groups = range(int(num_row_groups_to_use * 0.8), num_row_groups_to_use)\n",
    "\n",
    "        for i in row_groups:\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "\n",
    "            # Aplicar transformaciones logar√≠tmicas\n",
    "            for col in ['time_since_last', 'session_duration']:\n",
    "                df[col] = np.log1p(df[col])\n",
    "\n",
    "            # Normalizaci√≥n global segura\n",
    "            df[self.feature_cols] = (df[self.feature_cols] - self.global_feature_means) / self.global_feature_stds\n",
    "\n",
    "            # Reemplazar posibles NaN resultantes de la normalizaci√≥n\n",
    "            df[self.feature_cols] = df[self.feature_cols].fillna(0.0)\n",
    "\n",
    "            # Continuar con el resto del procesamiento...\n",
    "            if self.balance:\n",
    "                df_1 = df[df[\"add_to_cart\"] == 1]\n",
    "                df_0 = df[df[\"add_to_cart\"] == 0]\n",
    "                if len(df_1) > 0 and len(df_0) > 0:\n",
    "                    if len(df_1) > len(df_0):\n",
    "                        df_0 = df_0.sample(n=len(df_1), replace=True, random_state=42)\n",
    "                    else:\n",
    "                        df_1 = df_1.sample(n=len(df_0), replace=True, random_state=42)\n",
    "                df = pd.concat([df_0, df_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            for session_id, session_data in df.groupby(\"session_id\"):\n",
    "                product_embeddings = session_data[\"embedding_reduced\"].tolist()\n",
    "                partnumbers = session_data[\"partnumber\"].tolist()\n",
    "\n",
    "                # Verificar si hay NaN en embeddings\n",
    "                if any(embedding is None or np.isnan(embedding).any() for embedding in product_embeddings):\n",
    "                    continue\n",
    "\n",
    "                product_embeddings = torch.tensor(np.array(product_embeddings), dtype=torch.float32)\n",
    "\n",
    "                session_features = torch.tensor(session_data[self.feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "                # Verificar si hay NaN en features\n",
    "                if torch.isnan(session_features).any():\n",
    "                    continue\n",
    "\n",
    "                targets = torch.tensor(session_data[\"add_to_cart\"].values, dtype=torch.float32)\n",
    "                yield product_embeddings, session_features, targets, session_id, partnumbers\n",
    "                \n",
    "                \n",
    "# ‚úÖ Funci√≥n de Colaci√≥n\n",
    "def collate_fn(batch):\n",
    "    product_embeddings, session_features, targets, session_ids, partnumbers_list = zip(*batch)\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), torch.empty(0), torch.empty(0), [], []\n",
    "\n",
    "    max_len = min(max([x.shape[0] for x in product_embeddings]), MAX_SEQ_LENGTH)\n",
    "\n",
    "    padded_embeddings = torch.zeros((len(batch), max_len, product_embeddings[0].shape[1]))\n",
    "    padded_features = torch.zeros((len(batch), max_len, session_features[0].shape[1]))\n",
    "    padded_targets = torch.zeros((len(batch), max_len))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        seq_len = min(product_embeddings[i].shape[0], max_len)\n",
    "        padded_embeddings[i, :seq_len] = product_embeddings[i][:seq_len]\n",
    "        padded_features[i, :seq_len] = session_features[i][:seq_len]\n",
    "        padded_targets[i, :seq_len] = targets[i][:seq_len]\n",
    "\n",
    "    return padded_embeddings, padded_features, padded_targets, session_ids, partnumbers_list\n",
    "\n",
    "# ‚úÖ 7Ô∏è‚É£ Crear Datasets de Entrenamiento y Validaci√≥n\n",
    "data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "fraction = 0.2\n",
    "train_dataset = IterableSessionDataset(data_path, fraction=fraction, mode='train')\n",
    "val_dataset = IterableSessionDataset(data_path, fraction=fraction, mode='val')\n",
    "\n",
    "# ‚úÖ 8Ô∏è‚É£ Ejecutar Entrenamiento\n",
    "EPOCHS = 5\n",
    "print(f'\\nFraction of Dataset: {fraction*100}%\\n')\n",
    "print(f'--- N√∫mero de epochs: {EPOCHS} ---')\n",
    "best_model_file = train(model, train_dataset, val_dataset, criterion, optimizer, scheduler, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_train_pca50.parquet\"\n",
    "df = pd.read_parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_id                   0\n",
      "date                         0\n",
      "timestamp_local              0\n",
      "add_to_cart                  0\n",
      "user_id                      0\n",
      "partnumber                   0\n",
      "device_type                  0\n",
      "pagetype                     0\n",
      "discount                     0\n",
      "color_id                     0\n",
      "cod_section                  0\n",
      "family                       0\n",
      "embedding_reduced            0\n",
      "R                            0\n",
      "F                            0\n",
      "M                            0\n",
      "anonymous_user               0\n",
      "country                      0\n",
      "time_since_last              0\n",
      "session_position             0\n",
      "session_relative_position    0\n",
      "session_duration             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# üö® Verificar si hay `NaN`\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   mean          std\n",
      "time_since_last            3.173853e+01   150.635956\n",
      "session_relative_position  5.488508e-01     0.297142\n",
      "session_duration           1.250550e+03  5041.246094\n",
      "R                          3.350681e-08     0.389959\n",
      "F                          2.486847e-07     0.389959\n",
      "M                          4.615620e-07     0.389959\n",
      "device_type                1.143908e+00     0.507681\n",
      "pagetype                   2.390030e+01     1.283467\n",
      "discount                   6.614614e-02     0.248537\n",
      "cod_section                1.495020e+00     0.996848\n",
      "family                     8.771977e+01    54.136219\n"
     ]
    }
   ],
   "source": [
    "# Revisar medias y desviaciones est√°ndar\n",
    "stats = df[FEATURE_COLS].describe().T[['mean', 'std']]\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_since_last              0\n",
      "session_relative_position    0\n",
      "session_duration             0\n",
      "R                            0\n",
      "F                            0\n",
      "M                            0\n",
      "device_type                  0\n",
      "pagetype                     0\n",
      "discount                     0\n",
      "cod_section                  0\n",
      "family                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[FEATURE_COLS].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[FEATURE_COLS] = df[FEATURE_COLS].fillna(0)  # O usar .fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df[\"embedding_reduced\"].apply(lambda x: np.isnan(x).any()).sum())  # Contar filas con NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_to_cart\n",
      "0    0.940906\n",
      "1    0.059094\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"add_to_cart\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets √∫nicos en validaci√≥n: [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Targets √∫nicos en validaci√≥n:\", df[\"add_to_cart\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alineamos las columnas de Test Vs Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en el conjunto de datos de prueba:\n",
      "Index(['session_id', 'date', 'timestamp_local', 'user_id', 'partnumber',\n",
      "       'device_type', 'pagetype', 'discount', 'color_id', 'cod_section',\n",
      "       'family', 'embedding_reduced', 'R', 'F', 'M', 'anonymous_user',\n",
      "       'country'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "test_data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/test_full.parquet\"\n",
    "\n",
    "# Cargar el dataset de prueba completo\n",
    "test_table = pq.ParquetFile(test_data_path)\n",
    "num_row_groups = test_table.num_row_groups\n",
    "\n",
    "dfs = []\n",
    "for i in range(num_row_groups):\n",
    "    batch = test_table.read_row_group(i)\n",
    "    df_batch = batch.to_pandas()\n",
    "    dfs.append(df_batch)\n",
    "\n",
    "test_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Columnas en el conjunto de datos de prueba:\")\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que 'timestamp_local' es de tipo datetime\n",
    "test_df[\"timestamp_local\"] = pd.to_datetime(test_df[\"timestamp_local\"])\n",
    "\n",
    "# Ordenar por 'session_id' y 'timestamp_local'\n",
    "test_df = test_df.sort_values(by=[\"session_id\", \"timestamp_local\"])\n",
    "\n",
    "# Calcular 'time_since_last'\n",
    "test_df[\"time_since_last\"] = test_df.groupby(\"session_id\")[\"timestamp_local\"].diff().dt.total_seconds().fillna(0).astype(np.float32)\n",
    "\n",
    "# Calcular la posici√≥n en la sesi√≥n\n",
    "test_df[\"session_position\"] = test_df.groupby(\"session_id\").cumcount() + 1\n",
    "\n",
    "# Calcular 'session_relative_position'\n",
    "test_df[\"session_relative_position\"] = (test_df[\"session_position\"] / test_df.groupby(\"session_id\")[\"session_position\"].transform(\"max\")).astype(np.float32)\n",
    "\n",
    "# Calcular 'session_duration'\n",
    "test_df[\"session_duration\"] = test_df.groupby(\"session_id\")[\"timestamp_local\"].transform(lambda x: (x.max() - x.min()).total_seconds()).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas despu√©s del preprocesamiento:\n",
      "Index(['session_id', 'date', 'timestamp_local', 'user_id', 'partnumber',\n",
      "       'device_type', 'pagetype', 'discount', 'color_id', 'cod_section',\n",
      "       'family', 'embedding_reduced', 'R', 'F', 'M', 'anonymous_user',\n",
      "       'country', 'time_since_last', 'session_position',\n",
      "       'session_relative_position', 'session_duration'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columnas despu√©s del preprocesamiento:\")\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_test_full.parquet\"\n",
    "\n",
    "test_df.to_parquet(preprocessed_test_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las caracter√≠sticas est√°n presentes en el conjunto de datos de prueba.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Cargar una porci√≥n del conjunto de datos de prueba preprocesado\n",
    "table = pq.ParquetFile(preprocessed_test_data_path)\n",
    "batch = table.read_row_group(0)\n",
    "df_test = batch.to_pandas()\n",
    "\n",
    "FEATURE_COLS = [\"time_since_last\", \"session_relative_position\", \"session_duration\", \n",
    "                \"R\", \"F\", \"M\", \"device_type\", \"pagetype\", \"discount\", \"cod_section\", \"family\"]\n",
    "\n",
    "missing_features = [col for col in FEATURE_COLS if col not in df_test.columns]\n",
    "if missing_features:\n",
    "    print(f\"Las siguientes caracter√≠sticas faltan en el conjunto de prueba: {missing_features}\")\n",
    "else:\n",
    "    print(\"Todas las caracter√≠sticas est√°n presentes en el conjunto de datos de prueba.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_product_attributes(products_file_path):\n",
    "    df_products = pd.read_parquet(products_file_path)\n",
    "    product_attributes = {}\n",
    "    for idx, row in df_products.iterrows():\n",
    "        partnumber = int(row['partnumber'])\n",
    "        product_attributes[partnumber] = {\n",
    "            'cod_section': row.get('cod_section', None),\n",
    "            'family': row.get('family', None),\n",
    "            # A√±adir m√°s atributos si es necesario\n",
    "        }\n",
    "    return product_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableSessionDataset(IterableDataset):\n",
    "    def __init__(self, df_path, fraction=1.0, mode='train', balance=True):\n",
    "        self.df_path = df_path\n",
    "        self.feature_cols = FEATURE_COLS\n",
    "        self.fraction = fraction\n",
    "        self.mode = mode\n",
    "        self.balance = balance  # Solo aplica para entrenamiento\n",
    "        # Cargar estad√≠sticas globales para normalizaci√≥n\n",
    "        self.global_feature_means = None\n",
    "        self.global_feature_stds = None\n",
    "        self._compute_global_statistics()\n",
    "\n",
    "    def _compute_global_statistics(self):\n",
    "        # Cargar una muestra del dataset para calcular la media y desviaci√≥n est√°ndar\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        dfs = []\n",
    "        for i in range(num_row_groups_to_use):\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "            dfs.append(df)\n",
    "\n",
    "        full_df = pd.concat(dfs)\n",
    "        self.global_feature_means = full_df[self.feature_cols].mean()\n",
    "        self.global_feature_stds = full_df[self.feature_cols].std().replace(0, 1e-6)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Devuelve un iterador sobre los datos. \"\"\"\n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            return self._data_iterator()\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            return self._data_iterator(worker_id, num_workers)\n",
    "\n",
    "    def _data_iterator(self, worker_id=0, num_workers=1):\n",
    "        \"\"\" Iterador interno que carga los datos desde Parquet. \"\"\"\n",
    "        table = pq.ParquetFile(self.df_path)\n",
    "        total_row_groups = table.metadata.num_row_groups\n",
    "        num_row_groups_to_use = max(1, int(total_row_groups * self.fraction))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            row_groups = range(0, int(num_row_groups_to_use * 0.8))\n",
    "        elif self.mode == 'val':\n",
    "            row_groups = range(int(num_row_groups_to_use * 0.8), num_row_groups_to_use)\n",
    "        elif self.mode == 'test':\n",
    "            row_groups = range(0, num_row_groups_to_use)\n",
    "        else:\n",
    "            raise ValueError(\"Modo inv√°lido. Use 'train', 'val' o 'test'.\")\n",
    "\n",
    "        for i in row_groups:\n",
    "            batch = table.read_row_group(i)\n",
    "            df = batch.to_pandas()\n",
    "\n",
    "            # Aplicar transformaciones logar√≠tmicas\n",
    "            for col in ['time_since_last', 'session_duration']:\n",
    "                df[col] = np.log1p(df[col])\n",
    "\n",
    "            # Normalizaci√≥n global segura\n",
    "            df[self.feature_cols] = (df[self.feature_cols] - self.global_feature_means) / self.global_feature_stds\n",
    "            df[self.feature_cols] = df[self.feature_cols].fillna(0.0)\n",
    "\n",
    "            # Para el modo 'train' y 'val', podemos balancear los datos\n",
    "            if self.mode in ['train', 'val'] and self.balance:\n",
    "                df_1 = df[df[\"add_to_cart\"] == 1]\n",
    "                df_0 = df[df[\"add_to_cart\"] == 0]\n",
    "                if len(df_1) > 0 and len(df_0) > 0:\n",
    "                    if len(df_1) > len(df_0):\n",
    "                        df_0 = df_0.sample(n=len(df_1), replace=True, random_state=42)\n",
    "                    else:\n",
    "                        df_1 = df_1.sample(n=len(df_0), replace=True, random_state=42)\n",
    "                    df = pd.concat([df_0, df_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            for session_id, session_data in df.groupby(\"session_id\"):\n",
    "                product_embeddings = session_data[\"embedding_reduced\"].tolist()\n",
    "                partnumbers = session_data[\"partnumber\"].tolist()\n",
    "\n",
    "                # Verificar si hay NaN en embeddings\n",
    "                if any(embedding is None or np.isnan(embedding).any() for embedding in product_embeddings):\n",
    "                    continue\n",
    "\n",
    "                product_embeddings = torch.tensor(np.array(product_embeddings), dtype=torch.float32)\n",
    "                session_features = torch.tensor(session_data[self.feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "                # Verificar si hay NaN en features\n",
    "                if torch.isnan(session_features).any():\n",
    "                    continue\n",
    "\n",
    "                if self.mode in ['train', 'val']:\n",
    "                    targets = torch.tensor(session_data[\"add_to_cart\"].values, dtype=torch.float32)\n",
    "                else:\n",
    "                    targets = None  # No hay targets en el conjunto de prueba\n",
    "\n",
    "                yield product_embeddings, session_features, targets, session_id, partnumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    product_embeddings, session_features, targets, session_ids, partnumbers_list = zip(*batch)\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), torch.empty(0), None, [], []\n",
    "\n",
    "    max_len = min(max([x.shape[0] for x in product_embeddings]), MAX_SEQ_LENGTH)\n",
    "\n",
    "    padded_embeddings = torch.zeros((len(batch), max_len, product_embeddings[0].shape[1]))\n",
    "    padded_features = torch.zeros((len(batch), max_len, session_features[0].shape[1]))\n",
    "\n",
    "    if targets[0] is not None:\n",
    "        padded_targets = torch.zeros((len(batch), max_len))\n",
    "    else:\n",
    "        padded_targets = None  # No hay targets en el conjunto de prueba\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        seq_len = min(product_embeddings[i].shape[0], max_len)\n",
    "        padded_embeddings[i, :seq_len] = product_embeddings[i][:seq_len]\n",
    "        padded_features[i, :seq_len] = session_features[i][:seq_len]\n",
    "        if targets[0] is not None:\n",
    "            padded_targets[i, :seq_len] = targets[i][:seq_len]\n",
    "\n",
    "    return padded_embeddings, padded_features, padded_targets, session_ids, partnumbers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los atributos de los productos\n",
    "product_attributes = load_product_attributes(\"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/optimized_products_transformed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_v2(model, test_dataset, device, top_k=5):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    recommendations = {}\n",
    "    global_top_recommendations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            product_embeddings, session_features, _, session_ids, partnumbers_list = batch\n",
    "\n",
    "            if product_embeddings.shape[0] == 0:\n",
    "                continue  # Saltar batch vac√≠o\n",
    "\n",
    "            product_embeddings = product_embeddings.to(device, non_blocking=True)\n",
    "            session_features = session_features.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(product_embeddings, session_features)  # (batch, seq_len)\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()  # Convertimos logits a probabilidades\n",
    "\n",
    "            for sid, parts, score_seq in zip(session_ids, partnumbers_list, scores):\n",
    "                # Asegurar que `parts` y `score_seq` tienen la misma longitud\n",
    "                if len(parts) != len(score_seq):\n",
    "                    min_len = min(len(parts), len(score_seq))\n",
    "                    parts = parts[:min_len]\n",
    "                    score_seq = score_seq[:min_len]\n",
    "\n",
    "                product_scores = list(zip(parts, score_seq))\n",
    "                product_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                top_recommendations = [int(p) for p, s in product_scores[:top_k]]\n",
    "                top_recommendations = list(dict.fromkeys(top_recommendations))\n",
    "\n",
    "                # Recopilar productos similares basados en atributos\n",
    "                if len(top_recommendations) < top_k:\n",
    "                    # Obtener atributos de los productos ya recomendados\n",
    "                    similar_candidates = []\n",
    "                    for rec in top_recommendations:\n",
    "                        rec_attrs = product_attributes.get(rec, {})\n",
    "                        # Buscar otros productos con atributos similares\n",
    "                        for p in parts:\n",
    "                            p = int(p)\n",
    "                            if p in top_recommendations:\n",
    "                                continue\n",
    "                            p_attrs = product_attributes.get(p, {})\n",
    "                            # Comparar atributos (por ejemplo, cod_section y family)\n",
    "                            if (p_attrs.get(\"cod_section\") == rec_attrs.get(\"cod_section\") and\n",
    "                                p_attrs.get(\"family\") == rec_attrs.get(\"family\")):\n",
    "                                similar_candidates.append(p)\n",
    "                    # A√±adir candidatos similares si es necesario\n",
    "                    for candidate in similar_candidates:\n",
    "                        if candidate not in top_recommendations and len(top_recommendations) < top_k:\n",
    "                            top_recommendations.append(candidate)\n",
    "\n",
    "                    # Acumular productos para fallback global si a√∫n faltan recomendaciones\n",
    "                    if len(top_recommendations) < top_k:\n",
    "                        global_top_recommendations.extend(\n",
    "                            [p for p in parts if p not in top_recommendations]\n",
    "                        )\n",
    "\n",
    "                recommendations[int(sid)] = top_recommendations\n",
    "\n",
    "    # Fallback global\n",
    "    product_counts = {}\n",
    "    for p in global_top_recommendations:\n",
    "        product_counts[p] = product_counts.get(p, 0) + 1\n",
    "    sorted_global_top = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    global_top = [p for p, count in sorted_global_top]\n",
    "\n",
    "    for sid, recs in recommendations.items():\n",
    "        if len(recs) < top_k:\n",
    "            additional = [p for p in global_top if p not in recs]\n",
    "            recs.extend(additional[:top_k - len(recs)])\n",
    "            recommendations[sid] = recs\n",
    "\n",
    "    output_json = {\n",
    "        \"target\": recommendations\n",
    "    }\n",
    "\n",
    "    with open(\"submission_v3.json\", \"w\") as f:\n",
    "        json.dump(output_json, f, indent=4)\n",
    "\n",
    "    print(\"Archivo 'submission_v3.json' generado con √©xito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'submission_v3.json' generado con √©xito.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el mejor modelo entrenado\n",
    "best_model_file = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/new_model/best_model_epoch_5_f1_0.7831.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_file, weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Preparar el dataset de prueba\n",
    "test_data_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed_v2/processed_test_full.parquet\"\n",
    "test_dataset = IterableSessionDataset(test_data_path, fraction=1.0, mode='test', balance=False)\n",
    "\n",
    "# Ejecutar la inferencia\n",
    "inference_v2(model, test_dataset, device, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
