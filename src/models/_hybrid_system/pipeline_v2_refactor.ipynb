{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport numpy as np\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom scipy.sparse import csr_matrix\\nimport implicit\\n\\n\\nproduct_path = \\'/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/products_data.pkl\\'\\nuser_path = \\'/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/user_data.csv\\'\\ntrain_enriched_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/hybrid_model/train_preprocessed.pkl\"\\ntest_enriched_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/hybrid_model/test_preprocessed.pkl\"\\n\\n\\n# Cargar los datasets\\nproducts = pd.read_pickle(product_path)\\nusers = pd.read_csv(user_path)\\ntrain = pd.read_pickle(train_enriched_path)\\ntest = pd.read_pickle(test_enriched_path)\\n\\n# Convertir a numérico\\ntrain[\\'pagetype\\'] = pd.to_numeric(train[\\'pagetype\\'], errors=\\'coerce\\')\\n# Reemplazar los NaN y asignar el resultado de vuelta a la columna\\ntrain[\\'pagetype\\'] = train[\\'pagetype\\'].fillna(-1)\\n# Cambiar el tipo de dato\\ntrain[\\'pagetype\\'] = train[\\'pagetype\\'].astype(\\'int16\\')\\ntrain_user_ids = set(train[\\'user_id\\'].unique())\\ntrain_user_ids.discard(-1)  # Remover -1 si representa usuarios no logueados\\n# Asumimos que si hay \\'partnumber\\', hay interacción\\ntest_sessions_with_interactions = set(test[test[\\'partnumber\\'].notnull()][\\'session_id\\'].unique())\\n\\n\\n\\ndef classify_session(row):\\n    user_id = row[\\'user_id\\']\\n    session_id = row[\\'session_id\\']\\n    \\n    if user_id == -1:\\n        # Usuario no logueado\\n        if session_id in test_sessions_with_interactions:\\n            return \\'Usuario recurrente no logueado\\'\\n        else:\\n            return \\'Usuario nuevo no logueado\\'\\n    else:\\n        # Usuario logueado\\n        if user_id in train_user_ids:\\n            return \\'Usuario recurrente logueado\\'\\n        else:\\n            return \\'Usuario nuevo logueado\\'\\n\\n\\ntest[\\'user_class\\'] = test.apply(classify_session, axis=1)\\n\\n# Modelo de Popularidad\\n# Calcular la popularidad de los productos en el conjunto de entrenamiento\\nproduct_popularity = train.groupby(\\'partnumber\\')[\\'add_to_cart\\'].sum().reset_index()\\nproduct_popularity.rename(columns={\\'add_to_cart\\': \\'popularity\\'}, inplace=True)\\n\\n# Ordenar los productos por popularidad descendente\\nproduct_popularity.sort_values(by=\\'popularity\\', ascending=False, inplace=True)\\n\\n# Obtener la lista de productos populares\\npopular_products = product_popularity[\\'partnumber\\'].tolist()\\n\\n\\n\\n# Asegurarnos de que los embeddings son arrays de NumPy\\nproducts[\\'embedding\\'] = products[\\'embedding\\'].apply(np.array)\\n\\n# Crear un diccionario {partnumber: embedding}\\nembeddings_dict = dict(zip(products[\\'partnumber\\'], products[\\'embedding\\']))\\n\\ndef find_similar_products(partnumber, top_n=5):\\n    target_embedding = embeddings_dict.get(partnumber)\\n    if not isinstance(target_embedding, np.ndarray):\\n        print(f\"Advertencia: El embedding del producto {partnumber} es inválido.\")\\n        return []\\n\\n    # Obtener todas las embeddings y los partnumbers correspondientes\\n    all_partnumbers = []\\n    all_embeddings = []\\n    for pnum, emb in embeddings_dict.items():\\n        if isinstance(emb, np.ndarray) and emb.shape == target_embedding.shape:\\n            all_partnumbers.append(pnum)\\n            all_embeddings.append(emb)\\n\\n    # Convertir a arrays de NumPy\\n    all_embeddings = np.stack(all_embeddings)\\n\\n    # Calcular la similitud de coseno\\n    similarities = cosine_similarity([target_embedding], all_embeddings)[0]\\n\\n    # Obtener los índices de los productos más similares (excluyendo el propio producto)\\n    similar_indices = similarities.argsort()[::-1]\\n    similar_partnumbers = []\\n    for idx in similar_indices:\\n        if all_partnumbers[idx] != partnumber:\\n            similar_partnumbers.append(all_partnumbers[idx])\\n        if len(similar_partnumbers) == top_n:\\n            break\\n\\n    return similar_partnumbers\\n\\n# Seleccionar un producto para probar, por ejemplo, el más popular\\ntest_partnumber = popular_products[0]  # Primer producto de la lista de populares\\n\\n# Obtener productos similares\\nsimilar_products = find_similar_products(test_partnumber, top_n=5)\\n\\n\\n# Filtrar interacciones de usuarios logueados y crear una copia\\ntrain_logged_in = train[train[\\'user_id\\'] != -1].copy()\\n\\n\\n# Obtener user_ids únicos\\nunique_user_ids = train_logged_in[\\'user_id\\'].unique()\\n\\n# Crear diccionarios de mapeo\\nuser_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\\nindex_to_user_id = {idx: user_id for user_id, idx in user_id_to_index.items()}\\n\\n# Mapear \\'user_id\\' a \\'user_idx\\' en train_logged_in\\ntrain_logged_in[\\'user_idx\\'] = train_logged_in[\\'user_id\\'].map(user_id_to_index)\\n\\n# Obtener partnumbers únicos\\nunique_partnumbers = train_logged_in[\\'partnumber\\'].unique()\\n\\n# Crear diccionarios de mapeo\\npartnumber_to_index = {partnumber: idx for idx, partnumber in enumerate(unique_partnumbers)}\\nindex_to_partnumber = {idx: partnumber for partnumber, idx in partnumber_to_index.items()}\\n\\n# Mapear \\'partnumber\\' a \\'item_idx\\' en train_logged_in\\ntrain_logged_in[\\'item_idx\\'] = train_logged_in[\\'partnumber\\'].map(partnumber_to_index)\\n\\n# Construir la matriz de interacciones (usuarios x items)\\ninteraction_matrix = csr_matrix(\\n    (train_logged_in[\\'add_to_cart\\'], (train_logged_in[\\'user_idx\\'], train_logged_in[\\'item_idx\\'])),\\n    shape=(len(unique_user_ids), len(unique_partnumbers))\\n)\\n\\n# Configurar el modelo (no transponemos la matriz)\\nmodel = implicit.als.AlternatingLeastSquares(factors=50, iterations=10, regularization=0.1)\\n\\n# Entrenar el modelo\\nmodel.fit(interaction_matrix)\\n\\nuser_idx = 0  # Puedes elegir cualquier índice válido\\nuser_id = index_to_user_id[user_idx]\\n\\n# Obtener las interacciones del usuario\\nuser_items = interaction_matrix[user_idx]\\n\\n# Obtener recomendaciones\\nrecommended = model.recommend(user_idx, user_items, N=5)\\n\\nif isinstance(recommended, list):\\n    # recommended es una lista de tuplas (item_idx, score)\\n    recommended_indices = [item_idx for item_idx, score in recommended]\\nelif isinstance(recommended, tuple) and len(recommended) == 2:\\n    # recommended es una tupla de (indices, scores)\\n    recommended_indices = recommended[0].tolist()\\nelif isinstance(recommended, np.ndarray):\\n    # recommended es un array de índices\\n    recommended_indices = recommended.tolist()\\nelse:\\n    print(\"Formato de \\'recommended\\' no reconocido.\")\\n    recommended_indices = []\\n    \\n    \\n# Verificar si los índices existen en \\'index_to_partnumber\\'\\nmissing_indices = [idx for idx in recommended_indices if idx not in index_to_partnumber]\\n\\nif missing_indices:\\n    print(f\"Los siguientes índices no se encuentran en \\'index_to_partnumber\\': {missing_indices}\")\\nelse:\\n    print(\"Todos los índices recomendados están en \\'index_to_partnumber\\'\")\\n    \\n    # Mapear los índices a \\'partnumber\\'\\nrecommended_partnumbers = [index_to_partnumber[idx] for idx in recommended_indices]\\n\\ngroup = test[test[\\'user_class\\'] == \\'Usuario recurrente logueado\\']\\ntotal_rows = group.shape[0]\\nnum_sessions = group[\\'session_id\\'].nunique()\\n\\n# Filtrar las sesiones de \\'Usuario recurrente logueado\\'\\ngroup = test[test[\\'user_class\\'] == \\'Usuario recurrente logueado\\']\\n\\n# Obtener las sesiones únicas y los user_id asociados\\nsession_user_mapping = group[[\\'session_id\\', \\'user_id\\']].drop_duplicates()\\n\\ndef recommend_by_collaborative(user_id, top_n=5):\\n    # Obtener el índice del usuario\\n    user_idx = user_id_to_index.get(user_id)\\n    \\n    if user_idx is None:\\n        # Si el usuario no está en el modelo, retornamos una lista vacía o usamos popularidad\\n        print(f\"Usuario {user_id} no encontrado en el modelo colaborativo.\")\\n        return []\\n    \\n    # Obtener las interacciones del usuario\\n    user_items = interaction_matrix[user_idx]\\n    \\n    # Obtener recomendaciones del modelo\\n    recommended = model.recommend(user_idx, user_items, N=top_n)\\n    \\n    # Ajustar según el tipo de \\'recommended\\'\\n    if isinstance(recommended, list):\\n        # recommended es una lista de tuplas (item_idx, score)\\n        recommended_indices = [item_idx for item_idx, score in recommended]\\n    elif isinstance(recommended, tuple) and len(recommended) == 2:\\n        # recommended es una tupla de (indices, scores)\\n        recommended_indices = recommended[0].tolist()\\n    elif isinstance(recommended, np.ndarray):\\n        # recommended es un array de índices\\n        recommended_indices = recommended.tolist()\\n    else:\\n        print(\"Formato de \\'recommended\\' no reconocido.\")\\n        recommended_indices = []\\n    \\n    # Verificar si los índices existen en \\'index_to_partnumber\\'\\n    recommended_indices = [idx for idx in recommended_indices if idx in index_to_partnumber]\\n    \\n    # Mapear los índices a \\'partnumber\\'\\n    recommended_partnumbers = [index_to_partnumber[idx] for idx in recommended_indices]\\n    \\n    return recommended_partnumbers\\n\\n# Seleccionar un user_id de prueba (asegúrate de que el user_id existe en user_id_to_index)\\nuser_id = unique_user_ids[0]\\nrecommended_partnumbers = recommend_by_collaborative(user_id, top_n=5)\\n\\n# Mantener el diccionario \\'user_recommendations\\' si ya lo tienes definido\\n# De lo contrario, inicialízalo\\nuser_recommendations = {}\\n\\n# Iterar sobre cada sesión y user_id asociados\\nfor _, row in session_user_mapping.iterrows():\\n    session_id = row[\\'session_id\\']\\n    user_id = row[\\'user_id\\']\\n    \\n    # Obtener recomendaciones colaborativas\\n    recs = recommend_by_collaborative(user_id, top_n=10)  # Usamos top_n=10 para tener margen\\n    \\n    # Obtener productos ya interactuados por el usuario en el conjunto de entrenamiento\\n    user_interacted_items = train_logged_in[train_logged_in[\\'user_id\\'] == user_id][\\'partnumber\\'].unique()\\n    \\n    # Eliminar productos ya vistos\\n    recs = [p for p in recs if p not in user_interacted_items]\\n    \\n    # Si después de filtrar no tenemos suficientes recomendaciones, complementamos con popularidad\\n    if len(recs) < 5:\\n        more_recs = [p for p in popular_products if p not in recs and p not in user_interacted_items]\\n        recs.extend(more_recs[:5 - len(recs)])\\n    \\n    # Aseguramos 5 recomendaciones únicas\\n    recs = list(dict.fromkeys(recs))[:5]\\n    \\n    # Almacenar las recomendaciones\\n    user_recommendations[session_id] = recs\\n    \\n    \\n# Mostrar las recomendaciones para las primeras 5 sesiones\\nsession_ids = list(user_recommendations.keys())\\nfor session_id in session_ids[:5]:\\n    recs = user_recommendations[session_id]\\n    print(f\"Sesión {session_id} - Recomendaciones: {recs}\")\\n    \\n\\n# Obtener el número total de sesiones en el conjunto de prueba\\ntotal_sessions_in_test = test[\\'session_id\\'].nunique()\\n\\n# Obtener el número total de sesiones para las que hemos generado recomendaciones\\ntotal_sessions_with_recommendations = len(user_recommendations)\\n\\nprint(f\"Total de sesiones en el conjunto de prueba: {total_sessions_in_test}\")\\nprint(f\"Total de sesiones con recomendaciones: {total_sessions_with_recommendations}\")\\n\\nif total_sessions_in_test == total_sessions_with_recommendations:\\n    print(\"Todas las sesiones tienen recomendaciones.\")\\nelse:\\n    print(f\"Faltan recomendaciones para {total_sessions_in_test - total_sessions_with_recommendations} sesiones.\")\\n    \\n\\n# Filtrar las sesiones de \\'Usuario recurrente no logueado\\'\\ngroup = test[test[\\'user_class\\'] == \\'Usuario recurrente no logueado\\']\\n\\n# Obtener las sesiones únicas\\nunique_sessions = group[\\'session_id\\'].unique()\\n\\ndef recommend_by_content(partnumbers_interacted, top_n=5):\\n    recommendations = []\\n    for partnumber in partnumbers_interacted:\\n        # Obtener productos similares\\n        similar_products = find_similar_products(partnumber, top_n=top_n)\\n        # Añadir los productos similares a la lista de recomendaciones\\n        recommendations.extend(similar_products)\\n    \\n    # Eliminar productos ya vistos y duplicados\\n    recommendations = [p for p in recommendations if p not in partnumbers_interacted]\\n    recommendations = list(dict.fromkeys(recommendations))\\n    \\n    return recommendations[:top_n]\\n\\n# Seleccionar un \\'partnumber\\' de ejemplo con el que haya interactuado un usuario\\ntest_partnumbers_interacted = [40779]  # Este es el \\'partnumber\\' más popular según nuestro dato previo\\n\\n# Generar recomendaciones basadas en contenido\\nrecs_content = recommend_by_content(test_partnumbers_interacted, top_n=5)\\n\\n\\ndef recommend_by_popularity(top_n=5):\\n    return popular_products[:top_n]\\n\\n# Continuamos usando el diccionario \\'user_recommendations\\' existente\\n\\nfor session_id in unique_sessions:\\n    # Obtener los \\'partnumber\\' con los que interactuó en la sesión\\n    session_data = group[group[\\'session_id\\'] == session_id]\\n    partnumbers_interacted = session_data[\\'partnumber\\'].unique()\\n    \\n    if len(partnumbers_interacted) > 0:\\n        # Generar recomendaciones basadas en contenido\\n        recs = recommend_by_content(partnumbers_interacted, top_n=10)\\n    else:\\n        # Si no hay interacciones, usar popularidad\\n        recs = recommend_by_popularity(top_n=5)\\n    \\n    # Aseguramos 5 recomendaciones únicas\\n    recs = list(dict.fromkeys(recs))[:5]\\n    \\n    # Almacenar las recomendaciones en el diccionario existente\\n    user_recommendations[session_id] = recs\\n    \\n\\n# Filtrar las sesiones de \\'Usuario nuevo logueado\\'\\ngroup_new_logged_in = test[test[\\'user_class\\'] == \\'Usuario nuevo logueado\\']\\n\\n# Obtener las sesiones únicas y los \\'user_id\\' asociados\\nsession_user_mapping_new_logged_in = group_new_logged_in[[\\'session_id\\', \\'user_id\\']].drop_duplicates()\\n\\n# Continuamos utilizando el diccionario \\'user_recommendations\\'\\n\\nfor _, row in session_user_mapping_new_logged_in.iterrows():\\n    session_id = row[\\'session_id\\']\\n    user_id = row[\\'user_id\\']\\n    \\n    # Generar recomendaciones basadas en popularidad\\n    recs = recommend_by_popularity(top_n=5)\\n    \\n    # Almacenar las recomendaciones\\n    user_recommendations[session_id] = recs\\n    \\n\\n# Filtrar las sesiones de \\'Usuario nuevo no logueado\\'\\ngroup_new_not_logged_in = test[test[\\'user_class\\'] == \\'Usuario nuevo no logueado\\']\\n\\nif not group_new_not_logged_in.empty:\\n    # Obtener las sesiones únicas\\n    unique_sessions_new_not_logged_in = group_new_not_logged_in[\\'session_id\\'].unique()\\n    print(f\"Número de sesiones únicas de \\'Usuario nuevo no logueado\\': {len(unique_sessions_new_not_logged_in)}\")\\n    \\n    for session_id in unique_sessions_new_not_logged_in:\\n        # Generar recomendaciones basadas en popularidad\\n        recs = recommend_by_popularity(top_n=5)\\n        \\n        # Almacenar las recomendaciones\\n        user_recommendations[session_id] = recs\\n\\n    # Verificar el total de sesiones con recomendaciones\\n    total_sessions_with_recommendations = len(user_recommendations)\\n    print(f\"Total de sesiones con recomendaciones después de \\'Usuario nuevo no logueado\\': {total_sessions_with_recommendations}\")\\nelse:\\n    print(\"No hay sesiones clasificadas como \\'Usuario nuevo no logueado\\'.\")\\n    \\n\\nimport json\\n# Convertir las claves de \\'user_recommendations\\' a cadenas\\nuser_recommendations_str_keys = {str(session_id): recs for session_id, recs in user_recommendations.items()}\\n\\n# Convertir los elementos de las listas de recomendaciones a enteros nativos de Python\\nfor session_id, recs in user_recommendations_str_keys.items():\\n    user_recommendations_str_keys[session_id] = [int(p) for p in recs]\\n\\n# Crear el dictado de salida\\noutput = {\\'target\\': user_recommendations_str_keys}\\n\\n# Guardar el archivo JSON\\nwith open(\\'/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_pipeline_v2.json\\', \\'w\\') as f:\\n    json.dump(output, f)\\n\\nprint(\"Archivo \\'predictions_3_pipeline_v2.json\\' generado con éxito.\")\\n\\nwith open(\\'/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions_save/predictions_3_pipeline_v2.json\\', \\'r\\') as f:\\n    data = json.load(f)\\n\\n# Verificar el número de sesiones\\nnum_sessions_in_json = len(data[\\'target\\'])\\nprint(f\"Número de sesiones en el JSON: {num_sessions_in_json}\")\\n\\n# Mostrar algunas recomendaciones\\nprint(\"Primeras 5 sesiones en el JSON:\")\\nfor session_id in list(data[\\'target\\'].keys())[:5]:\\n    recs = data[\\'target\\'][session_id]\\n    print(f\"Sesión {session_id}: {recs}\")\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import implicit\n",
    "\n",
    "\n",
    "product_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/products_data.pkl'\n",
    "user_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/user_data.csv'\n",
    "train_enriched_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/hybrid_model/train_preprocessed.pkl\"\n",
    "test_enriched_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/hybrid_model/test_preprocessed.pkl\"\n",
    "\n",
    "\n",
    "# Cargar los datasets\n",
    "products = pd.read_pickle(product_path)\n",
    "users = pd.read_csv(user_path)\n",
    "train = pd.read_pickle(train_enriched_path)\n",
    "test = pd.read_pickle(test_enriched_path)\n",
    "\n",
    "# Convertir a numérico\n",
    "train['pagetype'] = pd.to_numeric(train['pagetype'], errors='coerce')\n",
    "# Reemplazar los NaN y asignar el resultado de vuelta a la columna\n",
    "train['pagetype'] = train['pagetype'].fillna(-1)\n",
    "# Cambiar el tipo de dato\n",
    "train['pagetype'] = train['pagetype'].astype('int16')\n",
    "train_user_ids = set(train['user_id'].unique())\n",
    "train_user_ids.discard(-1)  # Remover -1 si representa usuarios no logueados\n",
    "# Asumimos que si hay 'partnumber', hay interacción\n",
    "test_sessions_with_interactions = set(test[test['partnumber'].notnull()]['session_id'].unique())\n",
    "\n",
    "\n",
    "\n",
    "def classify_session(row):\n",
    "    user_id = row['user_id']\n",
    "    session_id = row['session_id']\n",
    "    \n",
    "    if user_id == -1:\n",
    "        # Usuario no logueado\n",
    "        if session_id in test_sessions_with_interactions:\n",
    "            return 'Usuario recurrente no logueado'\n",
    "        else:\n",
    "            return 'Usuario nuevo no logueado'\n",
    "    else:\n",
    "        # Usuario logueado\n",
    "        if user_id in train_user_ids:\n",
    "            return 'Usuario recurrente logueado'\n",
    "        else:\n",
    "            return 'Usuario nuevo logueado'\n",
    "\n",
    "\n",
    "test['user_class'] = test.apply(classify_session, axis=1)\n",
    "\n",
    "# Modelo de Popularidad\n",
    "# Calcular la popularidad de los productos en el conjunto de entrenamiento\n",
    "product_popularity = train.groupby('partnumber')['add_to_cart'].sum().reset_index()\n",
    "product_popularity.rename(columns={'add_to_cart': 'popularity'}, inplace=True)\n",
    "\n",
    "# Ordenar los productos por popularidad descendente\n",
    "product_popularity.sort_values(by='popularity', ascending=False, inplace=True)\n",
    "\n",
    "# Obtener la lista de productos populares\n",
    "popular_products = product_popularity['partnumber'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Asegurarnos de que los embeddings son arrays de NumPy\n",
    "products['embedding'] = products['embedding'].apply(np.array)\n",
    "\n",
    "# Crear un diccionario {partnumber: embedding}\n",
    "embeddings_dict = dict(zip(products['partnumber'], products['embedding']))\n",
    "\n",
    "def find_similar_products(partnumber, top_n=5):\n",
    "    target_embedding = embeddings_dict.get(partnumber)\n",
    "    if not isinstance(target_embedding, np.ndarray):\n",
    "        print(f\"Advertencia: El embedding del producto {partnumber} es inválido.\")\n",
    "        return []\n",
    "\n",
    "    # Obtener todas las embeddings y los partnumbers correspondientes\n",
    "    all_partnumbers = []\n",
    "    all_embeddings = []\n",
    "    for pnum, emb in embeddings_dict.items():\n",
    "        if isinstance(emb, np.ndarray) and emb.shape == target_embedding.shape:\n",
    "            all_partnumbers.append(pnum)\n",
    "            all_embeddings.append(emb)\n",
    "\n",
    "    # Convertir a arrays de NumPy\n",
    "    all_embeddings = np.stack(all_embeddings)\n",
    "\n",
    "    # Calcular la similitud de coseno\n",
    "    similarities = cosine_similarity([target_embedding], all_embeddings)[0]\n",
    "\n",
    "    # Obtener los índices de los productos más similares (excluyendo el propio producto)\n",
    "    similar_indices = similarities.argsort()[::-1]\n",
    "    similar_partnumbers = []\n",
    "    for idx in similar_indices:\n",
    "        if all_partnumbers[idx] != partnumber:\n",
    "            similar_partnumbers.append(all_partnumbers[idx])\n",
    "        if len(similar_partnumbers) == top_n:\n",
    "            break\n",
    "\n",
    "    return similar_partnumbers\n",
    "\n",
    "# Seleccionar un producto para probar, por ejemplo, el más popular\n",
    "test_partnumber = popular_products[0]  # Primer producto de la lista de populares\n",
    "\n",
    "# Obtener productos similares\n",
    "similar_products = find_similar_products(test_partnumber, top_n=5)\n",
    "\n",
    "\n",
    "# Filtrar interacciones de usuarios logueados y crear una copia\n",
    "train_logged_in = train[train['user_id'] != -1].copy()\n",
    "\n",
    "\n",
    "# Obtener user_ids únicos\n",
    "unique_user_ids = train_logged_in['user_id'].unique()\n",
    "\n",
    "# Crear diccionarios de mapeo\n",
    "user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "index_to_user_id = {idx: user_id for user_id, idx in user_id_to_index.items()}\n",
    "\n",
    "# Mapear 'user_id' a 'user_idx' en train_logged_in\n",
    "train_logged_in['user_idx'] = train_logged_in['user_id'].map(user_id_to_index)\n",
    "\n",
    "# Obtener partnumbers únicos\n",
    "unique_partnumbers = train_logged_in['partnumber'].unique()\n",
    "\n",
    "# Crear diccionarios de mapeo\n",
    "partnumber_to_index = {partnumber: idx for idx, partnumber in enumerate(unique_partnumbers)}\n",
    "index_to_partnumber = {idx: partnumber for partnumber, idx in partnumber_to_index.items()}\n",
    "\n",
    "# Mapear 'partnumber' a 'item_idx' en train_logged_in\n",
    "train_logged_in['item_idx'] = train_logged_in['partnumber'].map(partnumber_to_index)\n",
    "\n",
    "# Construir la matriz de interacciones (usuarios x items)\n",
    "interaction_matrix = csr_matrix(\n",
    "    (train_logged_in['add_to_cart'], (train_logged_in['user_idx'], train_logged_in['item_idx'])),\n",
    "    shape=(len(unique_user_ids), len(unique_partnumbers))\n",
    ")\n",
    "\n",
    "# Configurar el modelo (no transponemos la matriz)\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50, iterations=10, regularization=0.1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(interaction_matrix)\n",
    "\n",
    "user_idx = 0  # Puedes elegir cualquier índice válido\n",
    "user_id = index_to_user_id[user_idx]\n",
    "\n",
    "# Obtener las interacciones del usuario\n",
    "user_items = interaction_matrix[user_idx]\n",
    "\n",
    "# Obtener recomendaciones\n",
    "recommended = model.recommend(user_idx, user_items, N=5)\n",
    "\n",
    "if isinstance(recommended, list):\n",
    "    # recommended es una lista de tuplas (item_idx, score)\n",
    "    recommended_indices = [item_idx for item_idx, score in recommended]\n",
    "elif isinstance(recommended, tuple) and len(recommended) == 2:\n",
    "    # recommended es una tupla de (indices, scores)\n",
    "    recommended_indices = recommended[0].tolist()\n",
    "elif isinstance(recommended, np.ndarray):\n",
    "    # recommended es un array de índices\n",
    "    recommended_indices = recommended.tolist()\n",
    "else:\n",
    "    print(\"Formato de 'recommended' no reconocido.\")\n",
    "    recommended_indices = []\n",
    "    \n",
    "    \n",
    "# Verificar si los índices existen en 'index_to_partnumber'\n",
    "missing_indices = [idx for idx in recommended_indices if idx not in index_to_partnumber]\n",
    "\n",
    "if missing_indices:\n",
    "    print(f\"Los siguientes índices no se encuentran en 'index_to_partnumber': {missing_indices}\")\n",
    "else:\n",
    "    print(\"Todos los índices recomendados están en 'index_to_partnumber'\")\n",
    "    \n",
    "    # Mapear los índices a 'partnumber'\n",
    "recommended_partnumbers = [index_to_partnumber[idx] for idx in recommended_indices]\n",
    "\n",
    "group = test[test['user_class'] == 'Usuario recurrente logueado']\n",
    "total_rows = group.shape[0]\n",
    "num_sessions = group['session_id'].nunique()\n",
    "\n",
    "# Filtrar las sesiones de 'Usuario recurrente logueado'\n",
    "group = test[test['user_class'] == 'Usuario recurrente logueado']\n",
    "\n",
    "# Obtener las sesiones únicas y los user_id asociados\n",
    "session_user_mapping = group[['session_id', 'user_id']].drop_duplicates()\n",
    "\n",
    "def recommend_by_collaborative(user_id, top_n=5):\n",
    "    # Obtener el índice del usuario\n",
    "    user_idx = user_id_to_index.get(user_id)\n",
    "    \n",
    "    if user_idx is None:\n",
    "        # Si el usuario no está en el modelo, retornamos una lista vacía o usamos popularidad\n",
    "        print(f\"Usuario {user_id} no encontrado en el modelo colaborativo.\")\n",
    "        return []\n",
    "    \n",
    "    # Obtener las interacciones del usuario\n",
    "    user_items = interaction_matrix[user_idx]\n",
    "    \n",
    "    # Obtener recomendaciones del modelo\n",
    "    recommended = model.recommend(user_idx, user_items, N=top_n)\n",
    "    \n",
    "    # Ajustar según el tipo de 'recommended'\n",
    "    if isinstance(recommended, list):\n",
    "        # recommended es una lista de tuplas (item_idx, score)\n",
    "        recommended_indices = [item_idx for item_idx, score in recommended]\n",
    "    elif isinstance(recommended, tuple) and len(recommended) == 2:\n",
    "        # recommended es una tupla de (indices, scores)\n",
    "        recommended_indices = recommended[0].tolist()\n",
    "    elif isinstance(recommended, np.ndarray):\n",
    "        # recommended es un array de índices\n",
    "        recommended_indices = recommended.tolist()\n",
    "    else:\n",
    "        print(\"Formato de 'recommended' no reconocido.\")\n",
    "        recommended_indices = []\n",
    "    \n",
    "    # Verificar si los índices existen en 'index_to_partnumber'\n",
    "    recommended_indices = [idx for idx in recommended_indices if idx in index_to_partnumber]\n",
    "    \n",
    "    # Mapear los índices a 'partnumber'\n",
    "    recommended_partnumbers = [index_to_partnumber[idx] for idx in recommended_indices]\n",
    "    \n",
    "    return recommended_partnumbers\n",
    "\n",
    "# Seleccionar un user_id de prueba (asegúrate de que el user_id existe en user_id_to_index)\n",
    "user_id = unique_user_ids[0]\n",
    "recommended_partnumbers = recommend_by_collaborative(user_id, top_n=5)\n",
    "\n",
    "# Mantener el diccionario 'user_recommendations' si ya lo tienes definido\n",
    "# De lo contrario, inicialízalo\n",
    "user_recommendations = {}\n",
    "\n",
    "# Iterar sobre cada sesión y user_id asociados\n",
    "for _, row in session_user_mapping.iterrows():\n",
    "    session_id = row['session_id']\n",
    "    user_id = row['user_id']\n",
    "    \n",
    "    # Obtener recomendaciones colaborativas\n",
    "    recs = recommend_by_collaborative(user_id, top_n=10)  # Usamos top_n=10 para tener margen\n",
    "    \n",
    "    # Obtener productos ya interactuados por el usuario en el conjunto de entrenamiento\n",
    "    user_interacted_items = train_logged_in[train_logged_in['user_id'] == user_id]['partnumber'].unique()\n",
    "    \n",
    "    # Eliminar productos ya vistos\n",
    "    recs = [p for p in recs if p not in user_interacted_items]\n",
    "    \n",
    "    # Si después de filtrar no tenemos suficientes recomendaciones, complementamos con popularidad\n",
    "    if len(recs) < 5:\n",
    "        more_recs = [p for p in popular_products if p not in recs and p not in user_interacted_items]\n",
    "        recs.extend(more_recs[:5 - len(recs)])\n",
    "    \n",
    "    # Aseguramos 5 recomendaciones únicas\n",
    "    recs = list(dict.fromkeys(recs))[:5]\n",
    "    \n",
    "    # Almacenar las recomendaciones\n",
    "    user_recommendations[session_id] = recs\n",
    "    \n",
    "    \n",
    "# Mostrar las recomendaciones para las primeras 5 sesiones\n",
    "session_ids = list(user_recommendations.keys())\n",
    "for session_id in session_ids[:5]:\n",
    "    recs = user_recommendations[session_id]\n",
    "    print(f\"Sesión {session_id} - Recomendaciones: {recs}\")\n",
    "    \n",
    "\n",
    "# Obtener el número total de sesiones en el conjunto de prueba\n",
    "total_sessions_in_test = test['session_id'].nunique()\n",
    "\n",
    "# Obtener el número total de sesiones para las que hemos generado recomendaciones\n",
    "total_sessions_with_recommendations = len(user_recommendations)\n",
    "\n",
    "print(f\"Total de sesiones en el conjunto de prueba: {total_sessions_in_test}\")\n",
    "print(f\"Total de sesiones con recomendaciones: {total_sessions_with_recommendations}\")\n",
    "\n",
    "if total_sessions_in_test == total_sessions_with_recommendations:\n",
    "    print(\"Todas las sesiones tienen recomendaciones.\")\n",
    "else:\n",
    "    print(f\"Faltan recomendaciones para {total_sessions_in_test - total_sessions_with_recommendations} sesiones.\")\n",
    "    \n",
    "\n",
    "# Filtrar las sesiones de 'Usuario recurrente no logueado'\n",
    "group = test[test['user_class'] == 'Usuario recurrente no logueado']\n",
    "\n",
    "# Obtener las sesiones únicas\n",
    "unique_sessions = group['session_id'].unique()\n",
    "\n",
    "def recommend_by_content(partnumbers_interacted, top_n=5):\n",
    "    recommendations = []\n",
    "    for partnumber in partnumbers_interacted:\n",
    "        # Obtener productos similares\n",
    "        similar_products = find_similar_products(partnumber, top_n=top_n)\n",
    "        # Añadir los productos similares a la lista de recomendaciones\n",
    "        recommendations.extend(similar_products)\n",
    "    \n",
    "    # Eliminar productos ya vistos y duplicados\n",
    "    recommendations = [p for p in recommendations if p not in partnumbers_interacted]\n",
    "    recommendations = list(dict.fromkeys(recommendations))\n",
    "    \n",
    "    return recommendations[:top_n]\n",
    "\n",
    "# Seleccionar un 'partnumber' de ejemplo con el que haya interactuado un usuario\n",
    "test_partnumbers_interacted = [40779]  # Este es el 'partnumber' más popular según nuestro dato previo\n",
    "\n",
    "# Generar recomendaciones basadas en contenido\n",
    "recs_content = recommend_by_content(test_partnumbers_interacted, top_n=5)\n",
    "\n",
    "\n",
    "def recommend_by_popularity(top_n=5):\n",
    "    return popular_products[:top_n]\n",
    "\n",
    "# Continuamos usando el diccionario 'user_recommendations' existente\n",
    "\n",
    "for session_id in unique_sessions:\n",
    "    # Obtener los 'partnumber' con los que interactuó en la sesión\n",
    "    session_data = group[group['session_id'] == session_id]\n",
    "    partnumbers_interacted = session_data['partnumber'].unique()\n",
    "    \n",
    "    if len(partnumbers_interacted) > 0:\n",
    "        # Generar recomendaciones basadas en contenido\n",
    "        recs = recommend_by_content(partnumbers_interacted, top_n=10)\n",
    "    else:\n",
    "        # Si no hay interacciones, usar popularidad\n",
    "        recs = recommend_by_popularity(top_n=5)\n",
    "    \n",
    "    # Aseguramos 5 recomendaciones únicas\n",
    "    recs = list(dict.fromkeys(recs))[:5]\n",
    "    \n",
    "    # Almacenar las recomendaciones en el diccionario existente\n",
    "    user_recommendations[session_id] = recs\n",
    "    \n",
    "\n",
    "# Filtrar las sesiones de 'Usuario nuevo logueado'\n",
    "group_new_logged_in = test[test['user_class'] == 'Usuario nuevo logueado']\n",
    "\n",
    "# Obtener las sesiones únicas y los 'user_id' asociados\n",
    "session_user_mapping_new_logged_in = group_new_logged_in[['session_id', 'user_id']].drop_duplicates()\n",
    "\n",
    "# Continuamos utilizando el diccionario 'user_recommendations'\n",
    "\n",
    "for _, row in session_user_mapping_new_logged_in.iterrows():\n",
    "    session_id = row['session_id']\n",
    "    user_id = row['user_id']\n",
    "    \n",
    "    # Generar recomendaciones basadas en popularidad\n",
    "    recs = recommend_by_popularity(top_n=5)\n",
    "    \n",
    "    # Almacenar las recomendaciones\n",
    "    user_recommendations[session_id] = recs\n",
    "    \n",
    "\n",
    "# Filtrar las sesiones de 'Usuario nuevo no logueado'\n",
    "group_new_not_logged_in = test[test['user_class'] == 'Usuario nuevo no logueado']\n",
    "\n",
    "if not group_new_not_logged_in.empty:\n",
    "    # Obtener las sesiones únicas\n",
    "    unique_sessions_new_not_logged_in = group_new_not_logged_in['session_id'].unique()\n",
    "    print(f\"Número de sesiones únicas de 'Usuario nuevo no logueado': {len(unique_sessions_new_not_logged_in)}\")\n",
    "    \n",
    "    for session_id in unique_sessions_new_not_logged_in:\n",
    "        # Generar recomendaciones basadas en popularidad\n",
    "        recs = recommend_by_popularity(top_n=5)\n",
    "        \n",
    "        # Almacenar las recomendaciones\n",
    "        user_recommendations[session_id] = recs\n",
    "\n",
    "    # Verificar el total de sesiones con recomendaciones\n",
    "    total_sessions_with_recommendations = len(user_recommendations)\n",
    "    print(f\"Total de sesiones con recomendaciones después de 'Usuario nuevo no logueado': {total_sessions_with_recommendations}\")\n",
    "else:\n",
    "    print(\"No hay sesiones clasificadas como 'Usuario nuevo no logueado'.\")\n",
    "    \n",
    "\n",
    "import json\n",
    "# Convertir las claves de 'user_recommendations' a cadenas\n",
    "user_recommendations_str_keys = {str(session_id): recs for session_id, recs in user_recommendations.items()}\n",
    "\n",
    "# Convertir los elementos de las listas de recomendaciones a enteros nativos de Python\n",
    "for session_id, recs in user_recommendations_str_keys.items():\n",
    "    user_recommendations_str_keys[session_id] = [int(p) for p in recs]\n",
    "\n",
    "# Crear el dictado de salida\n",
    "output = {'target': user_recommendations_str_keys}\n",
    "\n",
    "# Guardar el archivo JSON\n",
    "with open('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_pipeline_v2.json', 'w') as f:\n",
    "    json.dump(output, f)\n",
    "\n",
    "print(\"Archivo 'predictions_3_pipeline_v2.json' generado con éxito.\")\n",
    "\n",
    "with open('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions_save/predictions_3_pipeline_v2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Verificar el número de sesiones\n",
    "num_sessions_in_json = len(data['target'])\n",
    "print(f\"Número de sesiones en el JSON: {num_sessions_in_json}\")\n",
    "\n",
    "# Mostrar algunas recomendaciones\n",
    "print(\"Primeras 5 sesiones en el JSON:\")\n",
    "for session_id in list(data['target'].keys())[:5]:\n",
    "    recs = data['target'][session_id]\n",
    "    print(f\"Sesión {session_id}: {recs}\")\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Refractorización del pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloque de Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/implicit/gpu/__init__.py:13: UserWarning: CUDA extension is built, but disabling GPU support because of 'Cuda Error: no CUDA-capable device is detected (/project/./implicit/gpu/utils.h:71)'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import implicit\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Definir rutas de los archivos\n",
    "product_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/products_data.pkl'\n",
    "user_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/user_data.csv'\n",
    "train_enriched_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/hybrid_model/train_preprocessed.pkl\"\n",
    "test_enriched_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/hybrid_model/test_preprocessed.pkl\"\n",
    "\n",
    "# Cargar los datasets\n",
    "products = pd.read_pickle(product_path)\n",
    "users = pd.read_csv(user_path)\n",
    "train = pd.read_pickle(train_enriched_path)\n",
    "test = pd.read_pickle(test_enriched_path)\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "def preprocess_data(train, test):\n",
    "    # Convertir 'pagetype' a numérico en 'train'\n",
    "    train['pagetype'] = pd.to_numeric(train['pagetype'], errors='coerce')\n",
    "    train['pagetype'] = train['pagetype'].fillna(-1)\n",
    "    train['pagetype'] = train['pagetype'].astype('int16')\n",
    "    \n",
    "    # Obtener los user_id únicos en 'train'\n",
    "    train_user_ids = set(train['user_id'].unique())\n",
    "    train_user_ids.discard(-1)  # Remover -1 si representa usuarios no logueados\n",
    "\n",
    "    # Identificar sesiones con interacciones en 'test'\n",
    "    test_sessions_with_interactions = set(test[test['partnumber'].notnull()]['session_id'].unique())\n",
    "    \n",
    "    return train_user_ids, test_sessions_with_interactions\n",
    "\n",
    "# Clasificar sesiones\n",
    "def classify_sessions(test, train_user_ids, test_sessions_with_interactions):\n",
    "    def classify_session(row):\n",
    "        user_id = row['user_id']\n",
    "        session_id = row['session_id']\n",
    "        \n",
    "        if user_id == -1:\n",
    "            # Usuario no logueado\n",
    "            if session_id in test_sessions_with_interactions:\n",
    "                return 'Usuario recurrente no logueado'\n",
    "            else:\n",
    "                return 'Usuario nuevo no logueado'\n",
    "        else:\n",
    "            # Usuario logueado\n",
    "            if user_id in train_user_ids:\n",
    "                return 'Usuario recurrente logueado'\n",
    "            else:\n",
    "                return 'Usuario nuevo logueado'\n",
    "    test['user_class'] = test.apply(classify_session, axis=1)\n",
    "    return test\n",
    "\n",
    "# Ejecutar preprocesamiento y clasificación\n",
    "train_user_ids, test_sessions_with_interactions = preprocess_data(train, test)\n",
    "test = classify_sessions(test, train_user_ids, test_sessions_with_interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de cada Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Popularidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_popularity_model(train):\n",
    "    # Calcular la popularidad de los productos en el conjunto de entrenamiento\n",
    "    product_popularity = train.groupby('partnumber')['add_to_cart'].sum().reset_index()\n",
    "    product_popularity.rename(columns={'add_to_cart': 'popularity'}, inplace=True)\n",
    "    product_popularity.sort_values(by='popularity', ascending=False, inplace=True)\n",
    "    popular_products = product_popularity['partnumber'].tolist()\n",
    "    return popular_products\n",
    "\n",
    "# Entrenar el modelo de popularidad\n",
    "popular_products = train_popularity_model(train)\n",
    "\n",
    "def recommend_by_popularity(popular_products, top_n=5):\n",
    "    return popular_products[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Basado en Contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_content_model(products):\n",
    "    # Asegurarnos de que los embeddings son arrays de NumPy\n",
    "    products['embedding'] = products['embedding'].apply(np.array)\n",
    "    # Crear un diccionario {partnumber: embedding}\n",
    "    embeddings_dict = dict(zip(products['partnumber'], products['embedding']))\n",
    "    return embeddings_dict\n",
    "\n",
    "def find_similar_products(partnumber, embeddings_dict, top_n=5):\n",
    "    target_embedding = embeddings_dict.get(partnumber)\n",
    "    if not isinstance(target_embedding, np.ndarray):\n",
    "        print(f\"Advertencia: El embedding del producto {partnumber} es inválido.\")\n",
    "        return []\n",
    "\n",
    "    # Obtener todas las embeddings y los partnumbers correspondientes\n",
    "    all_partnumbers = []\n",
    "    all_embeddings = []\n",
    "    for pnum, emb in embeddings_dict.items():\n",
    "        if isinstance(emb, np.ndarray) and emb.shape == target_embedding.shape:\n",
    "            all_partnumbers.append(pnum)\n",
    "            all_embeddings.append(emb)\n",
    "\n",
    "    # Convertir a arrays de NumPy\n",
    "    all_embeddings = np.stack(all_embeddings)\n",
    "\n",
    "    # Calcular la similitud de coseno\n",
    "    similarities = cosine_similarity([target_embedding], all_embeddings)[0]\n",
    "\n",
    "    # Obtener los índices de los productos más similares (excluyendo el propio producto)\n",
    "    similar_indices = similarities.argsort()[::-1]\n",
    "    similar_partnumbers = []\n",
    "    for idx in similar_indices:\n",
    "        if all_partnumbers[idx] != partnumber:\n",
    "            similar_partnumbers.append(all_partnumbers[idx])\n",
    "        if len(similar_partnumbers) == top_n:\n",
    "            break\n",
    "\n",
    "    return similar_partnumbers\n",
    "\n",
    "def recommend_by_content(partnumbers_interacted, embeddings_dict, top_n=5):\n",
    "    recommendations = []\n",
    "    for partnumber in partnumbers_interacted:\n",
    "        # Obtener productos similares\n",
    "        similar_products = find_similar_products(partnumber, embeddings_dict, top_n=top_n)\n",
    "        # Añadir los productos similares a la lista de recomendaciones\n",
    "        recommendations.extend(similar_products)\n",
    "\n",
    "    # Eliminar productos ya vistos y duplicados\n",
    "    recommendations = [p for p in recommendations if p not in partnumbers_interacted]\n",
    "    recommendations = list(dict.fromkeys(recommendations))\n",
    "\n",
    "    return recommendations[:top_n]\n",
    "\n",
    "# Preparar el modelo basado en contenido\n",
    "embeddings_dict = prepare_content_model(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Colaborativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 10 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_collaborative_model(train_logged_in):\n",
    "    # Obtener user_ids únicos\n",
    "    unique_user_ids = train_logged_in['user_id'].unique()\n",
    "    # Crear diccionarios de mapeo\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "    index_to_user_id = {idx: user_id for user_id, idx in user_id_to_index.items()}\n",
    "    # Mapear 'user_id' a 'user_idx' en train_logged_in\n",
    "    train_logged_in['user_idx'] = train_logged_in['user_id'].map(user_id_to_index)\n",
    "    # Obtener partnumbers únicos\n",
    "    unique_partnumbers = train_logged_in['partnumber'].unique()\n",
    "    # Crear diccionarios de mapeo\n",
    "    partnumber_to_index = {partnumber: idx for idx, partnumber in enumerate(unique_partnumbers)}\n",
    "    index_to_partnumber = {idx: partnumber for partnumber, idx in partnumber_to_index.items()}\n",
    "    # Mapear 'partnumber' a 'item_idx' en train_logged_in\n",
    "    train_logged_in['item_idx'] = train_logged_in['partnumber'].map(partnumber_to_index)\n",
    "    # Construir la matriz de interacciones (usuarios x items)\n",
    "    interaction_matrix = csr_matrix(\n",
    "        (train_logged_in['add_to_cart'], (train_logged_in['user_idx'], train_logged_in['item_idx'])),\n",
    "        shape=(len(unique_user_ids), len(unique_partnumbers))\n",
    "    )\n",
    "    # Configurar el modelo (no transponemos la matriz)\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=50, iterations=10, regularization=0.1)\n",
    "    # Entrenar el modelo\n",
    "    model.fit(interaction_matrix)\n",
    "    return model, interaction_matrix, user_id_to_index, index_to_user_id, index_to_partnumber\n",
    "\n",
    "def recommend_by_collaborative(user_id, model, interaction_matrix, user_id_to_index, index_to_partnumber, top_n=5):\n",
    "    # Obtener el índice del usuario\n",
    "    user_idx = user_id_to_index.get(user_id)\n",
    "    if user_idx is None:\n",
    "        # Si el usuario no está en el modelo, retornamos una lista vacía o usamos popularidad\n",
    "        print(f\"Usuario {user_id} no encontrado en el modelo colaborativo.\")\n",
    "        return []\n",
    "\n",
    "    # Obtener las interacciones del usuario\n",
    "    user_items = interaction_matrix[user_idx]\n",
    "\n",
    "    # Obtener recomendaciones del modelo\n",
    "    recommended = model.recommend(user_idx, user_items, N=top_n)\n",
    "\n",
    "    # Ajustar según el tipo de 'recommended'\n",
    "    if isinstance(recommended, list):\n",
    "        # recommended es una lista de tuplas (item_idx, score)\n",
    "        recommended_indices = [item_idx for item_idx, score in recommended]\n",
    "    elif isinstance(recommended, tuple) and len(recommended) == 2:\n",
    "        # recommended es una tupla de (indices, scores)\n",
    "        recommended_indices = recommended[0].tolist()\n",
    "    elif isinstance(recommended, np.ndarray):\n",
    "        # recommended es un array de índices\n",
    "        recommended_indices = recommended.tolist()\n",
    "    else:\n",
    "        print(\"Formato de 'recommended' no reconocido.\")\n",
    "        recommended_indices = []\n",
    "\n",
    "    # Verificar si los índices existen en 'index_to_partnumber'\n",
    "    recommended_indices = [idx for idx in recommended_indices if idx in index_to_partnumber]\n",
    "\n",
    "    # Mapear los índices a 'partnumber'\n",
    "    recommended_partnumbers = [index_to_partnumber[idx] for idx in recommended_indices]\n",
    "\n",
    "    return recommended_partnumbers\n",
    "\n",
    "# Preparar el modelo colaborativo\n",
    "train_logged_in = train[train['user_id'] != -1].copy()\n",
    "model_collaborative, interaction_matrix, user_id_to_index, index_to_user_id, index_to_partnumber = prepare_collaborative_model(train_logged_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m user_recommendations\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Generar todas las recomendaciones\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m user_recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_all_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Verificación de que todas las sesiones tienen recomendaciones\u001b[39;00m\n\u001b[1;32m     49\u001b[0m total_sessions_in_test \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mgenerate_all_recommendations\u001b[0;34m(test)\u001b[0m\n\u001b[1;32m     39\u001b[0m     user_class \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_class\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m     session_data \u001b[38;5;241m=\u001b[39m test[test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m session_id]\n\u001b[0;32m---> 41\u001b[0m     recs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_recommendations_for_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     user_recommendations[session_id] \u001b[38;5;241m=\u001b[39m recs\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_recommendations\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mgenerate_recommendations_for_session\u001b[0;34m(session_id, user_id, session_data, user_class)\u001b[0m\n\u001b[1;32m     18\u001b[0m partnumbers_interacted \u001b[38;5;241m=\u001b[39m session_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartnumber\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(partnumbers_interacted) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     recs \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_by_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartnumbers_interacted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     recs \u001b[38;5;241m=\u001b[39m recommend_by_popularity(popular_products, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mrecommend_by_content\u001b[0;34m(partnumbers_interacted, embeddings_dict, top_n)\u001b[0m\n\u001b[1;32m     40\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m partnumber \u001b[38;5;129;01min\u001b[39;00m partnumbers_interacted:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Obtener productos similares\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     similar_products \u001b[38;5;241m=\u001b[39m \u001b[43mfind_similar_products\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartnumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Añadir los productos similares a la lista de recomendaciones\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     recommendations\u001b[38;5;241m.\u001b[39mextend(similar_products)\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mfind_similar_products\u001b[0;34m(partnumber, embeddings_dict, top_n)\u001b[0m\n\u001b[1;32m     20\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mappend(emb)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Convertir a arrays de NumPy\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calcular la similitud de coseno\u001b[39;00m\n\u001b[1;32m     26\u001b[0m similarities \u001b[38;5;241m=\u001b[39m cosine_similarity([target_embedding], all_embeddings)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/numpy/core/shape_base.py:456\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    454\u001b[0m sl \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) \u001b[38;5;241m*\u001b[39m axis \u001b[38;5;241m+\u001b[39m (_nx\u001b[38;5;241m.\u001b[39mnewaxis,)\n\u001b[1;32m    455\u001b[0m expanded_arrays \u001b[38;5;241m=\u001b[39m [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Función para generar recomendaciones por sesión\n",
    "def generate_recommendations_for_session(session_id, user_id, session_data, user_class):\n",
    "    if user_class == 'Usuario recurrente logueado':\n",
    "        # Usar modelo colaborativo\n",
    "        recs = recommend_by_collaborative(\n",
    "            user_id, model_collaborative, interaction_matrix, user_id_to_index, index_to_partnumber, top_n=10\n",
    "        )\n",
    "        # Obtener productos ya interactuados por el usuario en el conjunto de entrenamiento\n",
    "        user_interacted_items = train_logged_in[train_logged_in['user_id'] == user_id]['partnumber'].unique()\n",
    "        # Eliminar productos ya vistos\n",
    "        recs = [p for p in recs if p not in user_interacted_items]\n",
    "        # Si después de filtrar no tenemos suficientes recomendaciones, complementamos con popularidad\n",
    "        if len(recs) < 5:\n",
    "            more_recs = [p for p in popular_products if p not in recs and p not in user_interacted_items]\n",
    "            recs.extend(more_recs[:5 - len(recs)])\n",
    "    elif user_class == 'Usuario recurrente no logueado':\n",
    "        # Usar modelo basado en contenido\n",
    "        partnumbers_interacted = session_data['partnumber'].unique()\n",
    "        if len(partnumbers_interacted) > 0:\n",
    "            recs = recommend_by_content(partnumbers_interacted, embeddings_dict, top_n=10)\n",
    "        else:\n",
    "            recs = recommend_by_popularity(popular_products, top_n=5)\n",
    "    else:  # 'Usuario nuevo logueado' o 'Usuario nuevo no logueado'\n",
    "        # Usar popularidad\n",
    "        recs = recommend_by_popularity(popular_products, top_n=5)\n",
    "    # Asegurar 5 recomendaciones únicas\n",
    "    recs = list(dict.fromkeys(recs))[:5]\n",
    "    return recs\n",
    "\n",
    "# Generar recomendaciones para todas las sesiones\n",
    "def generate_all_recommendations(test):\n",
    "    user_recommendations = {}\n",
    "    # Iterar sobre todas las sesiones únicas\n",
    "    sessions = test[['session_id', 'user_id', 'user_class']].drop_duplicates()\n",
    "\n",
    "    for _, row in sessions.iterrows():\n",
    "        session_id = row['session_id']\n",
    "        user_id = row['user_id']\n",
    "        user_class = row['user_class']\n",
    "        session_data = test[test['session_id'] == session_id]\n",
    "        recs = generate_recommendations_for_session(session_id, user_id, session_data, user_class)\n",
    "        user_recommendations[session_id] = recs\n",
    "    return user_recommendations\n",
    "\n",
    "# Generar todas las recomendaciones\n",
    "user_recommendations = generate_all_recommendations(test)\n",
    "\n",
    "# Verificación de que todas las sesiones tienen recomendaciones\n",
    "total_sessions_in_test = test['session_id'].nunique()\n",
    "total_sessions_with_recommendations = len(user_recommendations)\n",
    "\n",
    "print(f\"Total de sesiones en el conjunto de prueba: {total_sessions_in_test}\")\n",
    "print(f\"Total de sesiones con recomendaciones: {total_sessions_with_recommendations}\")\n",
    "\n",
    "if total_sessions_in_test == total_sessions_with_recommendations:\n",
    "    print(\"Todas las sesiones tienen recomendaciones.\")\n",
    "else:\n",
    "    print(f\"Faltan recomendaciones para {total_sessions_in_test - total_sessions_with_recommendations} sesiones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación del Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos para el JSON\n",
    "def prepare_output_for_json(user_recommendations):\n",
    "    # Convertir las claves de 'user_recommendations' a cadenas\n",
    "    user_recommendations_str_keys = {str(session_id): recs for session_id, recs in user_recommendations.items()}\n",
    "    # Convertir los elementos de las listas de recomendaciones a enteros nativos de Python\n",
    "    for session_id, recs in user_recommendations_str_keys.items():\n",
    "        user_recommendations_str_keys[session_id] = [int(p) for p in recs]\n",
    "    return user_recommendations_str_keys\n",
    "\n",
    "# Guardar el archivo JSON\n",
    "def save_recommendations_to_json(user_recommendations_str_keys, output_path):\n",
    "    output = {'target': user_recommendations_str_keys}\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(output, f)\n",
    "    print(f\"Archivo '{output_path}' generado con éxito.\")\n",
    "\n",
    "# Ruta de salida para el JSON\n",
    "output_json_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_pipeline_v2.json'\n",
    "\n",
    "# Preparar y guardar el JSON\n",
    "user_recommendations_str_keys = prepare_output_for_json(user_recommendations)\n",
    "save_recommendations_to_json(user_recommendations_str_keys, output_json_path)\n",
    "\n",
    "# Verificar el contenido del JSON\n",
    "def verify_json_output(output_path):\n",
    "    with open(output_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Verificar el número de sesiones\n",
    "    num_sessions_in_json = len(data['target'])\n",
    "    print(f\"Número de sesiones en el JSON: {num_sessions_in_json}\")\n",
    "    # Mostrar algunas recomendaciones\n",
    "    print(\"Primeras 5 sesiones en el JSON:\")\n",
    "    for session_id in list(data['target'].keys())[:5]:\n",
    "        recs = data['target'][session_id]\n",
    "        print(f\"Sesión {session_id}: {recs}\")\n",
    "\n",
    "# Verificar el JSON generado\n",
    "verify_json_output(output_json_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
