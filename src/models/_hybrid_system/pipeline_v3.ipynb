{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 10:02:18.631435: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-13 10:02:18.761481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736758938.809560    1405 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736758938.822557    1405 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-13 10:02:18.938450: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Refractorización del pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloque de Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import implicit\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Definir rutas de los archivos\n",
    "product_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/products_data.pkl'\n",
    "user_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/user_data.csv'\n",
    "train_enriched_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/hybrid_model/train_preprocessed.pkl\"\n",
    "test_enriched_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/hybrid_model/test_preprocessed.pkl\"\n",
    "\n",
    "# Cargar los datasets\n",
    "products = pd.read_pickle(product_path)\n",
    "users = pd.read_csv(user_path)\n",
    "train = pd.read_pickle(train_enriched_path)\n",
    "test = pd.read_pickle(test_enriched_path)\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "def preprocess_data(train, test):\n",
    "    # Convertir 'pagetype' a numérico en 'train'\n",
    "    train['pagetype'] = pd.to_numeric(train['pagetype'], errors='coerce')\n",
    "    train['pagetype'] = train['pagetype'].fillna(-1)\n",
    "    train['pagetype'] = train['pagetype'].astype('int16')\n",
    "    \n",
    "    # Obtener los user_id únicos en 'train'\n",
    "    train_user_ids = set(train['user_id'].unique())\n",
    "    train_user_ids.discard(-1)  # Remover -1 si representa usuarios no logueados\n",
    "\n",
    "    # Identificar sesiones con interacciones en 'test'\n",
    "    test_sessions_with_interactions = set(test[test['partnumber'].notnull()]['session_id'].unique())\n",
    "    \n",
    "    return train_user_ids, test_sessions_with_interactions\n",
    "\n",
    "# Clasificar sesiones\n",
    "def classify_sessions(test, train_user_ids, test_sessions_with_interactions):\n",
    "    def classify_session(row):\n",
    "        user_id = row['user_id']\n",
    "        session_id = row['session_id']\n",
    "        \n",
    "        if user_id == -1:\n",
    "            # Usuario no logueado\n",
    "            if session_id in test_sessions_with_interactions:\n",
    "                return 'Usuario recurrente no logueado'\n",
    "            else:\n",
    "                return 'Usuario nuevo no logueado'\n",
    "        else:\n",
    "            # Usuario logueado\n",
    "            if user_id in train_user_ids:\n",
    "                return 'Usuario recurrente logueado'\n",
    "            else:\n",
    "                return 'Usuario nuevo logueado'\n",
    "    test['user_class'] = test.apply(classify_session, axis=1)\n",
    "    return test\n",
    "\n",
    "# Ejecutar preprocesamiento y clasificación\n",
    "train_user_ids, test_sessions_with_interactions = preprocess_data(train, test)\n",
    "test = classify_sessions(test, train_user_ids, test_sessions_with_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de test Index(['session_id', 'date', 'timestamp_local', 'user_id', 'country',\n",
      "       'partnumber', 'device_type', 'pagetype', 'user_class'],\n",
      "      dtype='object')\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(f'Columnas de test {test.columns}')\n",
    "print(len(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de train Index(['session_id', 'date', 'timestamp_local', 'add_to_cart', 'user_id',\n",
      "       'country', 'partnumber', 'device_type', 'pagetype'],\n",
      "      dtype='object')\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(f'Columnas de train {train.columns}')\n",
    "print(len(train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of           session_id       date         timestamp_local  add_to_cart  user_id  \\\n",
      "10725642     4696487 2024-06-15 2024-06-15 11:02:27.710            1       -1   \n",
      "35628113     4572779 2024-06-15 2024-06-15 03:43:20.699            1       -1   \n",
      "35628118     4572779 2024-06-15 2024-06-15 03:43:08.240            1       -1   \n",
      "9475628      3239459 2024-06-15 2024-06-15 03:37:24.111            1       -1   \n",
      "9475555      3239459 2024-06-15 2024-06-15 03:37:03.935            1       -1   \n",
      "...              ...        ...                     ...          ...      ...   \n",
      "11700533      150565 2024-06-01 2024-06-01 02:00:00.337            0       -1   \n",
      "14954883     4285254 2024-06-01 2024-06-01 02:00:00.127            0       -1   \n",
      "44709791     4040227 2024-06-01 2024-06-01 02:00:00.083            0    64048   \n",
      "2046107      2464407 2024-06-01 2024-06-01 02:00:00.056            0     4996   \n",
      "35407364     4747800 2024-06-01 2024-06-01 02:00:00.051            0       -1   \n",
      "\n",
      "          country  partnumber  device_type  pagetype  \n",
      "10725642       29        7473            3        24  \n",
      "35628113       34       38861            1        24  \n",
      "35628118       34        1208            1        24  \n",
      "9475628        34       34842            3        24  \n",
      "9475555        34       41976            3        24  \n",
      "...           ...         ...          ...       ...  \n",
      "11700533       57       11113            1        24  \n",
      "14954883       34       24156            1        24  \n",
      "44709791       25        6794            1        24  \n",
      "2046107        25       29189            1        24  \n",
      "35407364       34       36463            1        24  \n",
      "\n",
      "[36858893 rows x 9 columns]>\n",
      "<bound method DataFrame.info of        session_id       date         timestamp_local  user_id  country  \\\n",
      "0             746 2024-06-15 2024-06-15 18:36:47.390       -1       57   \n",
      "1             746 2024-06-15 2024-06-15 18:37:04.052       -1       57   \n",
      "2             746 2024-06-15 2024-06-15 18:37:48.159       -1       57   \n",
      "3             746 2024-06-15 2024-06-15 18:38:19.899       -1       57   \n",
      "4             746 2024-06-15 2024-06-15 18:38:46.492       -1       57   \n",
      "...           ...        ...                     ...      ...      ...   \n",
      "29270     5167195 2024-06-16 2024-06-16 13:13:10.152       -1       34   \n",
      "29271     5167272 2024-06-16 2024-06-16 18:58:15.562       -1       34   \n",
      "29272     5168338 2024-06-15 2024-06-15 21:36:15.931       -1       34   \n",
      "29273     5168492 2024-06-16 2024-06-16 04:06:03.528       -1       34   \n",
      "29274     5168933 2024-06-16 2024-06-16 04:53:26.427       -1       57   \n",
      "\n",
      "       partnumber  device_type  pagetype                      user_class  \n",
      "0            1254            1        24  Usuario recurrente no logueado  \n",
      "1           32544            1        24  Usuario recurrente no logueado  \n",
      "2           12639            1        24  Usuario recurrente no logueado  \n",
      "3           18048            1        24  Usuario recurrente no logueado  \n",
      "4           13295            1        24  Usuario recurrente no logueado  \n",
      "...           ...          ...       ...                             ...  \n",
      "29270       30113            1        24  Usuario recurrente no logueado  \n",
      "29271       28922            1         8  Usuario recurrente no logueado  \n",
      "29272       13467            1        24  Usuario recurrente no logueado  \n",
      "29273       13732            1        24  Usuario recurrente no logueado  \n",
      "29274        8969            1        24  Usuario recurrente no logueado  \n",
      "\n",
      "[29275 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(train.info)\n",
    "print(test.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de cada Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Popularidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_popularity_model(train):\n",
    "    # Calcular la popularidad de los productos en el conjunto de entrenamiento\n",
    "    product_popularity = train.groupby('partnumber')['add_to_cart'].sum().reset_index()\n",
    "    product_popularity.rename(columns={'add_to_cart': 'popularity'}, inplace=True)\n",
    "    product_popularity.sort_values(by='popularity', ascending=False, inplace=True)\n",
    "    popular_products = product_popularity['partnumber'].tolist()\n",
    "    return popular_products\n",
    "\n",
    "# Entrenar el modelo de popularidad\n",
    "popular_products = train_popularity_model(train)\n",
    "\n",
    "def recommend_by_popularity(popular_products, top_n=5):\n",
    "    return popular_products[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_popularity_model(train_data):\n",
    "    # Crear una copia explícita del DataFrame\n",
    "    train = train_data.copy()\n",
    "    \n",
    "    # 1. Considerar el tipo de página\n",
    "    page_weights = {\n",
    "        0: 1.0,    # producto\n",
    "        1: 0.5,    # categoría\n",
    "        2: 0.3,    # búsqueda\n",
    "        -1: 0.1    # otros\n",
    "    }\n",
    "    train.loc[:, 'page_weight'] = train['pagetype'].map(page_weights)\n",
    "    \n",
    "    # 2. Calcular popularidad ponderada\n",
    "    popularity_df = train.groupby('partnumber').agg({\n",
    "        'add_to_cart': 'sum',\n",
    "        'page_weight': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # 3. Normalizar los valores\n",
    "    popularity_df['add_to_cart_norm'] = (popularity_df['add_to_cart'] - popularity_df['add_to_cart'].min()) / \\\n",
    "                                      (popularity_df['add_to_cart'].max() - popularity_df['add_to_cart'].min())\n",
    "    popularity_df['page_weight_norm'] = (popularity_df['page_weight'] - popularity_df['page_weight'].min()) / \\\n",
    "                                      (popularity_df['page_weight'].max() - popularity_df['page_weight'].min())\n",
    "    \n",
    "    # 4. Calcular score final\n",
    "    popularity_df['popularity_score'] = (\n",
    "        0.7 * popularity_df['add_to_cart_norm'] +\n",
    "        0.3 * popularity_df['page_weight_norm']\n",
    "    )\n",
    "    \n",
    "    return popularity_df.sort_values('popularity_score', ascending=False)['partnumber'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_specific_recommendations(train_data, country):\n",
    "    # Crear una copia explícita para el filtrado por país\n",
    "    country_data = train_data[train_data['country'] == country].copy()\n",
    "    \n",
    "    # Si no hay suficientes datos para el país, usar datos globales\n",
    "    if len(country_data) < 100:  # umbral arbitrario, ajustar según necesidad\n",
    "        return enhanced_popularity_model(train_data.copy())\n",
    "    \n",
    "    # Obtener productos populares específicos del país\n",
    "    country_popular = enhanced_popularity_model(country_data)\n",
    "    \n",
    "    return country_popular\n",
    "\n",
    "def blend_recommendations(general_recs, country_recs, ratio=0.7):\n",
    "    # Asegurarse de que tenemos suficientes recomendaciones\n",
    "    if not general_recs:\n",
    "        return country_recs[:5]\n",
    "    if not country_recs:\n",
    "        return general_recs[:5]\n",
    "    \n",
    "    num_general = int(5 * ratio)\n",
    "    num_country = 5 - num_general\n",
    "    \n",
    "    final_recs = general_recs[:num_general]\n",
    "    country_specific = [x for x in country_recs if x not in final_recs]\n",
    "    final_recs.extend(country_specific[:num_country])\n",
    "    \n",
    "    # Asegurarse de que tenemos exactamente 5 recomendaciones\n",
    "    while len(final_recs) < 5:\n",
    "        if len(general_recs) > len(final_recs):\n",
    "            next_rec = next(rec for rec in general_recs if rec not in final_recs)\n",
    "            final_recs.append(next_rec)\n",
    "    \n",
    "    return final_recs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Basado en Contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_content_model(products):\n",
    "    # Asegurarnos de que los embeddings son arrays de NumPy\n",
    "    products['embedding'] = products['embedding'].apply(np.array)\n",
    "    # Crear un diccionario {partnumber: embedding}\n",
    "    embeddings_dict = dict(zip(products['partnumber'], products['embedding']))\n",
    "    return embeddings_dict\n",
    "\n",
    "def find_similar_products(partnumber, embeddings_dict, top_n=5):\n",
    "    target_embedding = embeddings_dict.get(partnumber)\n",
    "    if not isinstance(target_embedding, np.ndarray):\n",
    "        print(f\"Advertencia: El embedding del producto {partnumber} es inválido.\")\n",
    "        return []\n",
    "\n",
    "    # Obtener todas las embeddings y los partnumbers correspondientes\n",
    "    all_partnumbers = []\n",
    "    all_embeddings = []\n",
    "    for pnum, emb in embeddings_dict.items():\n",
    "        if isinstance(emb, np.ndarray) and emb.shape == target_embedding.shape:\n",
    "            all_partnumbers.append(pnum)\n",
    "            all_embeddings.append(emb)\n",
    "\n",
    "    # Convertir a arrays de NumPy\n",
    "    all_embeddings = np.stack(all_embeddings)\n",
    "\n",
    "    # Calcular la similitud de coseno\n",
    "    similarities = cosine_similarity([target_embedding], all_embeddings)[0]\n",
    "\n",
    "    # Obtener los índices de los productos más similares (excluyendo el propio producto)\n",
    "    similar_indices = similarities.argsort()[::-1]\n",
    "    similar_partnumbers = []\n",
    "    for idx in similar_indices:\n",
    "        if all_partnumbers[idx] != partnumber:\n",
    "            similar_partnumbers.append(all_partnumbers[idx])\n",
    "        if len(similar_partnumbers) == top_n:\n",
    "            break\n",
    "\n",
    "    return similar_partnumbers\n",
    "\n",
    "def recommend_by_content(partnumbers_interacted, embeddings_dict, top_n=5):\n",
    "    recommendations = []\n",
    "    for partnumber in partnumbers_interacted:\n",
    "        # Obtener productos similares\n",
    "        similar_products = find_similar_products(partnumber, embeddings_dict, top_n=top_n)\n",
    "        # Añadir los productos similares a la lista de recomendaciones\n",
    "        recommendations.extend(similar_products)\n",
    "\n",
    "    # Eliminar productos ya vistos y duplicados\n",
    "    recommendations = [p for p in recommendations if p not in partnumbers_interacted]\n",
    "    recommendations = list(dict.fromkeys(recommendations))\n",
    "\n",
    "    return recommendations[:top_n]\n",
    "\n",
    "# Preparar el modelo basado en contenido\n",
    "embeddings_dict = prepare_content_model(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Colaborativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importaciones\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy.sparse import csr_matrix\n",
    "import gc\n",
    "import psutil\n",
    "import cupy as cp\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Función de preparación de datos\n",
    "def prepare_collaborative_model(train_logged_in, products, sample_fraction=0.05, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Preparar datos para LambdaRank con ranking explícito\n",
    "    \"\"\"\n",
    "    print(f\"Tamaño original del dataset: {len(train_logged_in)}\")\n",
    "    \n",
    "    # Muestrear usuarios\n",
    "    unique_users = train_logged_in['user_id'].unique()\n",
    "    num_users_to_sample = int(len(unique_users) * sample_fraction)\n",
    "    sampled_users = cp.random.choice(unique_users, size=num_users_to_sample, replace=False)\n",
    "    \n",
    "    # Filtrar dataset\n",
    "    train_sample = train_logged_in[train_logged_in['user_id'].isin(cp.asnumpy(sampled_users))].copy()\n",
    "    print(f\"Tamaño del dataset muestreado: {len(train_sample)}\")\n",
    "    \n",
    "    # Preparar datos\n",
    "    le_user = LabelEncoder()\n",
    "    le_product = LabelEncoder()\n",
    "    \n",
    "    train_sample['user_encoded'] = le_user.fit_transform(train_sample['user_id'])\n",
    "    train_sample['product_encoded'] = le_product.fit_transform(train_sample['partnumber'])\n",
    "    \n",
    "    # Crear ranking por usuario\n",
    "    train_sample['relevance'] = train_sample['add_to_cart']\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    group_sizes = []\n",
    "    current_group_size = 0\n",
    "    \n",
    "    # Procesar por usuario para mantener estructura de ranking\n",
    "    for user_id in train_sample['user_id'].unique():\n",
    "        user_data = train_sample[train_sample['user_id'] == user_id]\n",
    "        \n",
    "        # Asegurar que tenemos suficientes interacciones\n",
    "        if len(user_data) < 2:  # Necesitamos al menos 2 items para ranking\n",
    "            continue\n",
    "            \n",
    "        # Crear features para cada interacción del usuario\n",
    "        user_features = []\n",
    "        user_labels = []\n",
    "        \n",
    "        for _, row in user_data.iterrows():\n",
    "            product_data = products.loc[row['partnumber']]\n",
    "            feature_vector = [\n",
    "                row['user_encoded'],\n",
    "                row['product_encoded'],\n",
    "                row['pagetype'],\n",
    "                product_data['discount'],\n",
    "                product_data['color_id'],\n",
    "                product_data['cod_section'],\n",
    "                product_data['family'],\n",
    "                *product_data['embedding']\n",
    "            ]\n",
    "            user_features.append(feature_vector)\n",
    "            user_labels.append(row['relevance'])\n",
    "        \n",
    "        if user_features:\n",
    "            features.extend(user_features)\n",
    "            labels.extend(user_labels)\n",
    "            group_sizes.append(len(user_features))\n",
    "            current_group_size += 1\n",
    "            \n",
    "        if current_group_size % 100 == 0:\n",
    "            print(f\"Procesados {current_group_size} usuarios\")\n",
    "    \n",
    "    # Convertir a arrays numpy\n",
    "    features_array = np.array(features, dtype=np.float32)\n",
    "    labels_array = np.array(labels, dtype=np.int32)\n",
    "    \n",
    "    print(f\"Features shape: {features_array.shape}\")\n",
    "    print(f\"Labels shape: {labels_array.shape}\")\n",
    "    print(f\"Número de grupos: {len(group_sizes)}\")\n",
    "    print(f\"Distribución de tamaños de grupo: min={min(group_sizes)}, max={max(group_sizes)}, avg={np.mean(group_sizes):.2f}\")\n",
    "    \n",
    "    return features_array, labels_array, group_sizes, le_user, le_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(X, y, group_sizes, validation_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Crear conjunto de validación manteniendo grupos intactos\n",
    "    \"\"\"\n",
    "    n_groups = len(group_sizes)\n",
    "    n_val_groups = int(n_groups * validation_fraction)\n",
    "    \n",
    "    # Índices aleatorios para validación\n",
    "    val_group_indices = np.random.choice(n_groups, n_val_groups, replace=False)\n",
    "    \n",
    "    # Crear máscaras para train y validación\n",
    "    start_idx = 0\n",
    "    train_mask = np.ones(len(X), dtype=bool)\n",
    "    \n",
    "    for i, size in enumerate(group_sizes):\n",
    "        if i in val_group_indices:\n",
    "            train_mask[start_idx:start_idx + size] = False\n",
    "        start_idx += size\n",
    "    \n",
    "    val_mask = ~train_mask\n",
    "    \n",
    "    return (X[train_mask], y[train_mask], [s for i, s in enumerate(group_sizes) if i not in val_group_indices],\n",
    "            X[val_mask], y[val_mask], [s for i, s in enumerate(group_sizes) if i in val_group_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento del modelo LambdaRank...\n",
      "Tamaño original del dataset: 5488678\n",
      "Tamaño del dataset muestreado: 275401\n",
      "Procesados 100 usuarios\n",
      "Procesados 200 usuarios\n",
      "Procesados 300 usuarios\n",
      "Procesados 400 usuarios\n",
      "Procesados 500 usuarios\n",
      "Procesados 600 usuarios\n",
      "Procesados 700 usuarios\n",
      "Procesados 800 usuarios\n",
      "Procesados 900 usuarios\n",
      "Procesados 1000 usuarios\n",
      "Procesados 1100 usuarios\n",
      "Procesados 1200 usuarios\n",
      "Procesados 1300 usuarios\n",
      "Procesados 1400 usuarios\n",
      "Procesados 1500 usuarios\n",
      "Procesados 1600 usuarios\n",
      "Procesados 1700 usuarios\n",
      "Procesados 1800 usuarios\n",
      "Procesados 1900 usuarios\n",
      "Procesados 2000 usuarios\n",
      "Procesados 2100 usuarios\n",
      "Procesados 2200 usuarios\n",
      "Procesados 2300 usuarios\n",
      "Procesados 2400 usuarios\n",
      "Procesados 2500 usuarios\n",
      "Procesados 2600 usuarios\n",
      "Procesados 2700 usuarios\n",
      "Procesados 2800 usuarios\n",
      "Procesados 2900 usuarios\n",
      "Procesados 3000 usuarios\n",
      "Procesados 3100 usuarios\n",
      "Procesados 3200 usuarios\n",
      "Procesados 3300 usuarios\n",
      "Procesados 3400 usuarios\n",
      "Procesados 3500 usuarios\n",
      "Procesados 3600 usuarios\n",
      "Procesados 3700 usuarios\n",
      "Procesados 3800 usuarios\n",
      "Procesados 3900 usuarios\n",
      "Procesados 4000 usuarios\n",
      "Procesados 4100 usuarios\n",
      "Procesados 4200 usuarios\n",
      "Procesados 4300 usuarios\n",
      "Procesados 4400 usuarios\n",
      "Procesados 4500 usuarios\n",
      "Procesados 4600 usuarios\n",
      "Procesados 4700 usuarios\n",
      "Procesados 4800 usuarios\n",
      "Procesados 4900 usuarios\n",
      "Procesados 5000 usuarios\n",
      "Procesados 5100 usuarios\n",
      "Procesados 5200 usuarios\n",
      "Procesados 5300 usuarios\n",
      "Procesados 5400 usuarios\n",
      "Procesados 5500 usuarios\n",
      "Procesados 5600 usuarios\n",
      "Procesados 5700 usuarios\n",
      "Procesados 5800 usuarios\n",
      "Procesados 5900 usuarios\n",
      "Procesados 6000 usuarios\n",
      "Procesados 6100 usuarios\n",
      "Procesados 6200 usuarios\n",
      "Procesados 6300 usuarios\n",
      "Procesados 6400 usuarios\n",
      "Procesados 6500 usuarios\n",
      "Procesados 6600 usuarios\n",
      "Procesados 6700 usuarios\n",
      "Procesados 6800 usuarios\n",
      "Procesados 6900 usuarios\n",
      "Procesados 7000 usuarios\n",
      "Procesados 7100 usuarios\n",
      "Procesados 7200 usuarios\n",
      "Procesados 7300 usuarios\n",
      "Procesados 7400 usuarios\n",
      "Procesados 7500 usuarios\n",
      "Procesados 7600 usuarios\n",
      "Procesados 7700 usuarios\n",
      "Procesados 7800 usuarios\n",
      "Procesados 7900 usuarios\n",
      "Procesados 8000 usuarios\n",
      "Procesados 8100 usuarios\n",
      "Procesados 8200 usuarios\n",
      "Procesados 8300 usuarios\n",
      "Procesados 8400 usuarios\n",
      "Procesados 8500 usuarios\n",
      "Procesados 8600 usuarios\n",
      "Procesados 8700 usuarios\n",
      "Procesados 8800 usuarios\n",
      "Procesados 8900 usuarios\n",
      "Procesados 9000 usuarios\n",
      "Procesados 9100 usuarios\n",
      "Procesados 9200 usuarios\n",
      "Procesados 9300 usuarios\n",
      "Procesados 9400 usuarios\n",
      "Procesados 9500 usuarios\n",
      "Procesados 9600 usuarios\n",
      "Procesados 9700 usuarios\n",
      "Procesados 9800 usuarios\n",
      "Procesados 9900 usuarios\n",
      "Procesados 10000 usuarios\n",
      "Procesados 10100 usuarios\n",
      "Procesados 10200 usuarios\n",
      "Procesados 10300 usuarios\n",
      "Procesados 10400 usuarios\n",
      "Procesados 10500 usuarios\n",
      "Procesados 10600 usuarios\n",
      "Procesados 10700 usuarios\n",
      "Procesados 10800 usuarios\n",
      "Procesados 10900 usuarios\n",
      "Procesados 11000 usuarios\n",
      "Procesados 11100 usuarios\n",
      "Procesados 11200 usuarios\n",
      "Procesados 11300 usuarios\n",
      "Procesados 11400 usuarios\n",
      "Procesados 11500 usuarios\n",
      "Procesados 11600 usuarios\n",
      "Procesados 11700 usuarios\n",
      "Procesados 11800 usuarios\n",
      "Procesados 11900 usuarios\n",
      "Procesados 12000 usuarios\n",
      "Procesados 12100 usuarios\n",
      "Procesados 12200 usuarios\n",
      "Procesados 12300 usuarios\n",
      "Procesados 12400 usuarios\n",
      "Procesados 12500 usuarios\n",
      "Procesados 12600 usuarios\n",
      "Procesados 12700 usuarios\n",
      "Procesados 12800 usuarios\n",
      "Procesados 12900 usuarios\n",
      "Procesados 13000 usuarios\n",
      "Procesados 13100 usuarios\n",
      "Procesados 13200 usuarios\n",
      "Procesados 13300 usuarios\n",
      "Procesados 13400 usuarios\n",
      "Procesados 13500 usuarios\n",
      "Procesados 13600 usuarios\n",
      "Procesados 13700 usuarios\n",
      "Procesados 13800 usuarios\n",
      "Procesados 13900 usuarios\n",
      "Procesados 14000 usuarios\n",
      "Procesados 14100 usuarios\n",
      "Procesados 14200 usuarios\n",
      "Procesados 14300 usuarios\n",
      "Procesados 14400 usuarios\n",
      "Procesados 14500 usuarios\n",
      "Procesados 14600 usuarios\n",
      "Procesados 14700 usuarios\n",
      "Procesados 14800 usuarios\n",
      "Procesados 14900 usuarios\n",
      "Procesados 15000 usuarios\n",
      "Procesados 15100 usuarios\n",
      "Procesados 15200 usuarios\n",
      "Procesados 15300 usuarios\n",
      "Procesados 15400 usuarios\n",
      "Procesados 15500 usuarios\n",
      "Procesados 15600 usuarios\n",
      "Procesados 15700 usuarios\n",
      "Procesados 15800 usuarios\n",
      "Procesados 15900 usuarios\n",
      "Features shape: (272364, 1287)\n",
      "Labels shape: (272364,)\n",
      "Número de grupos: 15965\n",
      "Distribución de tamaños de grupo: min=2, max=942, avg=17.06\n",
      "Aplicando feature engineering...\n",
      "Escalando features...\n",
      "Creando conjuntos de train y validación...\n",
      "Creando datasets de LightGBM...\n",
      "Entrenando modelo LambdaRank en CPU...\n",
      "[LightGBM] [Info] Total groups: 13571, total data: 229422\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.575512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327895\n",
      "[LightGBM] [Info] Number of data points in the train set: 229422, number of used features: 1290\n",
      "[LightGBM] [Info] Total groups: 2394, total data: 42942\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\ttraining's ndcg@5: 0.790383\ttraining's ndcg@10: 0.81625\ttraining's ndcg@15: 0.829666\ttraining's map@5: 0.752076\ttraining's map@10: 0.765321\ttraining's map@15: 0.77227\ttraining's auc: 0.617652\tvalid_1's ndcg@5: 0.755461\tvalid_1's ndcg@10: 0.782337\tvalid_1's ndcg@15: 0.797608\tvalid_1's map@5: 0.719942\tvalid_1's map@10: 0.732242\tvalid_1's map@15: 0.7396\tvalid_1's auc: 0.562481\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\ttraining's ndcg@5: 0.800027\ttraining's ndcg@10: 0.82499\ttraining's ndcg@15: 0.837814\ttraining's map@5: 0.762104\ttraining's map@10: 0.774895\ttraining's map@15: 0.781691\ttraining's auc: 0.632804\tvalid_1's ndcg@5: 0.754953\tvalid_1's ndcg@10: 0.784538\tvalid_1's ndcg@15: 0.799095\tvalid_1's map@5: 0.719822\tvalid_1's map@10: 0.733939\tvalid_1's map@15: 0.741013\tvalid_1's auc: 0.570161\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\ttraining's ndcg@5: 0.80594\ttraining's ndcg@10: 0.830471\ttraining's ndcg@15: 0.84271\ttraining's map@5: 0.768365\ttraining's map@10: 0.781169\ttraining's map@15: 0.787758\ttraining's auc: 0.641437\tvalid_1's ndcg@5: 0.755542\tvalid_1's ndcg@10: 0.784274\tvalid_1's ndcg@15: 0.798261\tvalid_1's map@5: 0.719809\tvalid_1's map@10: 0.733378\tvalid_1's map@15: 0.739977\tvalid_1's auc: 0.5692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\ttraining's ndcg@5: 0.809562\ttraining's ndcg@10: 0.833827\ttraining's ndcg@15: 0.845708\ttraining's map@5: 0.771697\ttraining's map@10: 0.784397\ttraining's map@15: 0.790864\ttraining's auc: 0.649272\tvalid_1's ndcg@5: 0.753303\tvalid_1's ndcg@10: 0.783629\tvalid_1's ndcg@15: 0.797425\tvalid_1's map@5: 0.717961\tvalid_1's map@10: 0.732402\tvalid_1's map@15: 0.738889\tvalid_1's auc: 0.5696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\ttraining's ndcg@5: 0.811349\ttraining's ndcg@10: 0.835572\ttraining's ndcg@15: 0.847939\ttraining's map@5: 0.773813\ttraining's map@10: 0.786324\ttraining's map@15: 0.793097\ttraining's auc: 0.652414\tvalid_1's ndcg@5: 0.752073\tvalid_1's ndcg@10: 0.781678\tvalid_1's ndcg@15: 0.796054\tvalid_1's map@5: 0.715969\tvalid_1's map@10: 0.730047\tvalid_1's map@15: 0.737067\tvalid_1's auc: 0.571886\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's ndcg@5: 0.802362\ttraining's ndcg@10: 0.827357\ttraining's ndcg@15: 0.839206\ttraining's map@5: 0.763886\ttraining's map@10: 0.777293\ttraining's map@15: 0.783649\ttraining's auc: 0.573483\tvalid_1's ndcg@5: 0.788896\tvalid_1's ndcg@10: 0.813456\tvalid_1's ndcg@15: 0.824557\tvalid_1's map@5: 0.751258\tvalid_1's map@10: 0.763476\tvalid_1's map@15: 0.76924\tvalid_1's auc: 0.556864\n",
      "\n",
      "Métricas finales:\n",
      "\n",
      "Training:\n",
      "  ndcg@5: 0.8024\n",
      "  ndcg@10: 0.8274\n",
      "  map: No disponible\n",
      "\n",
      "Validation:\n",
      "  ndcg@5: No disponible\n",
      "  ndcg@10: No disponible\n",
      "  map: No disponible\n",
      "\n",
      "Top 10 features más importantes:\n",
      "       Feature   Importance\n",
      "2     pagetype  1277.687012\n",
      "877    emb_870    43.156200\n",
      "575    emb_568    32.561001\n",
      "1210  emb_1203    29.339899\n",
      "1161  emb_1154    26.404900\n",
      "205    emb_198    25.649799\n",
      "356    emb_349    25.050699\n",
      "930    emb_923    22.922600\n",
      "631    emb_624    22.606501\n",
      "973    emb_966    22.027399\n",
      "\n",
      "Información del modelo:\n",
      "Mejor iteración: 1\n",
      "\n",
      "Parámetros utilizados:\n",
      "objective: lambdarank\n",
      "metric: ['ndcg', 'map', 'auc']\n",
      "ndcg_eval_at: [5, 10, 15]\n",
      "num_leaves: 256\n",
      "max_depth: 6\n",
      "min_data_in_leaf: 10\n",
      "min_gain_to_split: 0.0001\n",
      "learning_rate: 0.01\n",
      "feature_fraction: 0.9\n",
      "bagging_fraction: 0.9\n",
      "bagging_freq: 1\n",
      "lambda_l1: 0.005\n",
      "lambda_l2: 0.005\n",
      "scale_pos_weight: 3.0\n",
      "pos_bagging_fraction: 1.0\n",
      "neg_bagging_fraction: 0.5\n",
      "max_bin: 255\n",
      "min_sum_hessian_in_leaf: 1e-05\n",
      "boost_from_average: True\n",
      "first_metric_only: False\n",
      "feature_fraction_bynode: 0.8\n",
      "is_unbalance: True\n",
      "early_stopping_round: 50\n"
     ]
    }
   ],
   "source": [
    "def train_lambdarank_model(train_logged_in, products, sample_fraction=0.05, batch_size=1000, validation_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Entrenar modelo LambdaRank con ajustes para evitar el sobreajuste perfecto\n",
    "    \"\"\"\n",
    "    # Preparar datos\n",
    "    X, y, group_sizes, le_user, le_product = prepare_collaborative_model(\n",
    "        train_logged_in, \n",
    "        products,\n",
    "        sample_fraction=sample_fraction,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Normalizar features numéricas\n",
    "    feature_names = ['user_id', 'product_id', 'pagetype', 'discount', 'color_id', \n",
    "                    'section', 'family'] + [f'emb_{i}' for i in range(X.shape[1]-7)]\n",
    "    \n",
    "    # Función para feature engineering\n",
    "    def add_interaction_features(X, feature_names):\n",
    "        X_df = pd.DataFrame(X, columns=feature_names)\n",
    "        \n",
    "        # Más interacciones\n",
    "        X_df['pagetype_x_user'] = X_df['pagetype'] * X_df['user_id']\n",
    "        X_df['pagetype_x_discount'] = X_df['pagetype'] * X_df['discount']\n",
    "        X_df['user_x_section'] = X_df['user_id'] * X_df['section']\n",
    "        \n",
    "        # Agregaciones por usuario\n",
    "        user_stats = X_df.groupby('user_id').agg({\n",
    "            'pagetype': ['mean', 'std'],\n",
    "            'discount': 'mean'\n",
    "        }).fillna(0)\n",
    "        \n",
    "        feature_names.extend(['pagetype_x_user', 'pagetype_x_discount', 'user_x_section'])\n",
    "        \n",
    "        return X_df.values, feature_names\n",
    "    \n",
    "    # Aplicar feature engineering\n",
    "    print(\"Aplicando feature engineering...\")\n",
    "    X, feature_names = add_interaction_features(X, feature_names)\n",
    "    \n",
    "    # Escalar features\n",
    "    print(\"Escalando features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Crear conjuntos de entrenamiento y validación\n",
    "    print(\"Creando conjuntos de train y validación...\")\n",
    "    X_train, y_train, group_sizes_train, X_val, y_val, group_sizes_val = create_validation_set(\n",
    "        X, y, group_sizes, validation_fraction=validation_fraction\n",
    "    )\n",
    "    \n",
    "    print(\"Creando datasets de LightGBM...\")\n",
    "    train_data = lgb.Dataset(\n",
    "        X_train, \n",
    "        label=y_train,\n",
    "        group=group_sizes_train,\n",
    "        feature_name=feature_names\n",
    "    )\n",
    "    \n",
    "    valid_data = lgb.Dataset(\n",
    "        X_val,\n",
    "        label=y_val,\n",
    "        group=group_sizes_val,\n",
    "        reference=train_data\n",
    "    )\n",
    "    \n",
    "    # Parámetros significativamente ajustados\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': ['ndcg', 'map', 'auc'],\n",
    "        'ndcg_eval_at': [5, 10, 15],\n",
    "        \n",
    "        # Ajustes para permitir más splits\n",
    "        'num_leaves': 256,              # Reducir\n",
    "        'max_depth': 6,                 # Reducir\n",
    "        'min_data_in_leaf': 10,         # Aumentar\n",
    "        'min_gain_to_split': 0.0001,    # Reducir\n",
    "        \n",
    "        # Learning rate más agresivo\n",
    "        'learning_rate': 0.01,          # Aumentar\n",
    "        \n",
    "        # Muestreo más agresivo\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 1,\n",
    "        \n",
    "        # Regularización más suave\n",
    "        'lambda_l1': 0.005,\n",
    "        'lambda_l2': 0.005,\n",
    "        \n",
    "        # Balance más agresivo\n",
    "        'scale_pos_weight': 3.0,\n",
    "        'pos_bagging_fraction': 1.0,\n",
    "        'neg_bagging_fraction': 0.5,\n",
    "        \n",
    "        # Otros ajustes\n",
    "        'max_bin': 255,\n",
    "        'min_sum_hessian_in_leaf': 0.00001,\n",
    "        'boost_from_average': True,\n",
    "        'first_metric_only': False,     # Permitir múltiples métricas\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        \n",
    "        # Nuevos parámetros\n",
    "        'boost_from_average': True,\n",
    "        'is_unbalance': True,\n",
    "        'early_stopping_round': 50      # Más paciencia\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(\"Entrenando modelo LambdaRank en CPU...\")\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=300,\n",
    "        valid_sets=[train_data, valid_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=10),\n",
    "            lgb.reset_parameter(learning_rate=lambda iter: \n",
    "                                    0.01 * (0.995 ** iter)),\n",
    "            # Nuevo callback para feature importance\n",
    "            lgb.record_evaluation({\n",
    "                'feature_importance': []\n",
    "            }),\n",
    "            # Añadir callback personalizado para métricas adicionales\n",
    "            lgb.callback.record_evaluation({\n",
    "                'ndcg@5': [],\n",
    "                'ndcg@10': [],\n",
    "                'ndcg@15': [],\n",
    "                'map@5': [],\n",
    "                'map@10': [],\n",
    "                'map@15': [],\n",
    "                'auc': [],\n",
    "                'average_precision': [],\n",
    "                'precision@5': [],\n",
    "                'recall@5': []\n",
    "                })\n",
    "            ]\n",
    "    )       \n",
    "        \n",
    "    # Análisis detallado\n",
    "    print(\"\\nMétricas finales:\")\n",
    "    metrics = ['ndcg@5', 'ndcg@10', 'map']\n",
    "    datasets = ['training', 'validation']\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"\\n{dataset.capitalize()}:\")\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                value = model.best_score[dataset][metric]\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "            except:\n",
    "                print(f\"  {metric}: No disponible\")\n",
    "    \n",
    "    # Análisis de features\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 features más importantes:\")\n",
    "    print(feature_imp.head(10))\n",
    "    \n",
    "    # Guardar información del modelo\n",
    "    model_info = {\n",
    "        'params': params,\n",
    "        'best_iteration': model.best_iteration,\n",
    "        'feature_importance': feature_imp.to_dict(),\n",
    "        'best_score': model.best_score\n",
    "    }\n",
    "    \n",
    "    return model, le_user, le_product, model_info\n",
    "\n",
    "# Uso\n",
    "print(\"Iniciando entrenamiento del modelo LambdaRank...\")\n",
    "train_logged_in = train[train['user_id'] != -1].copy()\n",
    "\n",
    "# Entrenar modelo con validación\n",
    "lambdarank_model, le_user, le_product, model_info = train_lambdarank_model(\n",
    "    train_logged_in, \n",
    "    products, \n",
    "    sample_fraction=0.05,\n",
    "    batch_size=2000,\n",
    "    validation_fraction=0.15\n",
    ")\n",
    "\n",
    "# Imprimir información detallada del modelo\n",
    "print(\"\\nInformación del modelo:\")\n",
    "print(f\"Mejor iteración: {model_info['best_iteration']}\")\n",
    "print(\"\\nParámetros utilizados:\")\n",
    "for param, value in model_info['params'].items():\n",
    "    print(f\"{param}: {value}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train_lambdarank_model(train_logged_in, products, sample_fraction=0.05, batch_size=1000, validation_fraction=0.2):\\n    \"\"\"\\n    Entrenar modelo LambdaRank con ajustes para evitar el sobreajuste perfecto\\n    \"\"\"\\n    # Preparar datos\\n    X, y, group_sizes, le_user, le_product = prepare_collaborative_model(\\n        train_logged_in, \\n        products,\\n        sample_fraction=sample_fraction,\\n        batch_size=batch_size\\n    )\\n    \\n    # Normalizar features numéricas\\n    feature_names = [\\'user_id\\', \\'product_id\\', \\'pagetype\\', \\'discount\\', \\'color_id\\', \\n                    \\'section\\', \\'family\\'] + [f\\'emb_{i}\\' for i in range(X.shape[1]-7)]\\n    \\n    # Función para feature engineering\\n    def add_interaction_features(X, feature_names):\\n        X_df = pd.DataFrame(X, columns=feature_names)\\n        \\n        # Más interacciones\\n        X_df[\\'pagetype_x_user\\'] = X_df[\\'pagetype\\'] * X_df[\\'user_id\\']\\n        X_df[\\'pagetype_x_discount\\'] = X_df[\\'pagetype\\'] * X_df[\\'discount\\']\\n        X_df[\\'user_x_section\\'] = X_df[\\'user_id\\'] * X_df[\\'section\\']\\n        \\n        # Agregaciones por usuario\\n        user_stats = X_df.groupby(\\'user_id\\').agg({\\n            \\'pagetype\\': [\\'mean\\', \\'std\\'],\\n            \\'discount\\': \\'mean\\'\\n        }).fillna(0)\\n        \\n        feature_names.extend([\\'pagetype_x_user\\', \\'pagetype_x_discount\\', \\'user_x_section\\'])\\n        \\n        return X_df.values, feature_names\\n    \\n    # Aplicar feature engineering\\n    print(\"Aplicando feature engineering...\")\\n    X, feature_names = add_interaction_features(X, feature_names)\\n    \\n    # Escalar features\\n    print(\"Escalando features...\")\\n    scaler = StandardScaler()\\n    X = scaler.fit_transform(X)\\n    \\n    # Crear conjuntos de entrenamiento y validación\\n    print(\"Creando conjuntos de train y validación...\")\\n    X_train, y_train, group_sizes_train, X_val, y_val, group_sizes_val = create_validation_set(\\n        X, y, group_sizes, validation_fraction=validation_fraction\\n    )\\n    \\n    print(\"Creando datasets de LightGBM...\")\\n    train_data = lgb.Dataset(\\n        X_train, \\n        label=y_train,\\n        group=group_sizes_train,\\n        feature_name=feature_names\\n    )\\n    \\n    valid_data = lgb.Dataset(\\n        X_val,\\n        label=y_val,\\n        group=group_sizes_val,\\n        reference=train_data\\n    )\\n    \\n    # Parámetros significativamente ajustados\\n    \\n    params = {\\n        \\'objective\\': \\'lambdarank\\',\\n        \\'metric\\': [\\'ndcg\\', \\'map\\', \\'auc\\'],\\n        \\'ndcg_eval_at\\': [5, 10, 15],\\n        \\n        # Ajustes para permitir más splits\\n        \\'num_leaves\\': 256,              # Reducir\\n        \\'max_depth\\': 6,                 # Reducir\\n        \\'min_data_in_leaf\\': 10,         # Aumentar\\n        \\'min_gain_to_split\\': 0.0001,    # Reducir\\n        \\n        # Learning rate más agresivo\\n        \\'learning_rate\\': 0.01,          # Aumentar\\n        \\n        # Muestreo más agresivo\\n        \\'feature_fraction\\': 0.9,\\n        \\'bagging_fraction\\': 0.9,\\n        \\'bagging_freq\\': 1,\\n        \\n        # Regularización más suave\\n        \\'lambda_l1\\': 0.005,\\n        \\'lambda_l2\\': 0.005,\\n        \\n        # Balance más agresivo\\n        \\'scale_pos_weight\\': 3.0,\\n        \\'pos_bagging_fraction\\': 1.0,\\n        \\'neg_bagging_fraction\\': 0.5,\\n        \\n        # Otros ajustes\\n        \\'max_bin\\': 255,\\n        \\'min_sum_hessian_in_leaf\\': 0.00001,\\n        \\'boost_from_average\\': True,\\n        \\'first_metric_only\\': False,     # Permitir múltiples métricas\\n        \\'feature_fraction_bynode\\': 0.8,\\n        \\n        # Nuevos parámetros\\n        \\'boost_from_average\\': True,\\n        \\'is_unbalance\\': True,\\n        \\'early_stopping_round\\': 50      # Más paciencia\\n    }\\n\\n    \\n    print(\"Entrenando modelo LambdaRank en CPU...\")\\n    model = lgb.train(\\n        params,\\n        train_data,\\n        num_boost_round=300,\\n        valid_sets=[train_data, valid_data],\\n        callbacks=[\\n            lgb.early_stopping(stopping_rounds=50),\\n            lgb.log_evaluation(period=10),\\n            lgb.reset_parameter(learning_rate=lambda iter: \\n                                    0.01 * (0.995 ** iter)),\\n            # Nuevo callback para feature importance\\n            lgb.record_evaluation({\\n                \\'feature_importance\\': []\\n            }),\\n            # Añadir callback personalizado para métricas adicionales\\n            lgb.callback.record_evaluation({\\n                \\'ndcg@5\\': [],\\n                \\'ndcg@10\\': [],\\n                \\'ndcg@15\\': [],\\n                \\'map@5\\': [],\\n                \\'map@10\\': [],\\n                \\'map@15\\': [],\\n                \\'auc\\': [],\\n                \\'average_precision\\': [],\\n                \\'precision@5\\': [],\\n                \\'recall@5\\': []\\n                })\\n            ]\\n    )       \\n        \\n    # Análisis detallado\\n    print(\"\\nMétricas finales:\")\\n    metrics = [\\'ndcg@5\\', \\'ndcg@10\\', \\'map\\']\\n    datasets = [\\'training\\', \\'validation\\']\\n    \\n    for dataset in datasets:\\n        print(f\"\\n{dataset.capitalize()}:\")\\n        for metric in metrics:\\n            try:\\n                value = model.best_score[dataset][metric]\\n                print(f\"  {metric}: {value:.4f}\")\\n            except:\\n                print(f\"  {metric}: No disponible\")\\n    \\n    # Análisis de features\\n    importance = model.feature_importance(importance_type=\\'gain\\')\\n    feature_imp = pd.DataFrame({\\n        \\'Feature\\': feature_names,\\n        \\'Importance\\': importance\\n    }).sort_values(by=\\'Importance\\', ascending=False)\\n    \\n    print(\"\\nTop 10 features más importantes:\")\\n    print(feature_imp.head(10))\\n    \\n    # Guardar información del modelo\\n    model_info = {\\n        \\'params\\': params,\\n        \\'best_iteration\\': model.best_iteration,\\n        \\'feature_importance\\': feature_imp.to_dict(),\\n        \\'best_score\\': model.best_score\\n    }\\n    \\n    return model, le_user, le_product, model_info\\n\\n# Uso\\nprint(\"Iniciando entrenamiento del modelo LambdaRank...\")\\ntrain_logged_in = train[train[\\'user_id\\'] != -1].copy()\\n\\n# Entrenar modelo con validación\\nlambdarank_model, le_user, le_product, model_info = train_lambdarank_model(\\n    train_logged_in, \\n    products, \\n    sample_fraction=0.05,\\n    batch_size=2000,\\n    validation_fraction=0.15\\n)\\n\\n# Imprimir información detallada del modelo\\nprint(\"\\nInformación del modelo:\")\\nprint(f\"Mejor iteración: {model_info[\\'best_iteration\\']}\")\\nprint(\"\\nParámetros utilizados:\")\\nfor param, value in model_info[\\'params\\'].items():\\n    print(f\"{param}: {value}\")\\n    \\n    '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copia de código\n",
    "'''def train_lambdarank_model(train_logged_in, products, sample_fraction=0.05, batch_size=1000, validation_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Entrenar modelo LambdaRank con ajustes para evitar el sobreajuste perfecto\n",
    "    \"\"\"\n",
    "    # Preparar datos\n",
    "    X, y, group_sizes, le_user, le_product = prepare_collaborative_model(\n",
    "        train_logged_in, \n",
    "        products,\n",
    "        sample_fraction=sample_fraction,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Normalizar features numéricas\n",
    "    feature_names = ['user_id', 'product_id', 'pagetype', 'discount', 'color_id', \n",
    "                    'section', 'family'] + [f'emb_{i}' for i in range(X.shape[1]-7)]\n",
    "    \n",
    "    # Función para feature engineering\n",
    "    def add_interaction_features(X, feature_names):\n",
    "        X_df = pd.DataFrame(X, columns=feature_names)\n",
    "        \n",
    "        # Más interacciones\n",
    "        X_df['pagetype_x_user'] = X_df['pagetype'] * X_df['user_id']\n",
    "        X_df['pagetype_x_discount'] = X_df['pagetype'] * X_df['discount']\n",
    "        X_df['user_x_section'] = X_df['user_id'] * X_df['section']\n",
    "        \n",
    "        # Agregaciones por usuario\n",
    "        user_stats = X_df.groupby('user_id').agg({\n",
    "            'pagetype': ['mean', 'std'],\n",
    "            'discount': 'mean'\n",
    "        }).fillna(0)\n",
    "        \n",
    "        feature_names.extend(['pagetype_x_user', 'pagetype_x_discount', 'user_x_section'])\n",
    "        \n",
    "        return X_df.values, feature_names\n",
    "    \n",
    "    # Aplicar feature engineering\n",
    "    print(\"Aplicando feature engineering...\")\n",
    "    X, feature_names = add_interaction_features(X, feature_names)\n",
    "    \n",
    "    # Escalar features\n",
    "    print(\"Escalando features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Crear conjuntos de entrenamiento y validación\n",
    "    print(\"Creando conjuntos de train y validación...\")\n",
    "    X_train, y_train, group_sizes_train, X_val, y_val, group_sizes_val = create_validation_set(\n",
    "        X, y, group_sizes, validation_fraction=validation_fraction\n",
    "    )\n",
    "    \n",
    "    print(\"Creando datasets de LightGBM...\")\n",
    "    train_data = lgb.Dataset(\n",
    "        X_train, \n",
    "        label=y_train,\n",
    "        group=group_sizes_train,\n",
    "        feature_name=feature_names\n",
    "    )\n",
    "    \n",
    "    valid_data = lgb.Dataset(\n",
    "        X_val,\n",
    "        label=y_val,\n",
    "        group=group_sizes_val,\n",
    "        reference=train_data\n",
    "    )\n",
    "    \n",
    "    # Parámetros significativamente ajustados\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': ['ndcg', 'map', 'auc'],\n",
    "        'ndcg_eval_at': [5, 10, 15],\n",
    "        \n",
    "        # Ajustes para permitir más splits\n",
    "        'num_leaves': 256,              # Reducir\n",
    "        'max_depth': 6,                 # Reducir\n",
    "        'min_data_in_leaf': 10,         # Aumentar\n",
    "        'min_gain_to_split': 0.0001,    # Reducir\n",
    "        \n",
    "        # Learning rate más agresivo\n",
    "        'learning_rate': 0.01,          # Aumentar\n",
    "        \n",
    "        # Muestreo más agresivo\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 1,\n",
    "        \n",
    "        # Regularización más suave\n",
    "        'lambda_l1': 0.005,\n",
    "        'lambda_l2': 0.005,\n",
    "        \n",
    "        # Balance más agresivo\n",
    "        'scale_pos_weight': 3.0,\n",
    "        'pos_bagging_fraction': 1.0,\n",
    "        'neg_bagging_fraction': 0.5,\n",
    "        \n",
    "        # Otros ajustes\n",
    "        'max_bin': 255,\n",
    "        'min_sum_hessian_in_leaf': 0.00001,\n",
    "        'boost_from_average': True,\n",
    "        'first_metric_only': False,     # Permitir múltiples métricas\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        \n",
    "        # Nuevos parámetros\n",
    "        'boost_from_average': True,\n",
    "        'is_unbalance': True,\n",
    "        'early_stopping_round': 50      # Más paciencia\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(\"Entrenando modelo LambdaRank en CPU...\")\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=300,\n",
    "        valid_sets=[train_data, valid_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=10),\n",
    "            lgb.reset_parameter(learning_rate=lambda iter: \n",
    "                                    0.01 * (0.995 ** iter)),\n",
    "            # Nuevo callback para feature importance\n",
    "            lgb.record_evaluation({\n",
    "                'feature_importance': []\n",
    "            }),\n",
    "            # Añadir callback personalizado para métricas adicionales\n",
    "            lgb.callback.record_evaluation({\n",
    "                'ndcg@5': [],\n",
    "                'ndcg@10': [],\n",
    "                'ndcg@15': [],\n",
    "                'map@5': [],\n",
    "                'map@10': [],\n",
    "                'map@15': [],\n",
    "                'auc': [],\n",
    "                'average_precision': [],\n",
    "                'precision@5': [],\n",
    "                'recall@5': []\n",
    "                })\n",
    "            ]\n",
    "    )       \n",
    "        \n",
    "    # Análisis detallado\n",
    "    print(\"\\nMétricas finales:\")\n",
    "    metrics = ['ndcg@5', 'ndcg@10', 'map']\n",
    "    datasets = ['training', 'validation']\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"\\n{dataset.capitalize()}:\")\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                value = model.best_score[dataset][metric]\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "            except:\n",
    "                print(f\"  {metric}: No disponible\")\n",
    "    \n",
    "    # Análisis de features\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 features más importantes:\")\n",
    "    print(feature_imp.head(10))\n",
    "    \n",
    "    # Guardar información del modelo\n",
    "    model_info = {\n",
    "        'params': params,\n",
    "        'best_iteration': model.best_iteration,\n",
    "        'feature_importance': feature_imp.to_dict(),\n",
    "        'best_score': model.best_score\n",
    "    }\n",
    "    \n",
    "    return model, le_user, le_product, model_info\n",
    "\n",
    "# Uso\n",
    "print(\"Iniciando entrenamiento del modelo LambdaRank...\")\n",
    "train_logged_in = train[train['user_id'] != -1].copy()\n",
    "\n",
    "# Entrenar modelo con validación\n",
    "lambdarank_model, le_user, le_product, model_info = train_lambdarank_model(\n",
    "    train_logged_in, \n",
    "    products, \n",
    "    sample_fraction=0.05,\n",
    "    batch_size=2000,\n",
    "    validation_fraction=0.15\n",
    ")\n",
    "\n",
    "# Imprimir información detallada del modelo\n",
    "print(\"\\nInformación del modelo:\")\n",
    "print(f\"Mejor iteración: {model_info['best_iteration']}\")\n",
    "print(\"\\nParámetros utilizados:\")\n",
    "for param, value in model_info['params'].items():\n",
    "    print(f\"{param}: {value}\")\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_collaborative(user_id, model, le_user, le_product, products, top_n=10):\n",
    "    try:\n",
    "        if user_id in le_user.classes_:\n",
    "            user_encoded = le_user.transform([user_id])[0]\n",
    "            predictions = []\n",
    "            batch_size = 1000\n",
    "            all_products = products.index.values\n",
    "            \n",
    "            for i in range(0, len(all_products), batch_size):\n",
    "                batch_products = all_products[i:i+batch_size]\n",
    "                batch_features = []\n",
    "                \n",
    "                for prod in batch_products:\n",
    "                    if prod in le_product.classes_:\n",
    "                        prod_encoded = le_product.transform([prod])[0]\n",
    "                        product_data = products.loc[prod]\n",
    "                        \n",
    "                        # Features base\n",
    "                        base_features = [\n",
    "                            user_encoded,\n",
    "                            prod_encoded,\n",
    "                            0,  # pagetype default\n",
    "                            product_data['discount'],\n",
    "                            product_data['color_id'],\n",
    "                            product_data['cod_section'],\n",
    "                            product_data['family']\n",
    "                        ]\n",
    "                        \n",
    "                        # Embeddings\n",
    "                        emb_features = product_data['embedding']\n",
    "                        \n",
    "                        # Features de interacción\n",
    "                        interaction_features = [\n",
    "                            0 * user_encoded,  # pagetype_x_user (con pagetype=0)\n",
    "                            0 * product_data['discount'],  # pagetype_x_discount\n",
    "                            user_encoded * product_data['cod_section'],  # user_x_section\n",
    "                        ]\n",
    "                        \n",
    "                        features = base_features + list(emb_features) + interaction_features\n",
    "                        batch_features.append(features)\n",
    "                \n",
    "                if batch_features:\n",
    "                    scores = model.predict(batch_features)\n",
    "                    for prod, score in zip(batch_products, scores):\n",
    "                        predictions.append((prod, score))\n",
    "            \n",
    "            predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "            return [p[0] for p in predictions[:top_n]]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"LambdaRank failed for user {user_id}: {str(e)}\")\n",
    "    \n",
    "    return recommend_by_popularity(popular_products, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de sesiones en el conjunto de prueba: 7349\n",
      "Total de sesiones con recomendaciones: 7349\n",
      "Todas las sesiones tienen recomendaciones.\n"
     ]
    }
   ],
   "source": [
    "#  6. Función de recomendación por sesión\n",
    "def generate_recommendations_for_session(session_id, user_id, session_data, user_class):\n",
    "    country = session_data['country'].iloc[0]\n",
    "    \n",
    "    if user_class == 'Usuario recurrente logueado':\n",
    "        recs = recommend_by_collaborative(\n",
    "            user_id,\n",
    "            lambdarank_model,\n",
    "            le_user,\n",
    "            le_product,\n",
    "            products,\n",
    "            top_n=10\n",
    "        )\n",
    "        \n",
    "        country_recs = get_country_specific_recommendations(train, country)\n",
    "        recs = blend_recommendations(recs, country_recs, ratio=0.8)  # Aumentar peso de LambdaRank\n",
    "        \n",
    "    elif user_class == 'Usuario recurrente no logueado':\n",
    "        partnumbers_interacted = session_data['partnumber'].unique()\n",
    "        if len(partnumbers_interacted) > 0:\n",
    "            recs = recommend_by_content(partnumbers_interacted, embeddings_dict, top_n=10)\n",
    "            country_recs = get_country_specific_recommendations(train, country)\n",
    "            recs = blend_recommendations(recs, country_recs, ratio=0.7)  # Ajustar ratio\n",
    "        else:\n",
    "            recs = recommend_by_popularity(popular_products, top_n=5)\n",
    "    else:\n",
    "        country_recs = get_country_specific_recommendations(train, country)\n",
    "        general_recs = recommend_by_popularity(popular_products, top_n=10)\n",
    "        recs = blend_recommendations(general_recs, country_recs, ratio=0.6)  # Ajustar ratio\n",
    "    \n",
    "    return recs[:5]\n",
    "\n",
    "# 7. Generar todas las recomendaciones\n",
    "def generate_all_recommendations(test):\n",
    "    user_recommendations = {}\n",
    "    sessions = test[['session_id', 'user_id', 'user_class']].drop_duplicates()\n",
    "\n",
    "    for _, row in sessions.iterrows():\n",
    "        session_id = row['session_id']\n",
    "        user_id = row['user_id']\n",
    "        user_class = row['user_class']\n",
    "        session_data = test[test['session_id'] == session_id]\n",
    "        recs = generate_recommendations_for_session(session_id, user_id, session_data, user_class)\n",
    "        user_recommendations[session_id] = recs\n",
    "    return user_recommendations\n",
    "\n",
    "# 8. Generar recomendaciones\n",
    "user_recommendations = generate_all_recommendations(test)\n",
    "\n",
    "# 9. Verificar completitud\n",
    "total_sessions_in_test = test['session_id'].nunique()\n",
    "total_sessions_with_recommendations = len(user_recommendations)\n",
    "\n",
    "print(f\"Total de sesiones en el conjunto de prueba: {total_sessions_in_test}\")\n",
    "print(f\"Total de sesiones con recomendaciones: {total_sessions_with_recommendations}\")\n",
    "\n",
    "if total_sessions_in_test == total_sessions_with_recommendations:\n",
    "    print(\"Todas las sesiones tienen recomendaciones.\")\n",
    "else:\n",
    "    print(f\"Faltan recomendaciones para {total_sessions_in_test - total_sessions_with_recommendations} sesiones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación del Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado exitosamente en: /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3.json\n",
      "Número de sesiones en el JSON: 7349\n",
      "\n",
      "Primeras 5 sesiones en el JSON:\n",
      "Sesión 746: [38002, 12468, 834, 9039, 17547]\n",
      "Sesión 1306: [693, 42576, 23029, 24271, 20647]\n",
      "Sesión 1364: [15604, 32851, 13891, 24271, 20647]\n",
      "Sesión 1377: [40779, 24271, 2763, 14230, 25032]\n",
      "Sesión 2251: [40779, 24271, 2763, 14230, 25032]\n"
     ]
    }
   ],
   "source": [
    "# 10. Preparar y guardar JSON\n",
    "def prepare_output_for_json(user_recommendations):\n",
    "    user_recommendations_str_keys = {str(session_id): recs for session_id, recs in user_recommendations.items()}\n",
    "    for session_id, recs in user_recommendations_str_keys.items():\n",
    "        user_recommendations_str_keys[session_id] = [int(p) for p in recs]\n",
    "    return user_recommendations_str_keys\n",
    "\n",
    "def save_recommendations_to_json(user_recommendations_str_keys, output_path):\n",
    "    \"\"\"\n",
    "    Guardar las recomendaciones en formato JSON\n",
    "    \"\"\"\n",
    "    # Crear el directorio si no existe\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Preparar el formato final\n",
    "    output = {'target': user_recommendations_str_keys}\n",
    "    \n",
    "    # Guardar el archivo\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(output, f)\n",
    "    print(f\"Archivo guardado exitosamente en: {output_path}\")\n",
    "\n",
    "# Preparar y guardar JSON\n",
    "output_json_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3.json'\n",
    "user_recommendations_str_keys = prepare_output_for_json(user_recommendations)\n",
    "save_recommendations_to_json(user_recommendations_str_keys, output_json_path)\n",
    "\n",
    "# Verificar JSON\n",
    "def verify_json_output(output_path):\n",
    "    with open(output_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Número de sesiones en el JSON: {len(data['target'])}\")\n",
    "    print(\"\\nPrimeras 5 sesiones en el JSON:\")\n",
    "    for session_id in list(data['target'].keys())[:5]:\n",
    "        print(f\"Sesión {session_id}: {data['target'][session_id]}\")\n",
    "\n",
    "verify_json_output(output_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizar resultados y comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_recommendations(user_recommendations, test, train):\n",
    "    print(\"=== Análisis de Recomendaciones ===\")\n",
    "    \n",
    "    # 1. Distribución de tipos de usuario\n",
    "    user_types = test.groupby('user_class').size()\n",
    "    print(\"\\nDistribución de tipos de usuario:\")\n",
    "    print(user_types)\n",
    "    \n",
    "    # 2. Análisis por país\n",
    "    country_dist = test.groupby('country').size()\n",
    "    print(\"\\nDistribución por país:\")\n",
    "    print(country_dist)\n",
    "    \n",
    "    # 3. Verificar diversidad de recomendaciones\n",
    "    all_recs = [item for sublist in user_recommendations.values() for item in sublist]\n",
    "    unique_recs = len(set(all_recs))\n",
    "    print(f\"\\nNúmero total de productos únicos recomendados: {unique_recs}\")\n",
    "    \n",
    "    # 4. Verificar overlap con productos populares\n",
    "    popular_products_set = set(popular_products[:100])\n",
    "    rec_overlap = sum(1 for x in set(all_recs) if x in popular_products_set)\n",
    "    print(f\"Overlap con productos populares: {rec_overlap/unique_recs:.2%}\")\n",
    "    \n",
    "    # 5. Análisis de recomendaciones por tipo de usuario\n",
    "    print(\"\\nAnálisis por tipo de usuario:\")\n",
    "    for user_class in test['user_class'].unique():\n",
    "        class_sessions = test[test['user_class'] == user_class]['session_id'].unique()\n",
    "        class_recs = [user_recommendations[s] for s in class_sessions if s in user_recommendations]\n",
    "        class_unique_recs = len(set([item for sublist in class_recs for item in sublist]))\n",
    "        print(f\"\\n{user_class}:\")\n",
    "        print(f\"Productos únicos recomendados: {class_unique_recs}\")\n",
    "    \n",
    "    # 6. Verificar recomendaciones por país\n",
    "    print(\"\\nEjemplo de recomendaciones por país:\")\n",
    "    for country in test['country'].unique()[:3]:\n",
    "        country_sessions = test[test['country'] == country]['session_id'].unique()[:2]\n",
    "        print(f\"\\nPaís: {country}\")\n",
    "        for session in country_sessions:\n",
    "            print(f\"Session {session}: {user_recommendations[session]}\")\n",
    "\n",
    "def compare_versions(json_path_v3, json_path_v4):\n",
    "    \"\"\"\n",
    "    Compara las versiones del modelo anterior y LambdaRank\n",
    "    \"\"\"\n",
    "    # Cargar ambas versiones\n",
    "    with open(json_path_v3, 'r') as f:\n",
    "        v3 = json.load(f)\n",
    "    with open(json_path_v4, 'r') as f:\n",
    "        v4 = json.load(f)\n",
    "    \n",
    "    # Comparar recomendaciones\n",
    "    different_recs = 0\n",
    "    total_sessions = len(v3['target'])\n",
    "    \n",
    "    # Análisis detallado de cambios\n",
    "    changes_analysis = {\n",
    "        'total_different': 0,\n",
    "        'completely_different': 0,\n",
    "        'partially_different': 0,\n",
    "        'overlap_stats': []\n",
    "    }\n",
    "    \n",
    "    for session_id in v3['target']:\n",
    "        old_recs = set(v3['target'][session_id])\n",
    "        new_recs = set(v4['target'][session_id])\n",
    "        \n",
    "        if old_recs != new_recs:\n",
    "            different_recs += 1\n",
    "            \n",
    "            # Analizar el tipo de cambio\n",
    "            overlap = len(old_recs.intersection(new_recs))\n",
    "            if overlap == 0:\n",
    "                changes_analysis['completely_different'] += 1\n",
    "            else:\n",
    "                changes_analysis['partially_different'] += 1\n",
    "                changes_analysis['overlap_stats'].append(overlap/5)  # 5 es el número total de recomendaciones\n",
    "    \n",
    "    changes_analysis['total_different'] = different_recs\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"Sesiones con recomendaciones diferentes: {different_recs/total_sessions:.2%}\")\n",
    "    print(f\"Cambios completos: {changes_analysis['completely_different']/total_sessions:.2%}\")\n",
    "    print(f\"Cambios parciales: {changes_analysis['partially_different']/total_sessions:.2%}\")\n",
    "    if changes_analysis['overlap_stats']:\n",
    "        print(f\"Promedio de overlap en cambios parciales: {np.mean(changes_analysis['overlap_stats']):.2%}\")\n",
    "    \n",
    "    # Mostrar ejemplos de cambios\n",
    "    print(\"\\nEjemplos de cambios en recomendaciones:\")\n",
    "    for session_id in list(v3['target'].keys())[:5]:\n",
    "        if v3['target'][session_id] != v4['target'][session_id]:\n",
    "            print(f\"\\nSession {session_id}:\")\n",
    "            print(f\"Anterior (v3): {v3['target'][session_id]}\")\n",
    "            print(f\"Nueva (v4): {v4['target'][session_id]}\")\n",
    "\n",
    "def analyze_lambdarank_performance(user_recommendations, test):\n",
    "    print(\"\\n=== Análisis de Rendimiento de LambdaRank ===\")\n",
    "    \n",
    "    # Análisis existente...\n",
    "    \n",
    "    # Añadir análisis por país\n",
    "    print(\"\\nRendimiento por país:\")\n",
    "    for country in test['country'].unique():\n",
    "        country_sessions = test[test['country'] == country]['session_id'].unique()\n",
    "        country_recs = [user_recommendations[s] for s in country_sessions if s in user_recommendations]\n",
    "        print(f\"\\nPaís {country}:\")\n",
    "        print(f\"Sesiones: {len(country_sessions)}\")\n",
    "        print(f\"Productos únicos recomendados: {len(set([item for sublist in country_recs for item in sublist]))}\")\n",
    "    \n",
    "    # Añadir análisis de cobertura de categorías\n",
    "    print(\"\\nCobertura de categorías:\")\n",
    "    all_recs = [item for sublist in user_recommendations.values() for item in sublist]\n",
    "    rec_categories = products.loc[all_recs]['cod_section'].unique()\n",
    "    print(f\"Categorías cubiertas: {len(rec_categories)}\")\n",
    "    \n",
    "    return lambdarank_used, unique_recs, rec_counts\n",
    "\n",
    "# Ejecutar análisis\n",
    "print(\"Ejecutando análisis completo...\")\n",
    "analyze_recommendations(user_recommendations, test, train)\n",
    "compare_versions('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions_save/old/predictions_3_shared_stratify_v3_436.json', '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3.json')\n",
    "lambdarank_used, unique_recs, rec_counts = analyze_lambdarank_performance(user_recommendations, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Análisis Detallado de Recomendaciones ===\n",
      "\n",
      "1. Métricas Básicas:\n",
      "Total de recomendaciones: 36745\n",
      "Productos únicos recomendados: 7918\n",
      "Ratio de diversidad: 0.2155\n",
      "\n",
      "2. Distribución por Tipo de Usuario:\n",
      "                                num_sessions  unique_users\n",
      "user_class                                                \n",
      "Usuario nuevo logueado                  3768           916\n",
      "Usuario recurrente logueado             1998           495\n",
      "Usuario recurrente no logueado         23509             1\n",
      "\n",
      "3. Análisis por País:\n",
      "\n",
      "País 57:\n",
      "- sessions: 1122.00\n",
      "- unique_products: 2465.00\n",
      "- avg_products_per_session: 5.00\n",
      "\n",
      "País 34:\n",
      "- sessions: 2860.00\n",
      "- unique_products: 4712.00\n",
      "- avg_products_per_session: 5.00\n",
      "\n",
      "País 25:\n",
      "- sessions: 2065.00\n",
      "- unique_products: 1540.00\n",
      "- avg_products_per_session: 5.00\n",
      "\n",
      "País 29:\n",
      "- sessions: 1302.00\n",
      "- unique_products: 2623.00\n",
      "- avg_products_per_session: 5.00\n",
      "\n",
      "4. Análisis de Categorías:\n",
      "\n",
      "Distribución por categoría:\n",
      "             recommendations  unique_families\n",
      "cod_section                                  \n",
      "1                      20792               81\n",
      "2                       5707               68\n",
      "3                       5398               83\n",
      "4                       4848               51\n",
      "\n",
      "5. Análisis de Cobertura:\n",
      "Cobertura de productos: 18.12%\n",
      "Cobertura de categorías: 100.00%\n",
      "Cobertura de familias: 87.56%\n",
      "\n",
      "6. Análisis de Balance:\n",
      "Estadísticas de frecuencia de recomendación:\n",
      "- Media: 4.64\n",
      "- Mediana: 1.00\n",
      "- Desv. Est.: 75.24\n",
      "- Max: 4253.00\n",
      "- Min: 1.00\n",
      "\n",
      "7. Análisis de Similitud:\n",
      "Similitud media entre sesiones: 0.1079\n",
      "Desviación estándar de similitud: 0.1751\n",
      "\n",
      "8. Métricas de Negocio:\n",
      "Ratio de productos con descuento: 7.17%\n",
      "\n",
      "9. Resumen de Hallazgos Clave:\n",
      "- Diversidad general: 21.55%\n",
      "- Cobertura de catálogo: 18.12%\n",
      "- Balance entre países: 40.92% (CV)\n"
     ]
    }
   ],
   "source": [
    "def analyze_recommendations_detailed(user_recommendations, test, train, products):\n",
    "    print(\"=== Análisis Detallado de Recomendaciones ===\")\n",
    "    \n",
    "    # 1. Métricas básicas\n",
    "    print(\"\\n1. Métricas Básicas:\")\n",
    "    all_recs = [item for sublist in user_recommendations.values() for item in sublist]\n",
    "    unique_recs = len(set(all_recs))\n",
    "    total_recs = len(all_recs)\n",
    "    print(f\"Total de recomendaciones: {total_recs}\")\n",
    "    print(f\"Productos únicos recomendados: {unique_recs}\")\n",
    "    print(f\"Ratio de diversidad: {unique_recs/total_recs:.4f}\")\n",
    "    \n",
    "    # 2. Distribución por tipo de usuario\n",
    "    print(\"\\n2. Distribución por Tipo de Usuario:\")\n",
    "    user_types = test.groupby('user_class').agg({\n",
    "        'session_id': 'count',\n",
    "        'user_id': 'nunique'\n",
    "    }).rename(columns={\n",
    "        'session_id': 'num_sessions',\n",
    "        'user_id': 'unique_users'\n",
    "    })\n",
    "    print(user_types)\n",
    "    \n",
    "    # 3. Análisis por país\n",
    "    print(\"\\n3. Análisis por País:\")\n",
    "    country_stats = {}\n",
    "    for country in test['country'].unique():\n",
    "        country_sessions = test[test['country'] == country]['session_id'].unique()\n",
    "        country_recs = [user_recommendations[s] for s in country_sessions if s in user_recommendations]\n",
    "        country_products = set([item for sublist in country_recs for item in sublist])\n",
    "        \n",
    "        country_stats[country] = {\n",
    "            'sessions': len(country_sessions),\n",
    "            'unique_products': len(country_products),\n",
    "            'avg_products_per_session': len([item for sublist in country_recs for item in sublist]) / len(country_sessions)\n",
    "        }\n",
    "    \n",
    "    for country, stats in country_stats.items():\n",
    "        print(f\"\\nPaís {country}:\")\n",
    "        for metric, value in stats.items():\n",
    "            print(f\"- {metric}: {value:.2f}\")\n",
    "    \n",
    "    # 4. Análisis de categorías\n",
    "    print(\"\\n4. Análisis de Categorías:\")\n",
    "    rec_products = pd.DataFrame(all_recs, columns=['partnumber'])\n",
    "    rec_products = rec_products.merge(products[['cod_section', 'family']], \n",
    "                                    left_on='partnumber', \n",
    "                                    right_index=True)\n",
    "    \n",
    "    category_stats = rec_products.groupby('cod_section').agg({\n",
    "        'partnumber': 'count',\n",
    "        'family': 'nunique'\n",
    "    }).rename(columns={\n",
    "        'partnumber': 'recommendations',\n",
    "        'family': 'unique_families'\n",
    "    })\n",
    "    \n",
    "    print(\"\\nDistribución por categoría:\")\n",
    "    print(category_stats.sort_values('recommendations', ascending=False).head())\n",
    "    \n",
    "    # 5. Análisis de diversidad temporal\n",
    "    print(\"\\n5. Análisis de Cobertura:\")\n",
    "    total_products = len(products)\n",
    "    total_categories = products['cod_section'].nunique()\n",
    "    total_families = products['family'].nunique()\n",
    "    \n",
    "    print(f\"Cobertura de productos: {unique_recs/total_products:.2%}\")\n",
    "    print(f\"Cobertura de categorías: {category_stats.index.nunique()/total_categories:.2%}\")\n",
    "    print(f\"Cobertura de familias: {rec_products['family'].nunique()/total_families:.2%}\")\n",
    "    \n",
    "    # 6. Análisis de balance\n",
    "    print(\"\\n6. Análisis de Balance:\")\n",
    "    rec_counts = pd.Series(all_recs).value_counts()\n",
    "    print(f\"Estadísticas de frecuencia de recomendación:\")\n",
    "    print(f\"- Media: {rec_counts.mean():.2f}\")\n",
    "    print(f\"- Mediana: {rec_counts.median():.2f}\")\n",
    "    print(f\"- Desv. Est.: {rec_counts.std():.2f}\")\n",
    "    print(f\"- Max: {rec_counts.max():.2f}\")\n",
    "    print(f\"- Min: {rec_counts.min():.2f}\")\n",
    "    \n",
    "    # 7. Análisis de similitud entre recomendaciones\n",
    "    print(\"\\n7. Análisis de Similitud:\")\n",
    "    def calculate_session_similarity(recs1, recs2):\n",
    "        return len(set(recs1) & set(recs2)) / len(set(recs1) | set(recs2))\n",
    "    \n",
    "    similarities = []\n",
    "    sample_sessions = list(user_recommendations.keys())[:1000]  # Limitar para eficiencia\n",
    "    for i in range(len(sample_sessions)-1):\n",
    "        for j in range(i+1, len(sample_sessions)):\n",
    "            sim = calculate_session_similarity(\n",
    "                user_recommendations[sample_sessions[i]],\n",
    "                user_recommendations[sample_sessions[j]]\n",
    "            )\n",
    "            similarities.append(sim)\n",
    "    \n",
    "    print(f\"Similitud media entre sesiones: {np.mean(similarities):.4f}\")\n",
    "    print(f\"Desviación estándar de similitud: {np.std(similarities):.4f}\")\n",
    "    \n",
    "    # 8. Métricas de negocio\n",
    "    print(\"\\n8. Métricas de Negocio:\")\n",
    "    discount_ratio = len([p for p in all_recs if products.loc[p, 'discount']]) / len(all_recs)\n",
    "    print(f\"Ratio de productos con descuento: {discount_ratio:.2%}\")\n",
    "    \n",
    "    # 9. Resumen de hallazgos\n",
    "    print(\"\\n9. Resumen de Hallazgos Clave:\")\n",
    "    print(f\"- Diversidad general: {unique_recs/total_recs:.2%}\")\n",
    "    print(f\"- Cobertura de catálogo: {unique_recs/total_products:.2%}\")\n",
    "    print(f\"- Balance entre países: {np.std([s['unique_products'] for s in country_stats.values()])/np.mean([s['unique_products'] for s in country_stats.values()]):.2%} (CV)\")\n",
    "    \n",
    "    return {\n",
    "        'unique_recs': unique_recs,\n",
    "        'country_stats': country_stats,\n",
    "        'category_stats': category_stats,\n",
    "        'rec_counts': rec_counts,\n",
    "        'similarities': similarities\n",
    "    }\n",
    "\n",
    "# Ejecutar el análisis detallado\n",
    "analysis_results = analyze_recommendations_detailed(user_recommendations, test, train, products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
