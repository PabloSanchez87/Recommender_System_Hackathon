{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Lambdarank + Item2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_with_embeddings_backup_v2_Item2Vec\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import lightgbm as lgb\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "print(\"Cargando modelo y datos...\")\n",
    "\n",
    "# ðŸ“¤ Cargar el modelo guardado\n",
    "model = lgb.Booster(model_file='/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/models/lightgbm_lamdarank_similar_v2.txt')\n",
    "\n",
    "# ðŸ“Œ Cargar datasets\n",
    "train = pd.read_parquet('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/last/train_final_pca20.parquet')\n",
    "test = pd.read_parquet('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/last/test_final_aligned.parquet')\n",
    "\n",
    "# ðŸ“Œ Definir las caracterÃ­sticas como en el entrenamiento\n",
    "features = [col for col in train.columns if col not in [\"session_id\", \"add_to_cart\"]]\n",
    "\n",
    "# ðŸ“Œ Convertir variables categÃ³ricas a tipo 'category'\n",
    "categorical_columns = ['family', 'cod_section_group', 'device_type', 'color_id', 'hour']\n",
    "for col in categorical_columns:\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "# ðŸ“Œ Asegurar que las columnas de test estÃ¡n en el mismo orden que en el entrenamiento\n",
    "test = test[[\"session_id\"] + features]\n",
    "\n",
    "print(\"Realizando predicciones...\")\n",
    "\n",
    "# ðŸ“Œ Predecir probabilidades en test\n",
    "test[\"score\"] = model.predict(test[features])\n",
    "\n",
    "# ðŸ“Œ Generar primeras recomendaciones por sesiÃ³n\n",
    "session_recommendations = (\n",
    "    test.groupby(\"session_id\", group_keys=False)\n",
    "    .apply(lambda x: x.sort_values(\"score\", ascending=False)[\"partnumber\"].tolist())\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(\"Entrenando modelo Item2Vec...\")\n",
    "\n",
    "# ðŸ“Œ Obtener secuencias de productos por sesiÃ³n en el conjunto de entrenamiento\n",
    "train_sessions = train.groupby('session_id')['partnumber'].apply(list)\n",
    "\n",
    "# Convertir los IDs de productos a strings (Word2Vec requiere strings)\n",
    "train_sessions = train_sessions.apply(lambda x: [str(item) for item in x])\n",
    "\n",
    "# Lista de secuencias para entrenar el modelo\n",
    "sentences = train_sessions.tolist()\n",
    "\n",
    "# ðŸ“Œ Entrenar el modelo Word2Vec\n",
    "model_i2v = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Objeto para obtener los embeddings de productos\n",
    "product_embeddings = model_i2v.wv\n",
    "\n",
    "print(\"Calculando artÃ­culos populares...\")\n",
    "\n",
    "# ðŸ“Œ Calcular los artÃ­culos mÃ¡s populares como Ãºltimo recurso\n",
    "popular_items = train.loc[train['add_to_cart'] == 1, 'partnumber'].value_counts().index.tolist()\n",
    "\n",
    "print(\"Generando recomendaciones finales usando Item2Vec...\")\n",
    "\n",
    "# NÃºmero de productos similares a obtener\n",
    "top_n_similar = 10\n",
    "\n",
    "final_recommendations = {}\n",
    "\n",
    "for session_id, items in session_recommendations.items():\n",
    "    unique_items = []\n",
    "    seen_items = set()\n",
    "    \n",
    "    # AÃ±adir items del modelo asegurando unicidad\n",
    "    for item in items:\n",
    "        if item not in seen_items:\n",
    "            unique_items.append(item)\n",
    "            seen_items.add(item)\n",
    "        if len(unique_items) == 5:\n",
    "            break\n",
    "    \n",
    "    # Si no tenemos suficientes, buscar artÃ­culos similares con Item2Vec\n",
    "    if len(unique_items) < 5:\n",
    "        for item in items:\n",
    "            similar_items = []\n",
    "            try:\n",
    "                # Obtener similares con Item2Vec\n",
    "                similar_items = product_embeddings.most_similar(str(item), topn=top_n_similar)\n",
    "                # Convertir los IDs de productos a enteros\n",
    "                similar_items = [int(sim_item[0]) for sim_item in similar_items]\n",
    "            except KeyError:\n",
    "                # Si el producto no estÃ¡ en el vocabulario\n",
    "                continue\n",
    "            for sim_item in similar_items:\n",
    "                if sim_item not in seen_items:\n",
    "                    unique_items.append(sim_item)\n",
    "                    seen_items.add(sim_item)\n",
    "                if len(unique_items) == 5:\n",
    "                    break\n",
    "            if len(unique_items) == 5:\n",
    "                break\n",
    "    \n",
    "    # Si aÃºn no tenemos suficientes, completar con artÃ­culos populares\n",
    "    if len(unique_items) < 5:\n",
    "        for item in popular_items:\n",
    "            if item not in seen_items:\n",
    "                unique_items.append(item)\n",
    "                seen_items.add(item)\n",
    "            if len(unique_items) == 5:\n",
    "                break\n",
    "\n",
    "    final_recommendations[session_id] = unique_items\n",
    "\n",
    "print(\"Guardando recomendaciones en JSON...\")\n",
    "\n",
    "# ðŸ“Œ Guardar en JSON\n",
    "output_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/back_up_json/predictions_with_embeddings_backup_v2_Item2Vec.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump({\"target\": final_recommendations}, f, indent=4)\n",
    "\n",
    "print(f\"\\nâœ… Archivo de predicciones guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_with_embeddings_backup_v2_cosine\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import lightgbm as lgb\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "print(\"Cargando modelo y datos...\")\n",
    "\n",
    "# ðŸ“¤ Cargar el modelo guardado\n",
    "model = lgb.Booster(model_file='/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/models/lightgbm_lamdarank_similar_v2.txt')\n",
    "\n",
    "# ðŸ“Œ Cargar datasets\n",
    "train = pd.read_parquet('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/last/train_final_pca20.parquet')\n",
    "test = pd.read_parquet('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/last/test_final_aligned.parquet')\n",
    "\n",
    "# ðŸ“Œ Definir las caracterÃ­sticas como en el entrenamiento\n",
    "features = [col for col in train.columns if col not in [\"session_id\", \"add_to_cart\"]]\n",
    "\n",
    "# ðŸ“Œ Convertir variables categÃ³ricas a tipo 'category'\n",
    "categorical_columns = ['family', 'cod_section_group', 'device_type', 'color_id', 'hour']\n",
    "for col in categorical_columns:\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "# ðŸ“Œ Asegurar que las columnas de test estÃ¡n en el mismo orden que en el entrenamiento\n",
    "test = test[[\"session_id\"] + features]\n",
    "\n",
    "print(\"Realizando predicciones...\")\n",
    "\n",
    "# ðŸ“Œ Predecir probabilidades en test\n",
    "test[\"score\"] = model.predict(test[features])\n",
    "\n",
    "# ðŸ“Œ Generar primeras recomendaciones por sesiÃ³n\n",
    "session_recommendations = (\n",
    "    test.groupby(\"session_id\", group_keys=False)\n",
    "    .apply(lambda x: x.sort_values(\"score\", ascending=False)[\"partnumber\"].tolist())\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(\"Preparando embeddings y modelo de vecinos mÃ¡s cercanos...\")\n",
    "\n",
    "# ðŸ“Œ Preparar los embeddings y aÃ±adir las categorÃ­as\n",
    "embedding_cols = [f'embedding_pca_{i}' for i in range(1, 21)]\n",
    "item_features = train.drop_duplicates('partnumber')[['partnumber', 'family'] + embedding_cols].set_index('partnumber')\n",
    "\n",
    "# ðŸ“Œ Entrenar modelos de NearestNeighbors con diferentes mÃ©tricas\n",
    "distance_metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "n_neighbors = 15  # Podemos ajustar este valor segÃºn sea necesario\n",
    "\n",
    "nn_models = {}\n",
    "for metric in distance_metrics:\n",
    "    nn_model = NearestNeighbors(metric=metric, algorithm='brute')\n",
    "    nn_model.fit(item_features[embedding_cols].values)\n",
    "    nn_models[metric] = nn_model\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para obtener artÃ­culos similares con filtros\n",
    "def get_similar_items(item_id, n_neighbors=15, metric='cosine'):\n",
    "    if item_id not in item_features.index:\n",
    "        return []\n",
    "    item_vector = item_features.loc[item_id, embedding_cols].values.reshape(1, -1)\n",
    "    item_family = item_features.loc[item_id, 'family']  # Obtener la categorÃ­a 'family' del item\n",
    "    nn_model = nn_models[metric]\n",
    "    distances, indices = nn_model.kneighbors(item_vector, n_neighbors=n_neighbors)\n",
    "    similar_items = item_features.iloc[indices[0]].index.tolist()\n",
    "    # Excluir el propio item\n",
    "    similar_items = [item for item in similar_items if item != item_id]\n",
    "    # Filtrar por la misma 'family'\n",
    "    similar_items_filtered = [item for item in similar_items if item_features.loc[item, 'family'] == item_family]\n",
    "    return similar_items_filtered\n",
    "\n",
    "print(\"Calculando artÃ­culos populares...\")\n",
    "\n",
    "# ðŸ“Œ Calcular los artÃ­culos mÃ¡s populares como Ãºltimo recurso\n",
    "popular_items = train.loc[train['add_to_cart'] == 1, 'partnumber'].value_counts().index.tolist()\n",
    "\n",
    "print(\"Generando recomendaciones finales...\")\n",
    "\n",
    "# ðŸ“Œ Elegir mÃ©trica de distancia y nÃºmero de vecinos\n",
    "chosen_metric = 'cosine'  # Puedes cambiar a 'euclidean' o 'manhattan' para probar\n",
    "n_neighbors = 15          # Puedes ajustar este valor\n",
    "\n",
    "# ðŸ“Œ Generar recomendaciones finales asegurando 5 artÃ­culos Ãºnicos por sesiÃ³n\n",
    "final_recommendations = {}\n",
    "\n",
    "for session_id, items in session_recommendations.items():\n",
    "    unique_items = []\n",
    "    seen_items = set()\n",
    "    \n",
    "    # AÃ±adir items del modelo asegurando unicidad\n",
    "    for item in items:\n",
    "        if item not in seen_items:\n",
    "            unique_items.append(item)\n",
    "            seen_items.add(item)\n",
    "        if len(unique_items) == 5:\n",
    "            break\n",
    "    \n",
    "    # Si no tenemos suficientes, buscar artÃ­culos similares con filtros\n",
    "    if len(unique_items) < 5:\n",
    "        for item in items:\n",
    "            similar_items = get_similar_items(item, n_neighbors=n_neighbors, metric=chosen_metric)\n",
    "            for sim_item in similar_items:\n",
    "                if sim_item not in seen_items:\n",
    "                    unique_items.append(sim_item)\n",
    "                    seen_items.add(sim_item)\n",
    "                if len(unique_items) == 5:\n",
    "                    break\n",
    "            if len(unique_items) == 5:\n",
    "                break\n",
    "    \n",
    "    # Si aÃºn no tenemos suficientes, completar con artÃ­culos populares\n",
    "    if len(unique_items) < 5:\n",
    "        for item in popular_items:\n",
    "            if item not in seen_items:\n",
    "                unique_items.append(item)\n",
    "                seen_items.add(item)\n",
    "            if len(unique_items) == 5:\n",
    "                break\n",
    "\n",
    "    final_recommendations[session_id] = unique_items\n",
    "\n",
    "print(\"Guardando recomendaciones en JSON...\")\n",
    "\n",
    "# ðŸ“Œ Guardar en JSON\n",
    "output_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/back_up_json/predictions_with_embeddings_backup_v2_cosine.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump({\"target\": final_recommendations}, f, indent=4)\n",
    "\n",
    "print(f\"\\nâœ… Archivo de predicciones guardado en: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
