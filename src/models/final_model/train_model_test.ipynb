{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Preparando folds de validación...\n",
      "Entrenando el modelo con validación cruzada...\n",
      "\n",
      "Fold 1/2\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[807]\tvalid_0's ndcg@5: 0.934851\tvalid_0's map@5: 0.921967\n",
      "Evaluated only: ndcg@5\n",
      "\n",
      "\n",
      "Fold 2/2\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[976]\tvalid_0's ndcg@5: 0.93486\tvalid_0's map@5: 0.921957\n",
      "Evaluated only: ndcg@5\n",
      "\n",
      "\n",
      "📥 Modelo guardado\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"Cargando datos...\")\n",
    "\n",
    "# 📌 Cargar dataset y tomar una muestra del 30%\n",
    "train = pd.read_parquet('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/last/train_final_pca20.parquet').sample(frac=0.30, random_state=42)\n",
    "\n",
    "# 📌 Convertir variables categóricas a tipo 'category'\n",
    "categorical_columns = ['family', 'cod_section_group', 'device_type', 'color_id', 'hour']\n",
    "for col in categorical_columns:\n",
    "    train[col] = train[col].astype('category')\n",
    "\n",
    "# 📌 Definir características y etiqueta\n",
    "features = [col for col in train.columns if col not in [\"session_id\", \"add_to_cart\"]]\n",
    "target = \"add_to_cart\"\n",
    "\n",
    "print(\"Preparando folds de validación...\")\n",
    "\n",
    "# 📌 Usar GroupKFold para evitar fuga entre sesiones\n",
    "group_kfold = GroupKFold(n_splits=2)\n",
    "folds = list(group_kfold.split(train[features], train[target], groups=train[\"session_id\"]))\n",
    "\n",
    "# 📌 Definir las características categóricas\n",
    "categorical_features = ['family', 'country', 'pagetype_group', 'cod_section_group', 'device_type', 'color_id', 'hour']\n",
    "\n",
    "# 📌 Calcular scale_pos_weight para manejar desbalanceo de clases\n",
    "neg_count = train[target].value_counts()[0]\n",
    "pos_count = train[target].value_counts()[1]\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "# 📌 Definir parámetros del modelo con mayor regularización\n",
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"metric\": [\"ndcg\", \"map\"],\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "    \"num_leaves\": 255,\n",
    "    \"max_depth\": 9,\n",
    "    \"verbosity\": -1,\n",
    "    \"lambdarank_truncation_level\": 15,\n",
    "    \"feature_fraction\": 0.7,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"lambda_l1\": 35.0,    # Incrementado de 5.0 a 20.0\n",
    "    \"lambda_l2\": 70.0,    # Incrementado de 15.0 a 50.0\n",
    "    \"lambdarank_norm\": True,\n",
    "    \"label_gain\": [i for i in range(25)],\n",
    "    \"seed\": 42,\n",
    "    \"eval_at\": [5],\n",
    "    \"num_threads\": 0,\n",
    "    \"scale_pos_weight\": scale_pos_weight,  # Manejar desbalanceo\n",
    "}\n",
    "\n",
    "print(\"Entrenando el modelo con validación cruzada...\\n\")\n",
    "\n",
    "# 🚀 Entrenar usando GroupKFold\n",
    "models = []\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"Fold {fold + 1}/{len(folds)}\")\n",
    "    X_train = train.iloc[train_idx][features]\n",
    "    y_train = train.iloc[train_idx][target]\n",
    "    session_train = train.iloc[train_idx][\"session_id\"]\n",
    "    X_val = train.iloc[val_idx][features]\n",
    "    y_val = train.iloc[val_idx][target]\n",
    "    session_val = train.iloc[val_idx][\"session_id\"]\n",
    "\n",
    "    # 📌 Calcular `query` correctamente\n",
    "    query_train = session_train.value_counts().sort_index().tolist()\n",
    "    query_val = session_val.value_counts().sort_index().tolist()\n",
    "\n",
    "    # 📌 Crear datasets de LightGBM con `group` y características categóricas\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, group=query_train, categorical_feature=categorical_features, free_raw_data=False)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, group=query_val, reference=lgb_train, categorical_feature=categorical_features, free_raw_data=False)\n",
    "\n",
    "    # 🚀 Entrenar el modelo con early stopping\n",
    "    callbacks = [lgb.early_stopping(stopping_rounds=30, first_metric_only=True, verbose=True)]\n",
    "    \n",
    "    model = lgb.train(params, lgb_train, valid_sets=[lgb_val], num_boost_round=2000, callbacks=callbacks)\n",
    "    models.append(model)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "# 📥 Guardar el último modelo entrenado\n",
    "models[-1].save_model('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/models/lightgbm_lamdarank_last_try.txt')\n",
    "print(\"📥 Modelo guardado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 📌 Restaurar `session_id` y `partnumber` en `X_val`\n",
    "X_val[\"session_id\"] = session_val.values  \n",
    "X_val[\"partnumber\"] = train.loc[X_val.index, \"partnumber\"]  # Recuperamos el producto real\n",
    "\n",
    "# 📌 Crear `y_val_df` asegurando que `session_id` y `partnumber` estén presentes\n",
    "y_val_df = pd.DataFrame({\n",
    "    \"session_id\": np.ravel(session_val),  \n",
    "    \"partnumber\": train.loc[y_val.index, \"partnumber\"],  # Recuperamos productos comprados\n",
    "    \"add_to_cart\": np.ravel(y_val)\n",
    "})\n",
    "\n",
    "# 📌 Obtener las características utilizadas en el entrenamiento\n",
    "features = model.feature_name()\n",
    "\n",
    "# 📌 Asegurar que `X_val` tenga exactamente las mismas columnas antes de predecir\n",
    "X_val_pred = X_val[features]\n",
    "\n",
    "# 📌 Generar predicciones en el conjunto de validación\n",
    "X_val[\"score\"] = model.predict(X_val_pred)\n",
    "\n",
    "# 📌 Obtener recomendaciones por sesión (productos en lugar de índices)\n",
    "session_recommendations = (\n",
    "    X_val.sort_values([\"session_id\", \"score\"], ascending=[True, False])\n",
    "    .groupby(\"session_id\")[\"partnumber\"]\n",
    "    .apply(list)  # Ahora obtenemos productos en lugar de índices\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# 📌 Obtener ground truth (productos realmente añadidos al carrito)\n",
    "true_items = (\n",
    "    y_val_df[y_val_df[\"add_to_cart\"] == 1]\n",
    "    .groupby(\"session_id\")[\"partnumber\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# 📌 Función de evaluación corregida\n",
    "def evaluate_ranking(predictions, ground_truth, k=5):\n",
    "    ndcg_scores, map_scores, mrr_scores, hit_rates = [], [], [], []\n",
    "\n",
    "    for session_id, recommended_items in predictions.items():\n",
    "        true_list = ground_truth.get(session_id, [])\n",
    "\n",
    "        # 📌 Filtrar sesiones sin suficientes elementos para evaluar NDCGa\n",
    "        if len(true_list) < 2 or len(recommended_items) < 2:\n",
    "            continue  \n",
    "\n",
    "        # 📌 Asegurar que y_true es 2D con valores de relevancia binarios\n",
    "        y_true = np.array([[(1 if i in true_list else 0) for i in recommended_items[:k]]])\n",
    "        y_score = np.array([[k - i for i in range(min(k, len(recommended_items)))]])\n",
    "\n",
    "        # 📌 Calcular métricas solo si hay al menos un relevante\n",
    "        if np.sum(y_true) > 0:\n",
    "            ndcg_scores.append(ndcg_score(y_true, y_score))\n",
    "            map_scores.append(average_precision_score(y_true[0], y_score[0]))\n",
    "            mrr_scores.append(next((1 / (i + 1) for i, val in enumerate(y_true[0]) if val == 1), 0))\n",
    "            hit_rates.append(1)\n",
    "        else:\n",
    "            ndcg_scores.append(0)\n",
    "            map_scores.append(0)\n",
    "            mrr_scores.append(0)\n",
    "            hit_rates.append(0)\n",
    "\n",
    "    return {\n",
    "        \"NDCG@5\": np.mean(ndcg_scores) if ndcg_scores else 0,\n",
    "        \"MAP@5\": np.mean(map_scores) if map_scores else 0,\n",
    "        \"MRR\": np.mean(mrr_scores) if mrr_scores else 0,\n",
    "        \"Hit Rate @5\": np.mean(hit_rates) if hit_rates else 0,\n",
    "    }\n",
    "\n",
    "# 📌 Evaluar el modelo\n",
    "metrics = evaluate_ranking(session_recommendations, true_items)\n",
    "print(\"\\n📊 Métricas de Evaluación:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo y datos...\n",
      "Realizando predicciones...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_525577/2297624147.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sort_values(\"score\", ascending=False)[\"partnumber\"].tolist())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo Item2Vec...\n",
      "Calculando artículos populares...\n",
      "Generando recomendaciones finales usando Item2Vec...\n",
      "Guardando recomendaciones en JSON...\n",
      "\n",
      "✅ Archivo de predicciones guardado en: /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/back_up_json/predictions_last_try.json\n"
     ]
    }
   ],
   "source": [
    "# predictions_with_embeddings_backup_v3_Item2Vec -- 253/900\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import lightgbm as lgb\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "print(\"Cargando modelo y datos...\")\n",
    "\n",
    "# 📤 Cargar el modelo guardado\n",
    "model = lgb.Booster(model_file='/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/models/lightgbm_lamdarank_last_try.txt')\n",
    "\n",
    "# 📌 Cargar datasets\n",
    "train = pd.read_parquet('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/last/train_final_pca20.parquet')\n",
    "test = pd.read_parquet('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/last/test_final_aligned.parquet')\n",
    "\n",
    "# 📌 Definir las características como en el entrenamiento\n",
    "features = [col for col in train.columns if col not in [\"session_id\", \"add_to_cart\"]]\n",
    "\n",
    "# 📌 Convertir variables categóricas a tipo 'category'\n",
    "categorical_columns = ['family', 'cod_section_group', 'device_type', 'color_id', 'hour']\n",
    "for col in categorical_columns:\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "# 📌 Asegurar que las columnas de test están en el mismo orden que en el entrenamiento\n",
    "test = test[[\"session_id\"] + features]\n",
    "\n",
    "print(\"Realizando predicciones...\")\n",
    "\n",
    "# 📌 Predecir probabilidades en test\n",
    "test[\"score\"] = model.predict(test[features])\n",
    "\n",
    "# 📌 Generar primeras recomendaciones por sesión\n",
    "session_recommendations = (\n",
    "    test.groupby(\"session_id\", group_keys=False)\n",
    "    .apply(lambda x: x.sort_values(\"score\", ascending=False)[\"partnumber\"].tolist())\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(\"Entrenando modelo Item2Vec...\")\n",
    "\n",
    "# 📌 Obtener secuencias de productos por sesión en el conjunto de entrenamiento\n",
    "train_sessions = train.groupby('session_id')['partnumber'].apply(list)\n",
    "\n",
    "# Convertir los IDs de productos a strings (Word2Vec requiere strings)\n",
    "train_sessions = train_sessions.apply(lambda x: [str(item) for item in x])\n",
    "\n",
    "# Lista de secuencias para entrenar el modelo\n",
    "sentences = train_sessions.tolist()\n",
    "\n",
    "# 📌 Entrenar el modelo Word2Vec\n",
    "\n",
    "model_i2v = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=110,        # Ajuste de vector_size\n",
    "    window=8,              # Ajuste de window\n",
    "    min_count=2,            # Ajuste de min_count\n",
    "    workers=8,\n",
    "    sg=0,                   # Uso de Skip-Gram // 0 CBOW \n",
    "    negative=20,            # Muestreo negativo +\n",
    "    epochs=60,              # Aumento de épocas +\n",
    "    sample=1e-4,            # +     \n",
    "    # min_alpha=0.0001,      # Tasa de aprendizaje mínima       \n",
    ")\n",
    "\n",
    "# Objeto para obtener los embeddings de productos\n",
    "product_embeddings = model_i2v.wv\n",
    "\n",
    "print(\"Calculando artículos populares...\")\n",
    "\n",
    "# 📌 Calcular los artículos más populares como último recurso\n",
    "popular_items = train.loc[train['add_to_cart'] == 1, 'partnumber'].value_counts().index.tolist()\n",
    "\n",
    "print(\"Generando recomendaciones finales usando Item2Vec...\")\n",
    "\n",
    "# Número de productos similares a obtener\n",
    "top_n_similar = 5\n",
    "\n",
    "final_recommendations = {}\n",
    "\n",
    "for session_id, items in session_recommendations.items():\n",
    "    unique_items = []\n",
    "    seen_items = set()\n",
    "    \n",
    "    # Añadir items del modelo asegurando unicidad\n",
    "    for item in items:\n",
    "        if item not in seen_items:\n",
    "            unique_items.append(item)\n",
    "            seen_items.add(item)\n",
    "        if len(unique_items) == 5:\n",
    "            break\n",
    "    \n",
    "    # Si no tenemos suficientes, buscar artículos similares con Item2Vec\n",
    "    if len(unique_items) < 5:\n",
    "        for item in items:\n",
    "            similar_items = []\n",
    "            try:\n",
    "                # Obtener similares con Item2Vec\n",
    "                similar_items = product_embeddings.most_similar(str(item), topn=top_n_similar)\n",
    "                # Convertir los IDs de productos a enteros\n",
    "                similar_items = [int(sim_item[0]) for sim_item in similar_items]\n",
    "            except KeyError:\n",
    "                # Si el producto no está en el vocabulario\n",
    "                continue\n",
    "            for sim_item in similar_items:\n",
    "                if sim_item not in seen_items:\n",
    "                    unique_items.append(sim_item)\n",
    "                    seen_items.add(sim_item)\n",
    "                if len(unique_items) == 5:\n",
    "                    break\n",
    "            if len(unique_items) == 5:\n",
    "                break\n",
    "    \n",
    "    # Si aún no tenemos suficientes, completar con artículos populares\n",
    "    if len(unique_items) < 5:\n",
    "        for item in popular_items:\n",
    "            if item not in seen_items:\n",
    "                unique_items.append(item)\n",
    "                seen_items.add(item)\n",
    "            if len(unique_items) == 5:\n",
    "                break\n",
    "\n",
    "    final_recommendations[session_id] = unique_items\n",
    "\n",
    "print(\"Guardando recomendaciones en JSON...\")\n",
    "\n",
    "# 📌 Guardar en JSON\n",
    "output_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/back_up_json/predictions_last_try.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump({\"target\": final_recommendations}, f, indent=4)\n",
    "\n",
    "print(f\"\\n✅ Archivo de predicciones guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Preparando folds de validación...\n",
      "Entrenando el modelo con validación cruzada...\n",
      "\n",
      "Fold 1/2\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's ndcg@5: 0.933622\tvalid_0's map@5: 0.920671\n",
      "Evaluated only: ndcg@5\n",
      "\n",
      "\n",
      "Fold 2/2\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid_0's ndcg@5: 0.933661\tvalid_0's map@5: 0.92069\n",
      "Evaluated only: ndcg@5\n",
      "\n",
      "\n",
      "📥 Modelo guardado\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"Cargando datos...\")\n",
    "\n",
    "# 📌 Cargar dataset y tomar una muestra del 50%\n",
    "train = pd.read_parquet('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/last/train_final_pca20.parquet').sample(frac=0.30, random_state=42)\n",
    "\n",
    "# 📌 Seleccionar solo las variables más importantes\n",
    "important_features = [\n",
    "    'country', \n",
    "    'device_type', \n",
    "    'pagetype_group', \n",
    "    'cod_section_group', \n",
    "    'family', \n",
    "    'color_id', \n",
    "    'hour', \n",
    "    'user_id', \n",
    "    'partnumber', \n",
    "    'RFM_PCA'\n",
    "]\n",
    "\n",
    "# 📌 Convertir variables categóricas a tipo 'category'\n",
    "categorical_columns = ['country', 'device_type', 'pagetype_group', 'cod_section_group', 'family', 'color_id', 'hour']\n",
    "for col in categorical_columns:\n",
    "    train[col] = train[col].astype('category')\n",
    "\n",
    "# 📌 Definir características y etiqueta\n",
    "features = important_features  # Usar solo las variables importantes\n",
    "target = \"add_to_cart\"\n",
    "\n",
    "print(\"Preparando folds de validación...\")\n",
    "\n",
    "# 📌 Usar GroupKFold para evitar fuga entre sesiones\n",
    "group_kfold = GroupKFold(n_splits=2)\n",
    "folds = list(group_kfold.split(train[features], train[target], groups=train[\"session_id\"]))\n",
    "\n",
    "# 📌 Definir las características categóricas\n",
    "categorical_features = ['country', 'device_type', 'pagetype_group', 'cod_section_group', 'family', 'color_id', 'hour']\n",
    "\n",
    "# 📌 Calcular scale_pos_weight para manejar desbalanceo de clases\n",
    "neg_count = train[target].value_counts()[0]\n",
    "pos_count = train[target].value_counts()[1]\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "# 📌 Definir parámetros del modelo con mayor regularización\n",
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"metric\": [\"ndcg\", \"map\"],\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "    \"num_leaves\": 255,\n",
    "    \"max_depth\": 9,\n",
    "    \"verbosity\": -1,\n",
    "    \"lambdarank_truncation_level\": 15,\n",
    "    \"feature_fraction\": 0.7,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"lambda_l1\": 35.0,    # Incrementado de 5.0 a 35.0\n",
    "    \"lambda_l2\": 70.0,    # Incrementado de 15.0 a 70.0\n",
    "    \"lambdarank_norm\": True,\n",
    "    \"label_gain\": [i for i in range(25)],\n",
    "    \"seed\": 42,\n",
    "    \"eval_at\": [5],\n",
    "    \"num_threads\": 0,\n",
    "    \"scale_pos_weight\": scale_pos_weight,  # Manejar desbalanceo\n",
    "}\n",
    "\n",
    "print(\"Entrenando el modelo con validación cruzada...\\n\")\n",
    "\n",
    "# 🚀 Entrenar usando GroupKFold\n",
    "models = []\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"Fold {fold + 1}/{len(folds)}\")\n",
    "    X_train = train.iloc[train_idx][features]\n",
    "    y_train = train.iloc[train_idx][target]\n",
    "    session_train = train.iloc[train_idx][\"session_id\"]\n",
    "    X_val = train.iloc[val_idx][features]\n",
    "    y_val = train.iloc[val_idx][target]\n",
    "    session_val = train.iloc[val_idx][\"session_id\"]\n",
    "\n",
    "    # 📌 Calcular `query` correctamente\n",
    "    query_train = session_train.value_counts().sort_index().tolist()\n",
    "    query_val = session_val.value_counts().sort_index().tolist()\n",
    "\n",
    "    # 📌 Crear datasets de LightGBM con `group` y características categóricas\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, group=query_train, categorical_feature=categorical_features, free_raw_data=False)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, group=query_val, reference=lgb_train, categorical_feature=categorical_features, free_raw_data=False)\n",
    "\n",
    "    # 🚀 Entrenar el modelo con early stopping\n",
    "    callbacks = [lgb.early_stopping(stopping_rounds=30, first_metric_only=True, verbose=True)]\n",
    "    \n",
    "    model = lgb.train(params, lgb_train, valid_sets=[lgb_val], num_boost_round=2000, callbacks=callbacks)\n",
    "    models.append(model)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "# 📥 Guardar el último modelo entrenado\n",
    "models[-1].save_model('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/models/lightgbm_lamdarank_last_try_v2.txt')\n",
    "print(\"📥 Modelo guardado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo y datos...\n",
      "Realizando predicciones...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_525577/21717755.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sort_values(\"score\", ascending=False)[\"partnumber\"].tolist())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo Item2Vec...\n",
      "Calculando artículos populares...\n",
      "Generando recomendaciones finales usando Item2Vec...\n",
      "Guardando recomendaciones en JSON...\n",
      "\n",
      "✅ Archivo de predicciones guardado en: /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/back_up_json/predictions_last_try_v2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import lightgbm as lgb\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "print(\"Cargando modelo y datos...\")\n",
    "\n",
    "# 📤 Cargar el modelo guardado\n",
    "model = lgb.Booster(model_file='/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/models/lightgbm_lamdarank_last_try_v2.txt')\n",
    "\n",
    "# 📌 Cargar datasets\n",
    "train = pd.read_parquet('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/last/train_final_pca20.parquet')\n",
    "test = pd.read_parquet('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/last/test_final_aligned.parquet')\n",
    "\n",
    "# 📌 Seleccionar solo las variables más importantes\n",
    "important_features = [\n",
    "    'country', \n",
    "    'device_type', \n",
    "    'pagetype_group', \n",
    "    'cod_section_group', \n",
    "    'family', \n",
    "    'color_id', \n",
    "    'hour', \n",
    "    'user_id', \n",
    "    'partnumber', \n",
    "    'RFM_PCA'\n",
    "]\n",
    "\n",
    "# 📌 Definir las características como en el entrenamiento\n",
    "features = important_features  # Usar solo las variables importantes\n",
    "\n",
    "# 📌 Convertir variables categóricas a tipo 'category'\n",
    "categorical_columns = ['country', 'device_type', 'pagetype_group', 'cod_section_group', 'family', 'color_id', 'hour']\n",
    "for col in categorical_columns:\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "# 📌 Asegurar que las columnas de test están en el mismo orden que en el entrenamiento\n",
    "test = test[[\"session_id\"] + features]\n",
    "\n",
    "print(\"Realizando predicciones...\")\n",
    "\n",
    "# 📌 Predecir probabilidades en test\n",
    "test[\"score\"] = model.predict(test[features])\n",
    "\n",
    "# 📌 Generar primeras recomendaciones por sesión\n",
    "session_recommendations = (\n",
    "    test.groupby(\"session_id\", group_keys=False)\n",
    "    .apply(lambda x: x.sort_values(\"score\", ascending=False)[\"partnumber\"].tolist())\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(\"Entrenando modelo Item2Vec...\")\n",
    "\n",
    "# 📌 Obtener secuencias de productos por sesión en el conjunto de entrenamiento\n",
    "train_sessions = train.groupby('session_id')['partnumber'].apply(list)\n",
    "\n",
    "# Convertir los IDs de productos a strings (Word2Vec requiere strings)\n",
    "train_sessions = train_sessions.apply(lambda x: [str(item) for item in x])\n",
    "\n",
    "# Lista de secuencias para entrenar el modelo\n",
    "sentences = train_sessions.tolist()\n",
    "\n",
    "# 📌 Entrenar el modelo Word2Vec\n",
    "model_i2v = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=110,        # Ajuste de vector_size\n",
    "    window=8,               # Ajuste de window\n",
    "    min_count=2,            # Ajuste de min_count\n",
    "    workers=10,\n",
    "    sg=0,                   # Uso de CBOW\n",
    "    negative=20,            # Muestreo negativo aumentado\n",
    "    epochs=60,              # Aumento de épocas\n",
    "    sample=1e-4,            # Muestreo\n",
    "    # min_alpha=0.0001,      # Tasa de aprendizaje mínima       \n",
    ")\n",
    "\n",
    "# Objeto para obtener los embeddings de productos\n",
    "product_embeddings = model_i2v.wv\n",
    "\n",
    "print(\"Calculando artículos populares...\")\n",
    "\n",
    "# 📌 Calcular los artículos más populares como último recurso\n",
    "popular_items = train.loc[train['add_to_cart'] == 1, 'partnumber'].value_counts().index.tolist()\n",
    "\n",
    "print(\"Generando recomendaciones finales usando Item2Vec...\")\n",
    "\n",
    "# Número de productos similares a obtener\n",
    "top_n_similar = 5\n",
    "\n",
    "final_recommendations = {}\n",
    "\n",
    "for session_id, items in session_recommendations.items():\n",
    "    unique_items = []\n",
    "    seen_items = set()\n",
    "    \n",
    "    # Añadir items del modelo asegurando unicidad\n",
    "    for item in items:\n",
    "        if item not in seen_items:\n",
    "            unique_items.append(item)\n",
    "            seen_items.add(item)\n",
    "        if len(unique_items) == 5:\n",
    "            break\n",
    "    \n",
    "    # Si no tenemos suficientes, buscar artículos similares con Item2Vec\n",
    "    if len(unique_items) < 5:\n",
    "        for item in items:\n",
    "            similar_items = []\n",
    "            try:\n",
    "                # Obtener similares con Item2Vec\n",
    "                similar_items = product_embeddings.most_similar(str(item), topn=top_n_similar)\n",
    "                # Convertir los IDs de productos a enteros\n",
    "                similar_items = [int(sim_item[0]) for sim_item in similar_items]\n",
    "            except KeyError:\n",
    "                # Si el producto no está en el vocabulario\n",
    "                continue\n",
    "            for sim_item in similar_items:\n",
    "                if sim_item not in seen_items:\n",
    "                    unique_items.append(sim_item)\n",
    "                    seen_items.add(sim_item)\n",
    "                if len(unique_items) == 5:\n",
    "                    break\n",
    "            if len(unique_items) == 5:\n",
    "                break\n",
    "    \n",
    "    # Si aún no tenemos suficientes, completar con artículos populares\n",
    "    if len(unique_items) < 5:\n",
    "        for item in popular_items:\n",
    "            if item not in seen_items:\n",
    "                unique_items.append(item)\n",
    "                seen_items.add(item)\n",
    "            if len(unique_items) == 5:\n",
    "                break\n",
    "\n",
    "    final_recommendations[session_id] = unique_items\n",
    "\n",
    "print(\"Guardando recomendaciones en JSON...\")\n",
    "\n",
    "# 📌 Guardar en JSON\n",
    "output_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/src/models/final/back_up_json/predictions_last_try_v2.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump({\"target\": final_recommendations}, f, indent=4)\n",
    "\n",
    "print(f\"\\n✅ Archivo de predicciones guardado en: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
