{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocesando datos ---\n",
      "\n",
      "--- Enriqueciendo características ---\n",
      "\n",
      "--- Calculado scale_pos_weight: 15.95 ---\n",
      "\n",
      "--- Dividiendo datos ---\n",
      "\n",
      "--- Configurando y entrenando el modelo LambdaMART ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.843676\ttrain's ndcg@3: 0.85858\ttrain's ndcg@5: 0.869545\tval's ndcg@1: 0.842769\tval's ndcg@3: 0.85768\tval's ndcg@5: 0.868707\n",
      "[20]\ttrain's ndcg@1: 0.843748\ttrain's ndcg@3: 0.858609\ttrain's ndcg@5: 0.869587\tval's ndcg@1: 0.84282\tval's ndcg@3: 0.857825\tval's ndcg@5: 0.868845\n",
      "[30]\ttrain's ndcg@1: 0.843708\ttrain's ndcg@3: 0.858674\ttrain's ndcg@5: 0.869643\tval's ndcg@1: 0.843045\tval's ndcg@3: 0.857745\tval's ndcg@5: 0.868878\n",
      "[40]\ttrain's ndcg@1: 0.843737\ttrain's ndcg@3: 0.858736\ttrain's ndcg@5: 0.86968\tval's ndcg@1: 0.842918\tval's ndcg@3: 0.857886\tval's ndcg@5: 0.868909\n",
      "[50]\ttrain's ndcg@1: 0.8437\ttrain's ndcg@3: 0.858691\ttrain's ndcg@5: 0.869702\tval's ndcg@1: 0.843054\tval's ndcg@3: 0.857943\tval's ndcg@5: 0.868944\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttrain's ndcg@1: 0.843713\ttrain's ndcg@3: 0.858574\ttrain's ndcg@5: 0.86953\tval's ndcg@1: 0.843237\tval's ndcg@3: 0.857986\tval's ndcg@5: 0.868918\n",
      "\n",
      "--- Guardando el modelo ---\n",
      "\n",
      "--- Modelo guardado en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model.txt ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# --- Preprocesamiento de datos ---\n",
    "def preprocess_data(df):\n",
    "    print(\"\\n--- Preprocesando datos ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    return df\n",
    "\n",
    "def enrich_features(df):\n",
    "    print(\"\\n--- Enriqueciendo características ---\")\n",
    "    session_length = df.groupby('session_id')['partnumber'].transform('count')\n",
    "    df['session_length'] = session_length\n",
    "    df['country_popularity'] = df.groupby('country')['partnumber'].transform('count') / session_length\n",
    "    df['product_interaction_rate'] = df.groupby('partnumber')['session_id'].transform('nunique') / session_length\n",
    "    return df\n",
    "\n",
    "# Cargar y enriquecer datos\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data.pkl')\n",
    "train_df = preprocess_data(train_df)\n",
    "train_df = enrich_features(train_df)\n",
    "\n",
    "# Separar características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Calcular scale_pos_weight\n",
    "pos_weight = len(y[y == 0]) / len(y[y == 1])\n",
    "print(f\"\\n--- Calculado scale_pos_weight: {pos_weight:.2f} ---\")\n",
    "\n",
    "# Dividir datos\n",
    "print(\"\\n--- Dividiendo datos ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "groups_train = groups.iloc[train_idx].value_counts().values\n",
    "groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "# Configurar y entrenar modelo\n",
    "print(\"\\n--- Configurando y entrenando el modelo LambdaMART ---\")\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 40,\n",
    "    'min_data_in_leaf': 15,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu',\n",
    "    'scale_pos_weight': pos_weight\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping(stopping_rounds=50, verbose=True),\n",
    "    log_evaluation(period=10)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=200,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Guardar modelo\n",
    "print(\"\\n--- Guardando el modelo ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model.txt'\n",
    "model.save_model(model_path)\n",
    "print(f\"\\n--- Modelo guardado en {model_path} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando datos de prueba ---\n",
      "\n",
      "--- Preprocesando y enriqueciendo datos de prueba ---\n",
      "\n",
      "--- Generando predicciones ---\n",
      "\n",
      "--- Guardando predicciones ---\n",
      "Predicciones guardadas en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Preprocesamiento y enriquecimiento ---\n",
    "def preprocess_and_enrich_test_data(df):\n",
    "    print(\"\\n--- Preprocesando y enriqueciendo datos de prueba ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    session_length = df.groupby('session_id')['partnumber'].transform('count')\n",
    "    df['session_length'] = session_length\n",
    "    df['country_popularity'] = df.groupby('country')['partnumber'].transform('count') / session_length\n",
    "    df['product_interaction_rate'] = df.groupby('partnumber')['session_id'].transform('nunique') / session_length\n",
    "    return df\n",
    "\n",
    "# --- Generar JSON ---\n",
    "def generate_predictions(model_path, test_path, output_path):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_and_enrich_test_data(test_df)\n",
    "\n",
    "    session_ids = test_df['session_id'].unique()\n",
    "    predictions = {}\n",
    "    popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "    print(\"\\n--- Generando predicciones ---\")\n",
    "    for session_id in session_ids:\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[X.columns.tolist()]\n",
    "        session_data.loc[:, 'score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        while len(recommended_products) < 5:\n",
    "            recommended_products.append(popular_products[len(recommended_products) % len(popular_products)])\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "    print(f\"Predicciones guardadas en {output_path}\")\n",
    "\n",
    "\n",
    "# --- Ejecutar predicciones y métricas ---\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model.txt'\n",
    "    test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "    output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched.json'\n",
    "\n",
    "    # Generar JSON\n",
    "    generate_predictions(model_path, test_path, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando datos de prueba ---\n",
      "\n",
      "--- Preprocesando y enriqueciendo datos de prueba ---\n",
      "\n",
      "--- Generando predicciones ---\n",
      "\n",
      "--- Guardando predicciones ---\n",
      "Predicciones guardadas en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched_v2.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "\n",
    "# --- Preprocesamiento y enriquecimiento ---\n",
    "def preprocess_and_enrich_test_data(df):\n",
    "    print(\"\\n--- Preprocesando y enriqueciendo datos de prueba ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    session_length = df.groupby('session_id')['partnumber'].transform('count')\n",
    "    df['session_length'] = session_length\n",
    "    df['country_popularity'] = df.groupby('country')['partnumber'].transform('count') / session_length\n",
    "    df['product_interaction_rate'] = df.groupby('partnumber')['session_id'].transform('nunique') / session_length\n",
    "    return df\n",
    "\n",
    "# --- Generar JSON ---\n",
    "def generate_predictions(model_path, test_path, output_path):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_and_enrich_test_data(test_df)\n",
    "\n",
    "    session_ids = test_df['session_id'].unique()\n",
    "    predictions = {}\n",
    "    popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "    print(\"\\n--- Generando predicciones ---\")\n",
    "    for session_id in session_ids:\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[X.columns.tolist()]\n",
    "        session_data.loc[:, 'score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # Eliminar repeticiones antes de completar\n",
    "        recommended_products = list(dict.fromkeys(recommended_products))\n",
    "\n",
    "        # Completar con productos populares si faltan\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "    print(f\"Predicciones guardadas en {output_path}\")\n",
    "\n",
    "\n",
    "# --- Ejecutar predicciones ---\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model.txt'\n",
    "    test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "    output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched_v2.json'\n",
    "\n",
    "    # Generar JSON\n",
    "    generate_predictions(model_path, test_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 3: 26% completada\n",
    "\n",
    "236/900 puntos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validando archivo JSON: /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched_v4.json ---\n",
      "Total de sesiones: 7349\n",
      "Sesiones con recomendaciones repetidas: 0\n",
      "Sesiones con menos de 5 recomendaciones: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_sessions': 7349,\n",
       " 'sessions_with_repeated_recommendations': 0,\n",
       " 'sessions_with_less_than_5_recommendations': 0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Ruta del JSON a validar\n",
    "json_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched_test.json'\n",
    "\n",
    "def validate_json(file_path):\n",
    "    print(f\"\\n--- Validando archivo JSON: {file_path} ---\")\n",
    "\n",
    "    # Cargar el JSON\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    recommendations = data.get(\"target\", {})\n",
    "\n",
    "    # Inicializar contadores\n",
    "    total_sessions = len(recommendations)\n",
    "    sessions_with_repeated_recommendations = 0\n",
    "    sessions_with_less_than_5_recommendations = 0\n",
    "\n",
    "    # Verificar cada sesión\n",
    "    for session_id, products in recommendations.items():\n",
    "        # Verificar si hay recomendaciones repetidas\n",
    "        if len(products) != len(set(products)):\n",
    "            sessions_with_repeated_recommendations += 1\n",
    "\n",
    "        # Verificar si tiene menos de 5 recomendaciones\n",
    "        if len(products) < 5:\n",
    "            sessions_with_less_than_5_recommendations += 1\n",
    "\n",
    "    # Imprimir resultados\n",
    "    print(f\"Total de sesiones: {total_sessions}\")\n",
    "    print(f\"Sesiones con recomendaciones repetidas: {sessions_with_repeated_recommendations}\")\n",
    "    print(f\"Sesiones con menos de 5 recomendaciones: {sessions_with_less_than_5_recommendations}\")\n",
    "\n",
    "    return {\n",
    "        \"total_sessions\": total_sessions,\n",
    "        \"sessions_with_repeated_recommendations\": sessions_with_repeated_recommendations,\n",
    "        \"sessions_with_less_than_5_recommendations\": sessions_with_less_than_5_recommendations,\n",
    "    }\n",
    "    \n",
    "\n",
    "# Validar JSON\n",
    "validate_json(json_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocesando datos ---\n",
      "\n",
      "--- Enriqueciendo características ---\n",
      "\n",
      "--- Calculado scale_pos_weight: 15.95 ---\n",
      "\n",
      "--- Dividiendo datos ---\n",
      "\n",
      "--- Configurando y entrenando el modelo LambdaMART ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.843844\ttrain's ndcg@3: 0.858705\ttrain's ndcg@5: 0.869633\tval's ndcg@1: 0.843356\tval's ndcg@3: 0.858004\tval's ndcg@5: 0.869006\n",
      "[20]\ttrain's ndcg@1: 0.8438\ttrain's ndcg@3: 0.858696\ttrain's ndcg@5: 0.869646\tval's ndcg@1: 0.843328\tval's ndcg@3: 0.85796\tval's ndcg@5: 0.869004\n",
      "[30]\ttrain's ndcg@1: 0.843757\ttrain's ndcg@3: 0.858743\ttrain's ndcg@5: 0.869626\tval's ndcg@1: 0.843098\tval's ndcg@3: 0.857892\tval's ndcg@5: 0.868939\n",
      "[40]\ttrain's ndcg@1: 0.843857\ttrain's ndcg@3: 0.858853\ttrain's ndcg@5: 0.86979\tval's ndcg@1: 0.842889\tval's ndcg@3: 0.857743\tval's ndcg@5: 0.868811\n",
      "[50]\ttrain's ndcg@1: 0.843939\ttrain's ndcg@3: 0.858961\ttrain's ndcg@5: 0.869852\tval's ndcg@1: 0.842683\tval's ndcg@3: 0.857623\tval's ndcg@5: 0.86867\n",
      "[60]\ttrain's ndcg@1: 0.844067\ttrain's ndcg@3: 0.859053\ttrain's ndcg@5: 0.869982\tval's ndcg@1: 0.842957\tval's ndcg@3: 0.857744\tval's ndcg@5: 0.868806\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttrain's ndcg@1: 0.843844\ttrain's ndcg@3: 0.858705\ttrain's ndcg@5: 0.869633\tval's ndcg@1: 0.843356\tval's ndcg@3: 0.858004\tval's ndcg@5: 0.869006\n",
      "\n",
      "--- Guardando el modelo ---\n",
      "\n",
      "--- Modelo guardado en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model_v2.txt ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# --- Preprocesamiento de datos ---\n",
    "def preprocess_data(df):\n",
    "    print(\"\\n--- Preprocesando datos ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    return df\n",
    "\n",
    "def enrich_features(df):\n",
    "    print(\"\\n--- Enriqueciendo características ---\")\n",
    "    session_length = df.groupby('session_id')['partnumber'].transform('count')\n",
    "    df['session_length'] = session_length\n",
    "    df['country_popularity'] = df.groupby('country')['partnumber'].transform('count') / session_length\n",
    "    df['product_interaction_rate'] = df.groupby('partnumber')['session_id'].transform('nunique') / session_length\n",
    "    return df\n",
    "\n",
    "# Cargar y enriquecer datos\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data.pkl')\n",
    "train_df = preprocess_data(train_df)\n",
    "train_df = enrich_features(train_df)\n",
    "\n",
    "# Separar características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Calcular scale_pos_weight\n",
    "pos_weight = len(y[y == 0]) / len(y[y == 1])\n",
    "print(f\"\\n--- Calculado scale_pos_weight: {pos_weight:.2f} ---\")\n",
    "\n",
    "# Dividir datos\n",
    "print(\"\\n--- Dividiendo datos ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "groups_train = groups.iloc[train_idx].value_counts().values\n",
    "groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "# Configurar y entrenar modelo\n",
    "print(\"\\n--- Configurando y entrenando el modelo LambdaMART ---\")\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 50,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu',\n",
    "    'scale_pos_weight': pos_weight\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping(stopping_rounds=50, verbose=True),\n",
    "    log_evaluation(period=10)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=200,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Guardar modelo\n",
    "print(\"\\n--- Guardando el modelo ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model_v2.txt'\n",
    "model.save_model(model_path)\n",
    "print(f\"\\n--- Modelo guardado en {model_path} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando datos de prueba ---\n",
      "\n",
      "--- Preprocesando y enriqueciendo datos de prueba ---\n",
      "\n",
      "--- Generando predicciones ---\n",
      "\n",
      "--- Guardando predicciones ---\n",
      "Predicciones guardadas en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched_v4.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "\n",
    "# --- Preprocesamiento y enriquecimiento ---\n",
    "def preprocess_and_enrich_test_data(df):\n",
    "    print(\"\\n--- Preprocesando y enriqueciendo datos de prueba ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    session_length = df.groupby('session_id')['partnumber'].transform('count')\n",
    "    df['session_length'] = session_length\n",
    "    df['country_popularity'] = df.groupby('country')['partnumber'].transform('count') / session_length\n",
    "    df['product_interaction_rate'] = df.groupby('partnumber')['session_id'].transform('nunique') / session_length\n",
    "    return df\n",
    "\n",
    "# --- Generar JSON ---\n",
    "def generate_predictions(model_path, test_path, output_path):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_and_enrich_test_data(test_df)\n",
    "\n",
    "    session_ids = test_df['session_id'].unique()\n",
    "    predictions = {}\n",
    "    popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "    print(\"\\n--- Generando predicciones ---\")\n",
    "    for session_id in session_ids:\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[X.columns.tolist()]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # Eliminar duplicados mientras se conserva el orden\n",
    "        recommended_products = list(dict.fromkeys(recommended_products))\n",
    "\n",
    "        # Completar con productos populares para asegurar 5 elementos únicos\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        # Verificar que las recomendaciones tengan exactamente 5 valores únicos\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "    print(f\"Predicciones guardadas en {output_path}\")\n",
    "\n",
    "\n",
    "# --- Ejecutar predicciones ---\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model_v2.txt'\n",
    "    test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "    output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched_v4.json'\n",
    "\n",
    "    # Generar JSON\n",
    "    generate_predictions(model_path, test_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocesando y alineando datos ---\n",
      "\n",
      "--- Enriqueciendo características ---\n",
      "\n",
      "--- Calculado scale_pos_weight: 15.95 ---\n",
      "\n",
      "--- Dividiendo datos ---\n",
      "\n",
      "--- Configurando y entrenando el modelo LambdaMART ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.843844\ttrain's ndcg@3: 0.858705\ttrain's ndcg@5: 0.869633\ttrain's auc: 0.663763\ttrain's map@1: 0.843844\ttrain's map@3: 0.847475\ttrain's map@5: 0.852684\tval's ndcg@1: 0.843356\tval's ndcg@3: 0.858004\tval's ndcg@5: 0.869006\tval's auc: 0.6637\tval's map@1: 0.843356\tval's map@3: 0.846688\tval's map@5: 0.851942\n",
      "[20]\ttrain's ndcg@1: 0.8438\ttrain's ndcg@3: 0.858696\ttrain's ndcg@5: 0.869646\ttrain's auc: 0.660903\ttrain's map@1: 0.8438\ttrain's map@3: 0.847472\ttrain's map@5: 0.852692\tval's ndcg@1: 0.843328\tval's ndcg@3: 0.85796\tval's ndcg@5: 0.869004\tval's auc: 0.661137\tval's map@1: 0.843328\tval's map@3: 0.84665\tval's map@5: 0.851911\n",
      "[30]\ttrain's ndcg@1: 0.843757\ttrain's ndcg@3: 0.858743\ttrain's ndcg@5: 0.869626\ttrain's auc: 0.662825\ttrain's map@1: 0.843757\ttrain's map@3: 0.847484\ttrain's map@5: 0.852655\tval's ndcg@1: 0.843098\tval's ndcg@3: 0.857892\tval's ndcg@5: 0.868939\tval's auc: 0.662849\tval's map@1: 0.843098\tval's map@3: 0.846579\tval's map@5: 0.851835\n",
      "[40]\ttrain's ndcg@1: 0.843857\ttrain's ndcg@3: 0.858853\ttrain's ndcg@5: 0.86979\ttrain's auc: 0.665657\ttrain's map@1: 0.843857\ttrain's map@3: 0.847586\ttrain's map@5: 0.852798\tval's ndcg@1: 0.842889\tval's ndcg@3: 0.857743\tval's ndcg@5: 0.868811\tval's auc: 0.665319\tval's map@1: 0.842889\tval's map@3: 0.846449\tval's map@5: 0.851707\n",
      "[50]\ttrain's ndcg@1: 0.843939\ttrain's ndcg@3: 0.858961\ttrain's ndcg@5: 0.869852\ttrain's auc: 0.667708\ttrain's map@1: 0.843939\ttrain's map@3: 0.847684\ttrain's map@5: 0.852862\tval's ndcg@1: 0.842683\tval's ndcg@3: 0.857623\tval's ndcg@5: 0.86867\tval's auc: 0.667215\tval's map@1: 0.842683\tval's map@3: 0.84628\tval's map@5: 0.851556\n",
      "[60]\ttrain's ndcg@1: 0.844067\ttrain's ndcg@3: 0.859053\ttrain's ndcg@5: 0.869982\ttrain's auc: 0.669026\ttrain's map@1: 0.844067\ttrain's map@3: 0.847759\ttrain's map@5: 0.852968\tval's ndcg@1: 0.842957\tval's ndcg@3: 0.857744\tval's ndcg@5: 0.868806\tval's auc: 0.668507\tval's map@1: 0.842957\tval's map@3: 0.846393\tval's map@5: 0.851679\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttrain's ndcg@1: 0.843844\ttrain's ndcg@3: 0.858705\ttrain's ndcg@5: 0.869633\ttrain's auc: 0.663763\ttrain's map@1: 0.843844\ttrain's map@3: 0.847475\ttrain's map@5: 0.852684\tval's ndcg@1: 0.843356\tval's ndcg@3: 0.858004\tval's ndcg@5: 0.869006\tval's auc: 0.6637\tval's map@1: 0.843356\tval's map@3: 0.846688\tval's map@5: 0.851942\n",
      "\n",
      "--- Guardando el modelo ---\n",
      "\n",
      "--- Modelo guardado en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_aligned_model.txt ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# --- Preprocesamiento y enriquecimiento ---\n",
    "def preprocess_and_align_data(df):\n",
    "    print(\"\\n--- Preprocesando y alineando datos ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    return df\n",
    "\n",
    "def enrich_features(df):\n",
    "    print(\"\\n--- Enriqueciendo características ---\")\n",
    "    session_length = df.groupby('session_id')['partnumber'].transform('count')\n",
    "    df['session_length'] = session_length\n",
    "    df['country_popularity'] = df.groupby('country')['partnumber'].transform('count') / session_length\n",
    "    df['product_interaction_rate'] = df.groupby('partnumber')['session_id'].transform('nunique') / session_length\n",
    "    return df\n",
    "\n",
    "# --- Cargar y preprocesar datos ---\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data.pkl')\n",
    "train_df = preprocess_and_align_data(train_df)\n",
    "train_df = enrich_features(train_df)\n",
    "\n",
    "# Filtrar características disponibles en test_df\n",
    "features_to_use = ['date', 'timestamp_local', 'hour', 'day_of_week', \n",
    "                   'session_length', 'country_popularity', 'product_interaction_rate',\n",
    "                   'user_id', 'country', 'partnumber', 'device_type', 'pagetype']\n",
    "train_df = train_df[features_to_use + ['add_to_cart', 'session_id']]\n",
    "\n",
    "# Separar características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Calcular scale_pos_weight\n",
    "pos_weight = len(y[y == 0]) / len(y[y == 1])\n",
    "print(f\"\\n--- Calculado scale_pos_weight: {pos_weight:.2f} ---\")\n",
    "\n",
    "# Dividir datos\n",
    "print(\"\\n--- Dividiendo datos ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "groups_train = groups.iloc[train_idx].value_counts().values\n",
    "groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "# Configurar y entrenar modelo\n",
    "print(\"\\n--- Configurando y entrenando el modelo LambdaMART ---\")\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': ['ndcg', 'auc', 'map'],\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 50,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu',\n",
    "    'scale_pos_weight': pos_weight\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping(stopping_rounds=50, verbose=True),\n",
    "    log_evaluation(period=10)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=200,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Guardar modelo\n",
    "print(\"\\n--- Guardando el modelo ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_aligned_model.txt'\n",
    "model.save_model(model_path)\n",
    "print(f\"\\n--- Modelo guardado en {model_path} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando datos de prueba ---\n",
      "\n",
      "--- Preprocesando y alineando datos de prueba ---\n",
      "\n",
      "--- Generando predicciones ---\n",
      "\n",
      "--- Guardando predicciones ---\n",
      "Predicciones guardadas en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_aligned.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "\n",
    "# --- Preprocesamiento y enriquecimiento ---\n",
    "def preprocess_and_align_test_data(df):\n",
    "    print(\"\\n--- Preprocesando y alineando datos de prueba ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    df['session_length'] = df.groupby('session_id')['partnumber'].transform('count')\n",
    "    df['country_popularity'] = df.groupby('country')['partnumber'].transform('count') / df['session_length']\n",
    "    df['product_interaction_rate'] = df.groupby('partnumber')['session_id'].transform('nunique') / df['session_length']\n",
    "    return df\n",
    "\n",
    "# --- Generar JSON ---\n",
    "def generate_predictions(model_path, test_path, output_path):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "    # Características alineadas al modelo entrenado\n",
    "    aligned_features = [\n",
    "        'date', 'timestamp_local', 'hour', 'day_of_week', \n",
    "        'session_length', 'country_popularity', 'product_interaction_rate',\n",
    "        'user_id', 'country', 'partnumber', 'device_type', 'pagetype'\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_and_align_test_data(test_df)\n",
    "\n",
    "    # Asegurar que las columnas coincidan con las características del modelo\n",
    "    missing_features = set(aligned_features) - set(test_df.columns)\n",
    "    if missing_features:\n",
    "        print(f\"Advertencia: Faltan las siguientes características en los datos de prueba: {missing_features}\")\n",
    "        for feature in missing_features:\n",
    "            test_df[feature] = 0  # Rellenar las columnas faltantes con valores predeterminados\n",
    "\n",
    "    session_ids = test_df['session_id'].unique()\n",
    "    predictions = {}\n",
    "    popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "    print(\"\\n--- Generando predicciones ---\")\n",
    "    for session_id in session_ids:\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[aligned_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # Eliminar repeticiones antes de completar\n",
    "        recommended_products = list(dict.fromkeys(recommended_products))\n",
    "\n",
    "        # Completar con productos populares si faltan\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "    print(f\"Predicciones guardadas en {output_path}\")\n",
    "\n",
    "# --- Ejecutar predicciones ---\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_aligned_model.txt'\n",
    "    test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "    output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_aligned.json'\n",
    "\n",
    "    # Generar JSON\n",
    "    generate_predictions(model_path, test_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Realizando ajuste de hiperparámetros ---\n",
      "\n",
      "--- Probando conjunto de parámetros 1/50 ---\n",
      "{'num_leaves': 62, 'learning_rate': 0.10300419603594589, 'min_data_in_leaf': 47, 'max_bin': 129, 'subsample': 0.6124099504493548, 'colsample_bytree': 0.7483568265898739}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 117\u001b[0m\n\u001b[1;32m    107\u001b[0m param_distributions \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m100\u001b[39m),\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: uniform(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: uniform(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m    114\u001b[0m }\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Suponiendo que ya tienes X, y, groups definidos\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m best_params, best_score, best_model \u001b[38;5;241m=\u001b[39m \u001b[43mmanual_hyperparameter_tuning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m    121\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Mejores parámetros ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_params)\n",
      "Cell \u001b[0;32mIn[5], line 71\u001b[0m, in \u001b[0;36mmanual_hyperparameter_tuning\u001b[0;34m(X, y, groups, param_distributions, n_iter)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Dividir los datos en entrenamiento y validación respetando grupos\u001b[39;00m\n\u001b[1;32m     70\u001b[0m gss \u001b[38;5;241m=\u001b[39m GroupShuffleSplit(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m train_idx, val_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m X_train, X_val \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[train_idx], X\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[1;32m     74\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_idx], y\u001b[38;5;241m.\u001b[39miloc[val_idx]\n",
      "File \u001b[0;32m~/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/sklearn/model_selection/_split.py:1909\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1879\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1880\u001b[0m \n\u001b[1;32m   1881\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1906\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[1;32m   1907\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1909\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1910\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2163\u001b[0m, in \u001b[0;36mGroupShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter should not be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2162\u001b[0m groups \u001b[38;5;241m=\u001b[39m check_array(groups, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m\"\u001b[39m, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2163\u001b[0m classes, group_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group_train, group_test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_iter_indices(X\u001b[38;5;241m=\u001b[39mclasses):\n\u001b[1;32m   2165\u001b[0m     \u001b[38;5;66;03m# these are the indices of classes in the partition\u001b[39;00m\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;66;03m# invert them into data indices\u001b[39;00m\n\u001b[1;32m   2168\u001b[0m     train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflatnonzero(np\u001b[38;5;241m.\u001b[39misin(group_indices, group_train))\n",
      "File \u001b[0;32m~/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/numpy/lib/arraysetops.py:362\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    360\u001b[0m     inv_idx[perm] \u001b[38;5;241m=\u001b[39m imask\n\u001b[1;32m    361\u001b[0m     ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (inv_idx,)\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_counts:\n\u001b[1;32m    363\u001b[0m     idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(np\u001b[38;5;241m.\u001b[39mnonzero(mask) \u001b[38;5;241m+\u001b[39m ([mask\u001b[38;5;241m.\u001b[39msize],))\n\u001b[1;32m    364\u001b[0m     ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mdiff(idx),)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hiperparámetros.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from scipy.stats import uniform, randint\n",
    "from itertools import product\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# --- Preprocesamiento y enriquecimiento ---\n",
    "def preprocess_and_align_data(df):\n",
    "    print(\"\\n--- Preprocesando y alineando datos ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    session_length = df.groupby('session_id')['partnumber'].transform('count')\n",
    "    df['session_length'] = session_length\n",
    "    df['country_popularity'] = df.groupby('country')['partnumber'].transform('count') / session_length\n",
    "    df['product_interaction_rate'] = df.groupby('partnumber')['session_id'].transform('nunique') / session_length\n",
    "    return df\n",
    "\n",
    "# --- Función para entrenar y validar el modelo ---\n",
    "def train_and_evaluate(params, X_train, y_train, groups_train, X_val, y_val, groups_val):\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train', 'val'],\n",
    "        num_boost_round=200,\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=50, verbose=False),\n",
    "            log_evaluation(period=10)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    return mse, model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from itertools import product\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Función de ajuste manual de hiperparámetros\n",
    "def manual_hyperparameter_tuning(X, y, groups, param_distributions, n_iter=50):\n",
    "    print(\"\\n--- Realizando ajuste de hiperparámetros ---\")\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    # Generar combinaciones de parámetros\n",
    "    param_grid = [\n",
    "        {key: dist.rvs() for key, dist in param_distributions.items()}\n",
    "        for _ in range(n_iter)\n",
    "    ]\n",
    "\n",
    "    for i, params in enumerate(param_grid):\n",
    "        print(f\"\\n--- Probando conjunto de parámetros {i + 1}/{n_iter} ---\")\n",
    "        print(params)\n",
    "        \n",
    "        # Dividir los datos en entrenamiento y validación respetando grupos\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "        train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        groups_train = groups.iloc[train_idx].value_counts().values\n",
    "        groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "        # Crear conjuntos de datos de LightGBM\n",
    "        lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "        lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "        # Configurar y entrenar el modelo\n",
    "        model = lgb.train(\n",
    "            {**params, 'objective': 'lambdarank', 'metric': 'ndcg', 'ndcg_eval_at': [1, 3, 5], 'verbose': -1},\n",
    "            lgb_train,\n",
    "            valid_sets=[lgb_train, lgb_val],\n",
    "            valid_names=['train', 'val'],\n",
    "            num_boost_round=200,\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50, verbose=True),\n",
    "                lgb.log_evaluation(period=10)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Obtener el mejor puntaje\n",
    "        score = model.best_score['val']['ndcg@5']\n",
    "        print(f\"--- Puntaje actual: {score} ---\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "    return best_params, best_score, best_model\n",
    "\n",
    "# Ejemplo de ejecución\n",
    "param_distributions = {\n",
    "    'num_leaves': randint(20, 100),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'min_data_in_leaf': randint(10, 50),\n",
    "    'max_bin': randint(50, 300),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "}\n",
    "\n",
    "# Suponiendo que ya tienes X, y, groups definidos\n",
    "best_params, best_score, best_model = manual_hyperparameter_tuning(\n",
    "    X, y, groups,\n",
    "    param_distributions,\n",
    "    n_iter=50\n",
    ")\n",
    "\n",
    "print(\"\\n--- Mejores parámetros ---\")\n",
    "print(best_params)\n",
    "print(\"\\n--- Mejor puntaje ---\")\n",
    "print(best_score)\n",
    "\n",
    "# Guardar el modelo final\n",
    "print(\"\\n--- Guardando el mejor modelo ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_tuned_model.txt'\n",
    "best_model.save_model(model_path)\n",
    "print(f\"Modelo guardado en {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocesando datos ---\n",
      "\n",
      "--- Enriqueciendo características ---\n",
      "\n",
      "--- Calculado scale_pos_weight: 15.95 ---\n",
      "\n",
      "--- Dividiendo datos ---\n",
      "\n",
      "--- Configurando y entrenando el modelo LambdaMART ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.84378\ttrain's ndcg@3: 0.858809\ttrain's ndcg@5: 0.869743\tval's ndcg@1: 0.842981\tval's ndcg@3: 0.857758\tval's ndcg@5: 0.868794\n",
      "[20]\ttrain's ndcg@1: 0.844208\ttrain's ndcg@3: 0.859226\ttrain's ndcg@5: 0.87023\tval's ndcg@1: 0.843053\tval's ndcg@3: 0.858036\tval's ndcg@5: 0.869007\n",
      "[30]\ttrain's ndcg@1: 0.844918\ttrain's ndcg@3: 0.860025\ttrain's ndcg@5: 0.870956\tval's ndcg@1: 0.843237\tval's ndcg@3: 0.858457\tval's ndcg@5: 0.869469\n",
      "[40]\ttrain's ndcg@1: 0.845783\ttrain's ndcg@3: 0.860867\ttrain's ndcg@5: 0.871765\tval's ndcg@1: 0.843602\tval's ndcg@3: 0.858688\tval's ndcg@5: 0.869788\n",
      "[50]\ttrain's ndcg@1: 0.846616\ttrain's ndcg@3: 0.861667\ttrain's ndcg@5: 0.872528\tval's ndcg@1: 0.843738\tval's ndcg@3: 0.85894\tval's ndcg@5: 0.870031\n",
      "[60]\ttrain's ndcg@1: 0.847073\ttrain's ndcg@3: 0.862147\ttrain's ndcg@5: 0.872961\tval's ndcg@1: 0.843907\tval's ndcg@3: 0.859066\tval's ndcg@5: 0.870139\n",
      "[70]\ttrain's ndcg@1: 0.847535\ttrain's ndcg@3: 0.862541\ttrain's ndcg@5: 0.87337\tval's ndcg@1: 0.844066\tval's ndcg@3: 0.859057\tval's ndcg@5: 0.870228\n",
      "[80]\ttrain's ndcg@1: 0.847936\ttrain's ndcg@3: 0.862913\ttrain's ndcg@5: 0.873688\tval's ndcg@1: 0.843909\tval's ndcg@3: 0.859047\tval's ndcg@5: 0.870189\n",
      "[90]\ttrain's ndcg@1: 0.848236\ttrain's ndcg@3: 0.86322\ttrain's ndcg@5: 0.873985\tval's ndcg@1: 0.844011\tval's ndcg@3: 0.859091\tval's ndcg@5: 0.870212\n",
      "[100]\ttrain's ndcg@1: 0.848594\ttrain's ndcg@3: 0.863537\ttrain's ndcg@5: 0.874306\tval's ndcg@1: 0.844069\tval's ndcg@3: 0.85911\tval's ndcg@5: 0.870225\n",
      "[110]\ttrain's ndcg@1: 0.848827\ttrain's ndcg@3: 0.86379\ttrain's ndcg@5: 0.87455\tval's ndcg@1: 0.844033\tval's ndcg@3: 0.859157\tval's ndcg@5: 0.870261\n",
      "[120]\ttrain's ndcg@1: 0.849129\ttrain's ndcg@3: 0.864052\ttrain's ndcg@5: 0.874813\tval's ndcg@1: 0.844003\tval's ndcg@3: 0.859146\tval's ndcg@5: 0.870228\n",
      "[130]\ttrain's ndcg@1: 0.849425\ttrain's ndcg@3: 0.864332\ttrain's ndcg@5: 0.875058\tval's ndcg@1: 0.84401\tval's ndcg@3: 0.859128\tval's ndcg@5: 0.870245\n",
      "[140]\ttrain's ndcg@1: 0.849633\ttrain's ndcg@3: 0.864499\ttrain's ndcg@5: 0.87521\tval's ndcg@1: 0.843957\tval's ndcg@3: 0.859118\tval's ndcg@5: 0.870214\n",
      "[150]\ttrain's ndcg@1: 0.849916\ttrain's ndcg@3: 0.864705\ttrain's ndcg@5: 0.875417\tval's ndcg@1: 0.844011\tval's ndcg@3: 0.85916\tval's ndcg@5: 0.870252\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttrain's ndcg@1: 0.848648\ttrain's ndcg@3: 0.863591\ttrain's ndcg@5: 0.87436\tval's ndcg@1: 0.844087\tval's ndcg@3: 0.859122\tval's ndcg@5: 0.870248\n",
      "\n",
      "--- Guardando el modelo ---\n",
      "\n",
      "--- Modelo guardado en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_hyper_model.txt ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# --- Preprocesamiento de datos ---\n",
    "def preprocess_data(df):\n",
    "    print(\"\\n--- Preprocesando datos ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    return df\n",
    "\n",
    "def enrich_features(df):\n",
    "    print(\"\\n--- Enriqueciendo características ---\")\n",
    "    session_length = df.groupby('session_id')['partnumber'].transform('count')\n",
    "    df['session_length'] = session_length\n",
    "    df['country_popularity'] = df.groupby('country')['partnumber'].transform('count') / session_length\n",
    "    df['product_interaction_rate'] = df.groupby('partnumber')['session_id'].transform('nunique') / session_length\n",
    "    return df\n",
    "\n",
    "# Cargar y enriquecer datos\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data.pkl')\n",
    "train_df = preprocess_data(train_df)\n",
    "train_df = enrich_features(train_df)\n",
    "\n",
    "# Separar características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Calcular scale_pos_weight\n",
    "pos_weight = len(y[y == 0]) / len(y[y == 1])\n",
    "print(f\"\\n--- Calculado scale_pos_weight: {pos_weight:.2f} ---\")\n",
    "\n",
    "# Dividir datos\n",
    "print(\"\\n--- Dividiendo datos ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "groups_train = groups.iloc[train_idx].value_counts().values\n",
    "groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "# Configurar y entrenar modelo\n",
    "print(\"\\n--- Configurando y entrenando el modelo LambdaMART ---\")\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.1022,\n",
    "    'num_leaves': 74,\n",
    "    'min_data_in_leaf': 12,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'max_bin': 255,\n",
    "    'subsample': 0.783,\n",
    "    'colsample_bytree': 0.726,\n",
    "    'device': 'cpu',\n",
    "    'scale_pos_weight': pos_weight\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping(stopping_rounds=50, verbose=True),\n",
    "    log_evaluation(period=10)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=200,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Guardar modelo\n",
    "print(\"\\n--- Guardando el modelo ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_hyper_model.txt'\n",
    "model.save_model(model_path)\n",
    "print(f\"\\n--- Modelo guardado en {model_path} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando datos de prueba ---\n",
      "\n",
      "--- Preprocesando y enriqueciendo datos de prueba ---\n",
      "\n",
      "--- Generando predicciones ---\n",
      "\n",
      "--- Guardando predicciones ---\n",
      "Predicciones guardadas en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_hyper_v1.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "\n",
    "# --- Preprocesamiento y enriquecimiento ---\n",
    "def preprocess_and_enrich_test_data(df):\n",
    "    print(\"\\n--- Preprocesando y enriqueciendo datos de prueba ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    session_length = df.groupby('session_id')['partnumber'].transform('count')\n",
    "    df['session_length'] = session_length\n",
    "    df['country_popularity'] = df.groupby('country')['partnumber'].transform('count') / session_length\n",
    "    df['product_interaction_rate'] = df.groupby('partnumber')['session_id'].transform('nunique') / session_length\n",
    "    return df\n",
    "\n",
    "# --- Generar JSON ---\n",
    "def generate_predictions(model_path, test_path, output_path):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_and_enrich_test_data(test_df)\n",
    "\n",
    "    session_ids = test_df['session_id'].unique()\n",
    "    predictions = {}\n",
    "    popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "    print(\"\\n--- Generando predicciones ---\")\n",
    "    for session_id in session_ids:\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[X.columns.tolist()]\n",
    "        session_data.loc[:, 'score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # Eliminar repeticiones antes de completar\n",
    "        recommended_products = list(dict.fromkeys(recommended_products))\n",
    "\n",
    "        # Completar con productos populares si faltan\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "    print(f\"Predicciones guardadas en {output_path}\")\n",
    "\n",
    "\n",
    "# --- Ejecutar predicciones ---\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_hyper_model.txt'\n",
    "    test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "    output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_hyper_v1.json'\n",
    "\n",
    "    # Generar JSON\n",
    "    generate_predictions(model_path, test_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
