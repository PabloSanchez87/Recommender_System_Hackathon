{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelo NCF (Neural Collaborative Filtering)**\n",
    "\n",
    "### **Pasos de Implementación**\n",
    "1. **Preprocesamiento de Datos**\n",
    "   - Submuestreo de interacciones donde `add_to_cart = 0`.\n",
    "   - Transformación logarítmica para variables altamente desbalanceadas como `popularity` y número de interacciones por usuario/producto.\n",
    "\n",
    "2. **Estrategias para Datos Desbalanceados**\n",
    "   - Incorporar pesos en la función de pérdida para dar mayor importancia a `add_to_cart = 1`.\n",
    "   - Utilizar técnicas de oversampling para la clase minoritaria.\n",
    "\n",
    "3. **Adaptación del Modelo**\n",
    "   - Crear un modelo híbrido que:\n",
    "     - Use embeddings de productos (`similar_products`, `embedding`) para manejar productos nuevos o poco populares.\n",
    "     - Agregue datos demográficos (como `country`) como entradas adicionales.\n",
    "     - Incluya una capa de clasificación para predecir la probabilidad de `add_to_cart`.\n",
    "\n",
    "4. **Pipeline para Usuarios Nuevos**\n",
    "   - Usar un enfoque basado en contenido para usuarios sin historial (`user_id = -1`).\n",
    "   - Crear recomendaciones basadas en popularidad y similitud de producto.\n",
    "\n",
    "5. **Evaluación y Métricas**\n",
    "   - Asegurar que las métricas utilizadas reflejen el desbalance de clases, como:\n",
    "     - NDCG.\n",
    "     - F1-Score ajustado para la clase `add_to_cart = 1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# --- Cargar datos ---\n",
    "train_df = pd.read_pickle(\"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_enriched.pkl\")\n",
    "test_df = pd.read_pickle(\"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl\")\n",
    "\n",
    "# --- Preprocesamiento ---\n",
    "# 1. Submuestreo\n",
    "add_to_cart_0 = train_df[train_df['add_to_cart'] == 0]\n",
    "add_to_cart_1 = train_df[train_df['add_to_cart'] == 1]\n",
    "subsampled_0 = add_to_cart_0.sample(n=len(add_to_cart_1) * 2, random_state=42)\n",
    "train_balanced = pd.concat([subsampled_0, add_to_cart_1]).sample(frac=1, random_state=42)\n",
    "\n",
    "# 2. Transformación logarítmica y normalización\n",
    "for col in ['popularity', 'session_interactions']:\n",
    "    train_balanced[col] = np.log1p(train_balanced[col])\n",
    "    train_balanced[col] = (train_balanced[col] - train_balanced[col].mean()) / train_balanced[col].std()\n",
    "\n",
    "# 3. Codificación de datos categóricos\n",
    "train_balanced['country'] = train_balanced['country'].astype('category').cat.codes\n",
    "train_balanced['pagetype'] = train_balanced['pagetype'].astype('category').cat.codes\n",
    "\n",
    "# 4. Reasignar índices\n",
    "user_mapping = {id_: idx for idx, id_ in enumerate(train_balanced['user_id'].unique())}\n",
    "item_mapping = {id_: idx for idx, id_ in enumerate(train_balanced['partnumber'].unique())}\n",
    "train_balanced['user_id'] = train_balanced['user_id'].map(user_mapping).fillna(len(user_mapping)).astype(int)\n",
    "train_balanced['partnumber'] = train_balanced['partnumber'].map(item_mapping).fillna(len(item_mapping)).astype(int)\n",
    "\n",
    "# --- Modelo ---\n",
    "num_users = train_balanced['user_id'].nunique() + 1\n",
    "num_items = train_balanced['partnumber'].nunique() + 1\n",
    "num_countries = train_balanced['country'].nunique() + 1\n",
    "num_pagetype = train_balanced['pagetype'].nunique() + 1\n",
    "\n",
    "# Entradas\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "country_input = Input(shape=(1,), name='country_input')\n",
    "pagetype_input = Input(shape=(1,), name='pagetype_input')\n",
    "\n",
    "# Embeddings con regularización L2\n",
    "user_embedding = Embedding(num_users, 64, name='user_embedding', embeddings_regularizer=l2(1e-4))(user_input)\n",
    "item_embedding = Embedding(num_items, 64, name='item_embedding', embeddings_regularizer=l2(1e-4))(item_input)\n",
    "country_embedding = Embedding(num_countries, 16, name='country_embedding', embeddings_regularizer=l2(1e-4))(country_input)\n",
    "pagetype_embedding = Embedding(num_pagetype, 16, name='pagetype_embedding', embeddings_regularizer=l2(1e-4))(pagetype_input)\n",
    "\n",
    "# Flatten layers\n",
    "user_flatten = Flatten()(user_embedding)\n",
    "item_flatten = Flatten()(item_embedding)\n",
    "country_flatten = Flatten()(country_embedding)\n",
    "pagetype_flatten = Flatten()(pagetype_embedding)\n",
    "\n",
    "# Concatenación\n",
    "concat = Concatenate()([user_flatten, item_flatten, country_flatten, pagetype_flatten])\n",
    "\n",
    "# Capas densas con Dropout y regularización L2\n",
    "fc1 = Dense(128, activation='relu', kernel_regularizer=l2(1e-4))(concat)\n",
    "fc1 = Dropout(0.4)(fc1)\n",
    "fc2 = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(fc1)\n",
    "fc2 = Dropout(0.4)(fc2)\n",
    "output = Dense(1, activation='sigmoid', name='output')(fc2)\n",
    "\n",
    "model = Model(inputs=[user_input, item_input, country_input, pagetype_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# --- Entrenamiento ---\n",
    "X = train_balanced[['user_id', 'partnumber', 'country', 'pagetype']]\n",
    "y = train_balanced['add_to_cart']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [\n",
    "        X_train['user_id'].values,\n",
    "        X_train['partnumber'].values,\n",
    "        X_train['country'].values,\n",
    "        X_train['pagetype'].values\n",
    "    ],\n",
    "    y_train.values,\n",
    "    validation_data=(\n",
    "        [\n",
    "            X_val['user_id'].values,\n",
    "            X_val['partnumber'].values,\n",
    "            X_val['country'].values,\n",
    "            X_val['pagetype'].values\n",
    "        ],\n",
    "        y_val.values\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# --- Evaluación ---\n",
    "eval_results = model.evaluate(\n",
    "    [\n",
    "        X_val['user_id'].values,\n",
    "        X_val['partnumber'].values,\n",
    "        X_val['country'].values,\n",
    "        X_val['pagetype'].values\n",
    "    ],\n",
    "    y_val.values\n",
    ")\n",
    "print(\"\\nResultados de evaluación:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo en el formato nativo de Keras (recomendado)\n",
    "model.save(\"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/recommender_model_v2.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustes Clave\n",
    "\n",
    "1. **Usar Embeddings**:\n",
    "   - Incorporar los `reduced_embedding` directamente en el modelo como entrada para capturar relaciones entre productos.\n",
    "\n",
    "2. **Incluir Variables Categóricas**:\n",
    "   - Añadir `color_id`, `family`, y `cod_section` como entradas adicionales.\n",
    "\n",
    "3. **Incluir Información de Popularidad y Descuento**:\n",
    "   - Usar `popularity` y `discount` como entradas normalizadas.\n",
    "\n",
    "4. **Procesar Similaridades entre Productos**:\n",
    "   - Usar `similar_products` para diversificar recomendaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficios del Ajuste\n",
    "\n",
    "1. **Embeddings de Producto**:\n",
    "   - Captura relaciones entre productos utilizando `reduced_embedding`.\n",
    "\n",
    "2. **Variables Adicionales**:\n",
    "   - `color_id`, `family`, y `cod_section` agregan contexto para recomendaciones.\n",
    "\n",
    "3. **Personalización Mejorada**:\n",
    "   - Variables como `popularity` y `discount` ayudan a ajustar recomendaciones basadas en características importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow CUDA Version:\", tf.sysconfig.get_build_info()['cuda_version'])\n",
    "print(\"TensorFlow cuDNN Version:\", tf.sysconfig.get_build_info()['cudnn_version'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow versión:\", tf.__version__)\n",
    "print(\"GPU detectada:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Prueba con una operación básica\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.random.normal([1000, 1000])\n",
    "    c = tf.matmul(a, b)\n",
    "print(\"Operación de prueba realizada en GPU con éxito.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout\n",
    "\n",
    "# --- Cargar el DataFrame enriquecido ---\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl')\n",
    "\n",
    "# --- Verificación y mapeo de índices ---\n",
    "user_mapping = {id_: idx for idx, id_ in enumerate(train_df['user_id'].unique())}\n",
    "item_mapping = {id_: idx for idx, id_ in enumerate(train_df['partnumber'].unique())}\n",
    "color_mapping = {id_: idx for idx, id_ in enumerate(train_df['color_id'].unique())}\n",
    "family_mapping = {id_: idx for idx, id_ in enumerate(train_df['family'].unique())}\n",
    "cod_section_mapping = {id_: idx for idx, id_ in enumerate(train_df['cod_section'].unique())}\n",
    "cluster_mapping = {id_: idx for idx, id_ in enumerate(train_df['cluster'].unique())}\n",
    "\n",
    "# Aplicar mapeo\n",
    "train_df['user_id'] = train_df['user_id'].map(user_mapping).fillna(0).astype(int)\n",
    "train_df['partnumber'] = train_df['partnumber'].map(item_mapping).fillna(0).astype(int)\n",
    "train_df['color_id'] = train_df['color_id'].map(color_mapping).fillna(0).astype(int)\n",
    "train_df['family'] = train_df['family'].map(family_mapping).fillna(0).astype(int)\n",
    "train_df['cod_section'] = train_df['cod_section'].map(cod_section_mapping).fillna(0).astype(int)\n",
    "train_df['cluster'] = train_df['cluster'].map(cluster_mapping).fillna(0).astype(int)\n",
    "\n",
    "# --- Verificar rangos y valores ---\n",
    "print(\"Verificando valores fuera de rango:\")\n",
    "print(f\"user_id max: {train_df['user_id'].max()} / {len(user_mapping)}\")\n",
    "print(f\"partnumber max: {train_df['partnumber'].max()} / {len(item_mapping)}\")\n",
    "print(f\"color_id max: {train_df['color_id'].max()} / {len(color_mapping)}\")\n",
    "print(f\"family max: {train_df['family'].max()} / {len(family_mapping)}\")\n",
    "print(f\"cod_section max: {train_df['cod_section'].max()} / {len(cod_section_mapping)}\")\n",
    "print(f\"cluster max: {train_df['cluster'].max()} / {len(cluster_mapping)}\")\n",
    "\n",
    "# --- Preparación de los datos ---\n",
    "X = train_df[['user_id', 'partnumber', 'color_id', 'family', 'cod_section', 'discount', 'popularity', 'session_interactions', 'cluster']]\n",
    "y = train_df['add_to_cart']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Construcción del modelo ---\n",
    "num_users = len(user_mapping) + 1\n",
    "num_items = len(item_mapping) + 1\n",
    "num_colors = len(color_mapping) + 1\n",
    "num_families = len(family_mapping) + 1\n",
    "num_cod_sections = len(cod_section_mapping) + 1\n",
    "num_clusters = len(cluster_mapping) + 1\n",
    "\n",
    "# Entradas\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "color_input = Input(shape=(1,), name='color_input')\n",
    "family_input = Input(shape=(1,), name='family_input')\n",
    "cod_section_input = Input(shape=(1,), name='cod_section_input')\n",
    "discount_input = Input(shape=(1,), name='discount_input')\n",
    "popularity_input = Input(shape=(1,), name='popularity_input')\n",
    "session_input = Input(shape=(1,), name='session_input')\n",
    "cluster_input = Input(shape=(1,), name='cluster_input')\n",
    "\n",
    "# Embeddings\n",
    "user_embedding = Embedding(num_users, 64)(user_input)\n",
    "item_embedding = Embedding(num_items, 64)(item_input)\n",
    "color_embedding = Embedding(num_colors, 16)(color_input)\n",
    "family_embedding = Embedding(num_families, 16)(family_input)\n",
    "cod_section_embedding = Embedding(num_cod_sections, 16)(cod_section_input)\n",
    "cluster_embedding = Embedding(num_clusters, 16)(cluster_input)\n",
    "\n",
    "# Flatten layers\n",
    "user_flatten = Flatten()(user_embedding)\n",
    "item_flatten = Flatten()(item_embedding)\n",
    "color_flatten = Flatten()(color_embedding)\n",
    "family_flatten = Flatten()(family_embedding)\n",
    "cod_section_flatten = Flatten()(cod_section_embedding)\n",
    "cluster_flatten = Flatten()(cluster_embedding)\n",
    "\n",
    "# Concatenation\n",
    "concat = Concatenate()([user_flatten, item_flatten, color_flatten, family_flatten, cod_section_flatten, discount_input, popularity_input, session_input, cluster_flatten])\n",
    "\n",
    "# Fully connected layers\n",
    "fc1 = Dense(128, activation='relu')(concat)\n",
    "fc1 = Dropout(0.4)(fc1)\n",
    "fc2 = Dense(64, activation='relu')(fc1)\n",
    "fc2 = Dropout(0.4)(fc2)\n",
    "output = Dense(1, activation='sigmoid')(fc2)\n",
    "\n",
    "model = Model(inputs=[user_input, item_input, color_input, family_input, cod_section_input, discount_input, popularity_input, session_input, cluster_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# Pesos de clase\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# --- Entrenamiento del modelo ---\n",
    "history = model.fit(\n",
    "    [\n",
    "        X_train['user_id'].values,\n",
    "        X_train['partnumber'].values,\n",
    "        X_train['color_id'].values,\n",
    "        X_train['family'].values,\n",
    "        X_train['cod_section'].values,\n",
    "        X_train['discount'].values,\n",
    "        X_train['popularity'].values,\n",
    "        X_train['session_interactions'].values,\n",
    "        X_train['cluster'].values\n",
    "    ],\n",
    "    y_train.values,\n",
    "    validation_data=(\n",
    "        [\n",
    "            X_val['user_id'].values,\n",
    "            X_val['partnumber'].values,\n",
    "            X_val['color_id'].values,\n",
    "            X_val['family'].values,\n",
    "            X_val['cod_section'].values,\n",
    "            X_val['discount'].values,\n",
    "            X_val['popularity'].values,\n",
    "            X_val['session_interactions'].values,\n",
    "            X_val['cluster'].values\n",
    "        ],\n",
    "        y_val.values\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# --- Evaluación del modelo ---\n",
    "eval_results = model.evaluate(\n",
    "    [\n",
    "        X_val['user_id'].values,\n",
    "        X_val['partnumber'].values,\n",
    "        X_val['color_id'].values,\n",
    "        X_val['family'].values,\n",
    "        X_val['cod_section'].values,\n",
    "        X_val['discount'].values,\n",
    "        X_val['popularity'].values,\n",
    "        X_val['session_interactions'].values,\n",
    "        X_val['cluster'].values\n",
    "    ],\n",
    "    y_val.values\n",
    ")\n",
    "print(\"\\nResultados de evaluación:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo en el formato nativo de Keras (recomendado)\n",
    "model.save(\"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/recommender_model_v3.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout\n",
    "\n",
    "\n",
    "# --- Cargar el DataFrame enriquecido ---\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl')\n",
    "\n",
    "# --- Preparación de los datos ---\n",
    "X = train_df[['user_id', 'partnumber', 'color_id', 'family', 'cod_section', 'discount', 'popularity', 'session_interactions', 'cluster']]\n",
    "y = train_df['add_to_cart']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis para implementar mejoras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Análisis de importancia de características**\n",
    "Usaremos técnicas como **árboles de decisión** o **SHAP** para medir la contribución relativa de cada característica en el modelo. Esto nos ayudará a identificar características redundantes o poco relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir la proporción de datos a usar\n",
    "sample_fraction = 0.1\n",
    "\n",
    "# Seleccionar una muestra aleatoria del 10% del conjunto de entrenamiento\n",
    "train_sample_idx = np.random.choice(X_train.index, size=int(len(X_train) * sample_fraction), replace=False)\n",
    "\n",
    "# Entrenar el modelo con la muestra seleccionada\n",
    "rf_model = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=42)\n",
    "rf_model.fit(X_train.loc[train_sample_idx], y_train.loc[train_sample_idx])\n",
    "\n",
    "# Importancia de características\n",
    "feature_importances = rf_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importances)\n",
    "\n",
    "# Gráfica de importancia de características\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X_train.columns[sorted_idx], feature_importances[sorted_idx])\n",
    "plt.xlabel('Importancia')\n",
    "plt.title('Importancia de Características (Random Forest)')\n",
    "plt.show()\n",
    "\n",
    "# Seleccionar una muestra aleatoria del 10% del conjunto de validación\n",
    "val_sample_idx = np.random.choice(X_val.index, size=int(len(X_val) * sample_fraction), replace=False)\n",
    "\n",
    "# Análisis de importancia por permutación\n",
    "perm_importance = permutation_importance(rf_model, X_val.loc[val_sample_idx], y_val.loc[val_sample_idx], random_state=42)\n",
    "sorted_perm_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X_train.columns[sorted_perm_idx], perm_importance.importances_mean[sorted_perm_idx])\n",
    "plt.xlabel('Importancia')\n",
    "plt.title('Importancia de Características (Permutación)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Análisis del impacto del balanceo de clases**\n",
    "Comprobemos cómo el desbalance afecta el aprendizaje del modelo y evaluemos el impacto del balanceo con **submuestreo** o **SMOTE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submuestreo de la clase mayoritaria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separar clases\n",
    "majority = train_df[train_df['add_to_cart'] == 0]\n",
    "minority = train_df[train_df['add_to_cart'] == 1]\n",
    "\n",
    "# Submuestreo\n",
    "majority_downsampled = resample(majority, replace=False, n_samples=len(minority), random_state=42)\n",
    "\n",
    "# Combinar datos balanceados\n",
    "balanced_train_df = pd.concat([majority_downsampled, minority])\n",
    "\n",
    "# Revisar proporciones\n",
    "print(\"Proporciones tras submuestreo:\")\n",
    "print(balanced_train_df['add_to_cart'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicación de SMOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Balancear con SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Confirmar nuevas proporciones\n",
    "print(\"Proporciones tras SMOTE:\")\n",
    "print(np.unique(y_resampled, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Curvas de ROC y Precision-Recall**\n",
    "Las curvas ROC y Precision-Recall son útiles para evaluar el rendimiento general y ajustar el umbral de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Ruta al archivo del modelo guardado\n",
    "model_path = \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/recommender_model_v3.keras\"\n",
    "\n",
    "\n",
    "# Cargar el modelo\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "# Predicciones en el conjunto de validación\n",
    "y_pred_proba = model.predict([\n",
    "    X_val['user_id'].values,\n",
    "    X_val['partnumber'].values,\n",
    "    X_val['color_id'].values,\n",
    "    X_val['family'].values,\n",
    "    X_val['cod_section'].values,\n",
    "    X_val['discount'].values,\n",
    "    X_val['popularity'].values,\n",
    "    X_val['session_interactions'].values,\n",
    "    X_val['cluster'].values\n",
    "])\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Curva Precision-Recall')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Curva Precision-Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_columns = X_train.select_dtypes(include=['datetime', 'datetime64']).columns\n",
    "print(f\"Columnas datetime: {datetime_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar y ajustar el dataset\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl')\n",
    "\n",
    "# Reducción de características\n",
    "train_df = train_df.drop(['discount', 'cod_section'], axis=1)\n",
    "\n",
    "# Convertir columnas datetime a numéricas\n",
    "train_df['date'] = pd.to_datetime(train_df['date']).astype(int) / 10**9\n",
    "train_df['timestamp_local'] = pd.to_datetime(train_df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "# Balanceo de clases\n",
    "majority = train_df[train_df['add_to_cart'] == 0]\n",
    "minority = train_df[train_df['add_to_cart'] == 1]\n",
    "\n",
    "majority_downsampled = majority.sample(n=len(minority), random_state=42)\n",
    "balanced_train_df = pd.concat([majority_downsampled, minority])\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X = balanced_train_df.drop('add_to_cart', axis=1)\n",
    "y = balanced_train_df['add_to_cart']\n",
    "\n",
    "# Codificar columnas categóricas y listas si existen\n",
    "label_encoders = {}\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object' or isinstance(X[column].iloc[0], list):\n",
    "        le = LabelEncoder()\n",
    "        X[column] = X[column].astype(str).apply(lambda x: ' '.join(map(str, x)) if isinstance(x, list) else x)\n",
    "        X[column] = le.fit_transform(X[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Dividir datos en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generar interacciones\n",
    "X_train['session_popularity'] = X_train['session_interactions'] * X_train['popularity']\n",
    "X_val['session_popularity'] = X_val['session_interactions'] * X_val['popularity']\n",
    "\n",
    "X_train['user_item'] = X_train['user_id'] * X_train['partnumber']\n",
    "X_val['user_item'] = X_val['user_id'] * X_val['partnumber']\n",
    "\n",
    "# Hiperparámetros para búsqueda aleatoria\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Modelo base\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=4)\n",
    "\n",
    "# Búsqueda aleatoria\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "                               n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=4)\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Mejor modelo\n",
    "best_rf = rf_random.best_estimator_\n",
    "\n",
    "# Predicciones\n",
    "y_pred_proba = best_rf.predict_proba(X_val)[:, 1]\n",
    "y_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Evaluación\n",
    "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "mcc = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "# Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Curva Precision-Recall\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Curva Precision-Recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Validación cruzada\n",
    "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(\"Validación cruzada AUC promedio:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar y ajustar el dataset\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl')\n",
    "\n",
    "# Seleccionar una muestra aleatoria del 10% del dataset para pruebas\n",
    "sample_fraction = 0.1\n",
    "train_df = train_df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# Reducción de características\n",
    "train_df = train_df.drop(['discount', 'cod_section'], axis=1)\n",
    "\n",
    "# Convertir columnas datetime a numéricas\n",
    "train_df['date'] = pd.to_datetime(train_df['date']).astype(int) / 10**9\n",
    "train_df['timestamp_local'] = pd.to_datetime(train_df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "# Balanceo de clases\n",
    "majority = train_df[train_df['add_to_cart'] == 0]\n",
    "minority = train_df[train_df['add_to_cart'] == 1]\n",
    "\n",
    "majority_downsampled = majority.sample(n=len(minority), random_state=42)\n",
    "balanced_train_df = pd.concat([majority_downsampled, minority])\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X = balanced_train_df.drop('add_to_cart', axis=1)\n",
    "y = balanced_train_df['add_to_cart']\n",
    "\n",
    "# Codificar columnas categóricas y listas si existen\n",
    "label_encoders = {}\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object' or isinstance(X[column].iloc[0], list):\n",
    "        le = LabelEncoder()\n",
    "        X[column] = X[column].astype(str).apply(lambda x: ' '.join(map(str, x)) if isinstance(x, list) else x)\n",
    "        X[column] = le.fit_transform(X[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Dividir datos en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generar interacciones\n",
    "X_train['session_popularity'] = X_train['session_interactions'] * X_train['popularity']\n",
    "X_val['session_popularity'] = X_val['session_interactions'] * X_val['popularity']\n",
    "\n",
    "X_train['user_item'] = X_train['user_id'] * X_train['partnumber']\n",
    "X_val['user_item'] = X_val['user_id'] * X_val['partnumber']\n",
    "\n",
    "# Hiperparámetros para búsqueda aleatoria\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Modelo base\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=4)\n",
    "\n",
    "# Búsqueda aleatoria\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "                               n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=4)\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Mejor modelo\n",
    "best_rf = rf_random.best_estimator_\n",
    "\n",
    "# Predicciones\n",
    "y_pred_proba = best_rf.predict_proba(X_val)[:, 1]\n",
    "y_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Evaluación\n",
    "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "mcc = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "# Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Curva Precision-Recall\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Curva Precision-Recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Validación cruzada\n",
    "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(\"Validación cruzada AUC promedio:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluación de los Resultados**\n",
    "\n",
    "#### **Métricas del Modelo**\n",
    "\n",
    "1. **ROC AUC (Conjunto de Validación):** 0.70\n",
    "   - Indica que el modelo tiene una capacidad moderada para distinguir entre positivos y negativos. En el rango de 0.5 a 1, esto es un desempeño decente para un modelo inicial, aunque hay espacio para mejora.\n",
    "   \n",
    "2. **Precisión:** 0.65\n",
    "   - De cada predicción positiva realizada por el modelo, aproximadamente el 65% son correctas. Esto significa que hay falsos positivos que deberían ser reducidos para mejorar la precisión.\n",
    "\n",
    "3. **Recall:** 0.62\n",
    "   - El modelo identifica correctamente el 62% de los verdaderos positivos. Aunque este es un buen inicio, trabajar en mejorar el recall ayudaría a capturar más interacciones positivas (productos añadidos al carrito).\n",
    "\n",
    "4. **F1 Score:** 0.63\n",
    "   - Como métrica de balance entre precisión y recall, este valor indica un desempeño moderado. A medida que el modelo se afine, deberíamos buscar incrementarlo.\n",
    "\n",
    "5. **MCC (Matthews Correlation Coefficient):** 0.28\n",
    "   - Una métrica más estricta que combina las tasas de falsos positivos, falsos negativos, verdaderos positivos y verdaderos negativos. Aunque positiva, este valor sugiere que hay margen considerable para mejora.\n",
    "\n",
    "6. **Validación cruzada AUC promedio:** 0.70\n",
    "   - Este resultado consistente con el ROC AUC del conjunto de validación sugiere que el modelo no está sobreajustado y que los resultados son estables en diferentes particiones de los datos.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Gráficas**\n",
    "\n",
    "1. **Curva ROC**\n",
    "   - El AUC de 0.70 refleja una separación moderada entre las clases. \n",
    "   - **Interpretación:** El modelo tiene un desempeño razonable para maximizar los verdaderos positivos mientras minimiza los falsos positivos.\n",
    "   - **Mejora:** Podríamos intentar ajustes de hiperparámetros más detallados, técnicas de ensamble o agregar más features para mejorar la capacidad de discriminación.\n",
    "\n",
    "2. **Curva Precision-Recall**\n",
    "   - El AUC de 0.72 muestra que el modelo tiene un desempeño razonable en un escenario desequilibrado, donde las clases positivas (productos añadidos al carrito) podrían ser menos frecuentes.\n",
    "   - **Interpretación:** La curva decrece rápidamente, lo que indica que a medida que el recall aumenta, la precisión disminuye. Esto es típico cuando el modelo está intentando maximizar el recall.\n",
    "   - **Mejora:** Podríamos intentar estrategias para manejar el desequilibrio de clases (e.g., cambiar el umbral de decisión, o técnicas de re-muestreo más avanzadas).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Análisis Final**\n",
    "\n",
    "- El modelo tiene un **desempeño inicial prometedor**, pero hay margen para optimización:\n",
    "  - Experimentar con diferentes arquitecturas de modelos, como ensambles (e.g., Gradient Boosting, XGBoost) o redes neuronales.\n",
    "  - Investigar características adicionales, como embeddings más detallados o combinaciones de variables que capten mejor las interacciones usuario-producto.\n",
    "  - Aplicar técnicas avanzadas de ajuste de hiperparámetros, como `GridSearchCV` o `Bayesian Optimization`.\n",
    "\n",
    "- El **dataset reducido (10%)** permitió completar la ejecución con tiempos razonables, pero sería ideal escalar el modelo completo para obtener resultados más robustos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el dataset de entrenamiento es grande y el modelo de **Random Forest** puede no ser ideal en estos escenarios, ajustar el modelo basado en **embeddings** es una decisión estratégica sólida. Aquí te detallo cómo podríamos proceder para optimizar y validar este modelo:\n",
    "\n",
    "---\n",
    "\n",
    "### **Ajustes y Mejoras para el Modelo de Embeddings**\n",
    "\n",
    "1. **Incrementar el Tamaño de las Embeddings**\n",
    "   - Actualmente tienes embeddings de tamaño:\n",
    "     - `user_id` y `partnumber`: **64**\n",
    "     - Otras variables categóricas: **16**\n",
    "   - Considera aumentar las dimensiones de las embeddings para usuarios y productos a **128**, especialmente si tienes muchos usuarios/productos únicos.\n",
    "   - Otras categorías podrían incrementarse a **32** dependiendo del número de valores únicos.\n",
    "\n",
    "   ```python\n",
    "   user_embedding = Embedding(num_users, 128)(user_input)\n",
    "   item_embedding = Embedding(num_items, 128)(item_input)\n",
    "   ```\n",
    "\n",
    "2. **Capas Densas y Regularización**\n",
    "   - Agregar una capa adicional al modelo denso puede ayudar a capturar relaciones más complejas.\n",
    "   - Aumenta el `Dropout` si notas señales de sobreajuste (reduce las métricas en `val_loss` mientras `loss` mejora).\n",
    "\n",
    "   ```python\n",
    "   fc3 = Dense(32, activation='relu')(fc2)\n",
    "   fc3 = Dropout(0.5)(fc3)\n",
    "   output = Dense(1, activation='sigmoid')(fc3)\n",
    "   ```\n",
    "\n",
    "3. **Ajustar el Optimizador**\n",
    "   - Cambia de `Adam` a otros optimizadores como:\n",
    "     - **AdamW**: Incluye regularización L2 automáticamente.\n",
    "     - **SGD** con `momentum`: Más estable para datasets grandes, pero más lento.\n",
    "\n",
    "   ```python\n",
    "   model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001),\n",
    "                 loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "   ```\n",
    "\n",
    "4. **Callback de Reducción del Learning Rate**\n",
    "   - Introduce un callback para reducir automáticamente el `learning rate` si no mejora la métrica en el conjunto de validación.\n",
    "\n",
    "   ```python\n",
    "   callbacks = [\n",
    "       tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "       tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "5. **Clase `BatchNormalization`**\n",
    "   - Inserta una capa de normalización después de la concatenación para estabilizar el entrenamiento.\n",
    "\n",
    "   ```python\n",
    "   concat = BatchNormalization()(concat)\n",
    "   ```\n",
    "\n",
    "6. **Pesos de Clases**\n",
    "   - Mantén el uso de `class_weight` para balancear el desequilibrio de clases en el dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Plan de Validación**\n",
    "\n",
    "1. **K-Fold Cross Validation**\n",
    "   - Divide el dataset en **k** particiones (e.g., `k=5`) para evaluar el modelo en cada partición y obtener una métrica promedio (como AUC).\n",
    "\n",
    "   ```python\n",
    "   from sklearn.model_selection import KFold\n",
    "   kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "   ```\n",
    "\n",
    "2. **Métricas de Evaluación**\n",
    "   - Genera métricas como:\n",
    "     - **AUC** y **Accuracy** en validación.\n",
    "     - Curvas ROC y Precision-Recall.\n",
    "   - Esto permitirá una comparación directa con modelos previos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "# Configurar para que el crecimiento de memoria sea dinámico\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# --- Cargar el DataFrame enriquecido ---\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl')\n",
    "\n",
    "# --- Verificación y mapeo de índices ---\n",
    "user_mapping = {id_: idx for idx, id_ in enumerate(train_df['user_id'].unique())}\n",
    "item_mapping = {id_: idx for idx, id_ in enumerate(train_df['partnumber'].unique())}\n",
    "color_mapping = {id_: idx for idx, id_ in enumerate(train_df['color_id'].unique())}\n",
    "family_mapping = {id_: idx for idx, id_ in enumerate(train_df['family'].unique())}\n",
    "cod_section_mapping = {id_: idx for idx, id_ in enumerate(train_df['cod_section'].unique())}\n",
    "cluster_mapping = {id_: idx for idx, id_ in enumerate(train_df['cluster'].unique())}\n",
    "\n",
    "# Aplicar mapeo\n",
    "train_df['user_id'] = train_df['user_id'].map(user_mapping).fillna(0).astype(int)\n",
    "train_df['partnumber'] = train_df['partnumber'].map(item_mapping).fillna(0).astype(int)\n",
    "train_df['color_id'] = train_df['color_id'].map(color_mapping).fillna(0).astype(int)\n",
    "train_df['family'] = train_df['family'].map(family_mapping).fillna(0).astype(int)\n",
    "train_df['cod_section'] = train_df['cod_section'].map(cod_section_mapping).fillna(0).astype(int)\n",
    "train_df['cluster'] = train_df['cluster'].map(cluster_mapping).fillna(0).astype(int)\n",
    "\n",
    "# --- Verificar rangos y valores ---\n",
    "print(\"Verificando valores fuera de rango:\")\n",
    "print(f\"user_id max: {train_df['user_id'].max()} / {len(user_mapping)}\")\n",
    "print(f\"partnumber max: {train_df['partnumber'].max()} / {len(item_mapping)}\")\n",
    "print(f\"color_id max: {train_df['color_id'].max()} / {len(color_mapping)}\")\n",
    "print(f\"family max: {train_df['family'].max()} / {len(family_mapping)}\")\n",
    "print(f\"cod_section max: {train_df['cod_section'].max()} / {len(cod_section_mapping)}\")\n",
    "print(f\"cluster max: {train_df['cluster'].max()} / {len(cluster_mapping)}\")\n",
    "\n",
    "# --- Preparación de los datos ---\n",
    "X = train_df[['user_id', 'partnumber', 'color_id', 'family', 'cod_section', 'discount', 'popularity', 'session_interactions', 'cluster']]\n",
    "y = train_df['add_to_cart']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Dimensiones del modelo ---\n",
    "num_users = len(user_mapping) + 1\n",
    "num_items = len(item_mapping) + 1\n",
    "num_colors = len(color_mapping) + 1\n",
    "num_families = len(family_mapping) + 1\n",
    "num_cod_sections = len(cod_section_mapping) + 1\n",
    "num_clusters = len(cluster_mapping) + 1\n",
    "\n",
    "# --- Construcción del modelo optimizado ---\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "color_input = Input(shape=(1,), name='color_input')\n",
    "family_input = Input(shape=(1,), name='family_input')\n",
    "cod_section_input = Input(shape=(1,), name='cod_section_input')\n",
    "discount_input = Input(shape=(1,), name='discount_input')\n",
    "popularity_input = Input(shape=(1,), name='popularity_input')\n",
    "session_input = Input(shape=(1,), name='session_input')\n",
    "cluster_input = Input(shape=(1,), name='cluster_input')\n",
    "\n",
    "# Embeddings\n",
    "user_embedding = Embedding(num_users, 128)(user_input)\n",
    "item_embedding = Embedding(num_items, 128)(item_input)\n",
    "color_embedding = Embedding(num_colors, 32)(color_input)\n",
    "family_embedding = Embedding(num_families, 32)(family_input)\n",
    "cod_section_embedding = Embedding(num_cod_sections, 32)(cod_section_input)\n",
    "cluster_embedding = Embedding(num_clusters, 32)(cluster_input)\n",
    "\n",
    "# Flatten layers\n",
    "user_flatten = Flatten()(user_embedding)\n",
    "item_flatten = Flatten()(item_embedding)\n",
    "color_flatten = Flatten()(color_embedding)\n",
    "family_flatten = Flatten()(family_embedding)\n",
    "cod_section_flatten = Flatten()(cod_section_embedding)\n",
    "cluster_flatten = Flatten()(cluster_embedding)\n",
    "\n",
    "# Concatenation\n",
    "concat = Concatenate()([user_flatten, item_flatten, color_flatten, family_flatten, cod_section_flatten,\n",
    "                        discount_input, popularity_input, session_input, cluster_flatten])\n",
    "concat = BatchNormalization()(concat)\n",
    "\n",
    "# Fully connected layers\n",
    "fc1 = Dense(256, activation='relu')(concat)\n",
    "fc1 = Dropout(0.5)(fc1)\n",
    "fc2 = Dense(128, activation='relu')(fc1)\n",
    "fc2 = Dropout(0.5)(fc2)\n",
    "fc3 = Dense(32, activation='relu')(fc2)\n",
    "fc3 = Dropout(0.5)(fc3)\n",
    "output = Dense(1, activation='sigmoid')(fc3)\n",
    "\n",
    "model = Model(inputs=[user_input, item_input, color_input, family_input, cod_section_input,\n",
    "                      discount_input, popularity_input, session_input, cluster_input], outputs=output)\n",
    "model.compile(optimizer=AdamW(learning_rate=0.001),\n",
    "              loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# --- Pesos de clase ---\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# --- Entrenamiento ---\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    [\n",
    "        X_train['user_id'].values, X_train['partnumber'].values, X_train['color_id'].values,\n",
    "        X_train['family'].values, X_train['cod_section'].values, X_train['discount'].values,\n",
    "        X_train['popularity'].values, X_train['session_interactions'].values, X_train['cluster'].values\n",
    "    ],\n",
    "    y_train.values,\n",
    "    validation_data=(\n",
    "        [\n",
    "            X_val['user_id'].values, X_val['partnumber'].values, X_val['color_id'].values,\n",
    "            X_val['family'].values, X_val['cod_section'].values, X_val['discount'].values,\n",
    "            X_val['popularity'].values, X_val['session_interactions'].values, X_val['cluster'].values\n",
    "        ],\n",
    "        y_val.values\n",
    "    ),\n",
    "    epochs=15,\n",
    "    batch_size=512,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# --- Evaluación ---\n",
    "eval_results = model.evaluate(\n",
    "    [\n",
    "        X_val['user_id'].values, X_val['partnumber'].values, X_val['color_id'].values,\n",
    "        X_val['family'].values, X_val['cod_section'].values, X_val['discount'].values,\n",
    "        X_val['popularity'].values, X_val['session_interactions'].values, X_val['cluster'].values\n",
    "    ],\n",
    "    y_val.values\n",
    ")\n",
    "print(\"\\nResultados de evaluación:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo en el formato nativo de Keras (recomendado)\n",
    "model.save(\"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/recommender_model_v4.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de Resultados\n",
    "\n",
    "#### 1. **Métricas finales del modelo**\n",
    "- **Pérdida de validación (`val_loss`)**: 0.5834\n",
    "- **AUC de validación (`val_AUC`)**: 0.6694\n",
    "- **Accuracy de validación (`val_accuracy`)**: 68.84%\n",
    "\n",
    "El modelo ha logrado un rendimiento decente en términos de AUC de validación y accuracy. Sin embargo, hay algunos puntos importantes que debemos analizar:\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Evolución de las métricas durante el entrenamiento**\n",
    "- **AUC en validación**:\n",
    "  - Comenzó en 0.6686 y alcanzó un máximo de 0.6713, pero no mostró mejoras significativas después de la tercera epoch.\n",
    "  - Las últimas epochs no aportaron mejoras consistentes en la AUC, lo que sugiere que el modelo puede haber alcanzado su límite de aprendizaje para la configuración actual.\n",
    "\n",
    "- **Pérdida de validación (`val_loss`)**:\n",
    "  - Fluctuó entre 0.5834 y 0.6412 en las últimas epochs, indicando un posible ajuste excesivo o falta de aprendizaje adicional.\n",
    "\n",
    "- **Learning Rate Decay**:\n",
    "  - La reducción de la tasa de aprendizaje (a 0.0005) no tuvo un impacto significativo en la mejora de las métricas.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Aspectos técnicos del entrenamiento**\n",
    "- **Advertencias relacionadas con CUDA/cuDNN**:\n",
    "  - Aunque estas advertencias no detuvieron el entrenamiento, pueden haber reducido la eficiencia del hardware.\n",
    "  - Es recomendable validar la configuración de CUDA/cuDNN para maximizar el rendimiento.\n",
    "\n",
    "- **Uso de memoria GPU**:\n",
    "  - La configuración para crecimiento dinámico de memoria fue efectiva, permitiendo una ejecución estable en términos de recursos.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Interpretación general**\n",
    "- **Puntos positivos**:\n",
    "  - La AUC en validación es consistente (~0.67), lo que muestra un buen ajuste del modelo.\n",
    "  - La accuracy (~68.84%) en validación es competitiva.\n",
    "\n",
    "- **Puntos de mejora**:\n",
    "  - El modelo parece alcanzar un punto de saturación después de pocas epochs.\n",
    "  - Fluctuaciones en la pérdida de validación podrían indicar un ajuste no óptimo en las capas finales o en la arquitectura del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Recomendaciones para próximos pasos**\n",
    "\n",
    "1. **Optimización adicional del modelo**\n",
    "   - **Incrementar la regularización**: Probar un aumento en los valores de Dropout o ajustar los pesos de clase.\n",
    "   - **Tuning de la arquitectura**:\n",
    "     - Probar con menos unidades en las capas densas (reducir 256 → 128, etc.).\n",
    "     - Cambiar las funciones de activación en las capas densas, como ReLU → LeakyReLU.\n",
    "   - **Optimización del Learning Rate**:\n",
    "     - Usar un planificador dinámico con más control (por ejemplo, `LearningRateScheduler`).\n",
    "\n",
    "2. **Ajustes en el preprocesamiento de datos**\n",
    "   - Revisar posibles interacciones adicionales entre las características para mejorar el modelo.\n",
    "\n",
    "3. **Validación más robusta**\n",
    "   - Realizar un análisis más exhaustivo en un conjunto de test para validar la capacidad de generalización del modelo.\n",
    "\n",
    "4. **Alternativas al modelo actual**\n",
    "   - Explorar arquitecturas más avanzadas como **DeepFM** o enfoques híbridos basados en embeddings y métodos basados en grafos.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reentreno del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m72737/72737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 6ms/step - AUC: 0.6460 - accuracy: 0.5924 - loss: 0.6562 - val_AUC: 0.6699 - val_accuracy: 0.6419 - val_loss: 0.6481 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m72737/72737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 6ms/step - AUC: 0.6787 - accuracy: 0.6298 - loss: 0.6374 - val_AUC: 0.6729 - val_accuracy: 0.6235 - val_loss: 0.6419 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m72737/72737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 6ms/step - AUC: 0.6850 - accuracy: 0.6255 - loss: 0.6333 - val_AUC: 0.6727 - val_accuracy: 0.6574 - val_loss: 0.6263 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m72737/72737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 6ms/step - AUC: 0.6869 - accuracy: 0.6240 - loss: 0.6325 - val_AUC: 0.6719 - val_accuracy: 0.5827 - val_loss: 0.6437 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m72737/72737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 6ms/step - AUC: 0.6885 - accuracy: 0.6243 - loss: 0.6312 - val_AUC: 0.6738 - val_accuracy: 0.6105 - val_loss: 0.6316 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m72737/72737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 6ms/step - AUC: 0.6890 - accuracy: 0.6233 - loss: 0.6311 - val_AUC: 0.6732 - val_accuracy: 0.6054 - val_loss: 0.6482 - learning_rate: 0.0010\n",
      "\u001b[1m290947/290947\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m668s\u001b[0m 2ms/step - AUC: 0.6715 - accuracy: 0.6571 - loss: 0.6266\n",
      "\n",
      "Resultados de evaluación: [0.6263159513473511, 0.6574017405509949, 0.672680139541626]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Configurar para que el crecimiento de memoria sea dinámico\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# --- Cargar el DataFrame enriquecido ---\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl')\n",
    "\n",
    "# --- Verificación y mapeo de índices ---\n",
    "user_mapping = {id_: idx for idx, id_ in enumerate(train_df['user_id'].unique())}\n",
    "item_mapping = {id_: idx for idx, id_ in enumerate(train_df['partnumber'].unique())}\n",
    "color_mapping = {id_: idx for idx, id_ in enumerate(train_df['color_id'].unique())}\n",
    "family_mapping = {id_: idx for idx, id_ in enumerate(train_df['family'].unique())}\n",
    "cod_section_mapping = {id_: idx for idx, id_ in enumerate(train_df['cod_section'].unique())}\n",
    "cluster_mapping = {id_: idx for idx, id_ in enumerate(train_df['cluster'].unique())}\n",
    "\n",
    "# Aplicar mapeo\n",
    "train_df['user_id'] = train_df['user_id'].map(user_mapping).fillna(0).astype(int)\n",
    "train_df['partnumber'] = train_df['partnumber'].map(item_mapping).fillna(0).astype(int)\n",
    "train_df['color_id'] = train_df['color_id'].map(color_mapping).fillna(0).astype(int)\n",
    "train_df['family'] = train_df['family'].map(family_mapping).fillna(0).astype(int)\n",
    "train_df['cod_section'] = train_df['cod_section'].map(cod_section_mapping).fillna(0).astype(int)\n",
    "train_df['cluster'] = train_df['cluster'].map(cluster_mapping).fillna(0).astype(int)\n",
    "\n",
    "# --- Preparación de los datos ---\n",
    "X = train_df[['user_id', 'partnumber', 'color_id', 'family', 'cod_section', 'discount', 'popularity', 'session_interactions', 'cluster']]\n",
    "y = train_df['add_to_cart']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Dimensiones del modelo ---\n",
    "num_users = len(user_mapping) + 1\n",
    "num_items = len(item_mapping) + 1\n",
    "num_colors = len(color_mapping) + 1\n",
    "num_families = len(family_mapping) + 1\n",
    "num_cod_sections = len(cod_section_mapping) + 1\n",
    "num_clusters = len(cluster_mapping) + 1\n",
    "\n",
    "# --- Construcción del modelo optimizado ---\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "color_input = Input(shape=(1,), name='color_input')\n",
    "family_input = Input(shape=(1,), name='family_input')\n",
    "cod_section_input = Input(shape=(1,), name='cod_section_input')\n",
    "discount_input = Input(shape=(1,), name='discount_input')\n",
    "popularity_input = Input(shape=(1,), name='popularity_input')\n",
    "session_input = Input(shape=(1,), name='session_input')\n",
    "cluster_input = Input(shape=(1,), name='cluster_input')\n",
    "\n",
    "# Embeddings\n",
    "user_embedding = Embedding(num_users, 64)(user_input)\n",
    "item_embedding = Embedding(num_items, 64)(item_input)\n",
    "color_embedding = Embedding(num_colors, 16)(color_input)\n",
    "family_embedding = Embedding(num_families, 16)(family_input)\n",
    "cod_section_embedding = Embedding(num_cod_sections, 16)(cod_section_input)\n",
    "cluster_embedding = Embedding(num_clusters, 16)(cluster_input)\n",
    "\n",
    "# Flatten layers\n",
    "user_flatten = Flatten()(user_embedding)\n",
    "item_flatten = Flatten()(item_embedding)\n",
    "color_flatten = Flatten()(color_embedding)\n",
    "family_flatten = Flatten()(family_embedding)\n",
    "cod_section_flatten = Flatten()(cod_section_embedding)\n",
    "cluster_flatten = Flatten()(cluster_embedding)\n",
    "\n",
    "# Concatenation\n",
    "concat = Concatenate()([user_flatten, item_flatten, color_flatten, family_flatten, cod_section_flatten,\n",
    "                        discount_input, popularity_input, session_input, cluster_flatten])\n",
    "concat = BatchNormalization()(concat)\n",
    "\n",
    "# Fully connected layers with LeakyReLU\n",
    "fc1 = Dense(128)(concat)\n",
    "fc1 = LeakyReLU(negative_slope=0.1)(fc1)\n",
    "fc1 = Dropout(0.4)(fc1)\n",
    "fc2 = Dense(64)(fc1)\n",
    "fc2 = LeakyReLU(negative_slope=0.1)(fc2)\n",
    "fc2 = Dropout(0.4)(fc2)\n",
    "output = Dense(1, activation='sigmoid')(fc2)\n",
    "\n",
    "model = Model(inputs=[user_input, item_input, color_input, family_input, cod_section_input,\n",
    "                      discount_input, popularity_input, session_input, cluster_input], outputs=output)\n",
    "model.compile(optimizer=AdamW(learning_rate=0.001),\n",
    "              loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# --- Pesos de clase ---\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# --- Scheduler para el Learning Rate ---\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 5:\n",
    "        return lr * 0.5\n",
    "    return lr\n",
    "\n",
    "# --- Entrenamiento ---\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    LearningRateScheduler(lr_schedule)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    [\n",
    "        X_train['user_id'].values, X_train['partnumber'].values, X_train['color_id'].values,\n",
    "        X_train['family'].values, X_train['cod_section'].values, X_train['discount'].values,\n",
    "        X_train['popularity'].values, X_train['session_interactions'].values, X_train['cluster'].values\n",
    "    ],\n",
    "    y_train.values,\n",
    "    validation_data=(\n",
    "        [\n",
    "            X_val['user_id'].values, X_val['partnumber'].values, X_val['color_id'].values,\n",
    "            X_val['family'].values, X_val['cod_section'].values, X_val['discount'].values,\n",
    "            X_val['popularity'].values, X_val['session_interactions'].values, X_val['cluster'].values\n",
    "        ],\n",
    "        y_val.values\n",
    "    ),\n",
    "    epochs=15,\n",
    "    batch_size=512,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# --- Evaluación ---\n",
    "eval_results = model.evaluate(\n",
    "    [\n",
    "        X_val['user_id'].values, X_val['partnumber'].values, X_val['color_id'].values,\n",
    "        X_val['family'].values, X_val['cod_section'].values, X_val['discount'].values,\n",
    "        X_val['popularity'].values, X_val['session_interactions'].values, X_val['cluster'].values\n",
    "    ],\n",
    "    y_val.values\n",
    ")\n",
    "print(\"\\nResultados de evaluación:\", eval_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# Guardar el modelo en el formato nativo de Keras\n",
    "model.save(\"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/recommender_model_v5.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --- Cargar el conjunto de prueba ---\n",
    "test_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl')\n",
    "\n",
    "# --- Mapear IDs disponibles ---\n",
    "test_df['user_id'] = test_df['user_id'].map(user_mapping).fillna(0).astype(int)\n",
    "test_df['partnumber'] = test_df['partnumber'].map(item_mapping).fillna(0).astype(int)\n",
    "\n",
    "# --- Añadir columnas faltantes con valores por defecto ---\n",
    "default_columns = ['color_id', 'family', 'cod_section', 'discount', 'popularity', 'session_interactions', 'cluster']\n",
    "for col in default_columns:\n",
    "    test_df[col] = 0\n",
    "\n",
    "# --- Obtener productos populares globales ---\n",
    "global_popularity = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "# --- Obtener las session_id únicas ---\n",
    "session_ids = test_df['session_id'].unique()\n",
    "\n",
    "# --- Cargar el modelo entrenado ---\n",
    "model = load_model('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/recommender_model_v5.keras')\n",
    "\n",
    "# --- Generar predicciones ---\n",
    "predictions = {}\n",
    "\n",
    "for session_id in session_ids:\n",
    "    # Filtrar los datos de la sesión actual\n",
    "    session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "\n",
    "    # Preparar las entradas para el modelo\n",
    "    user_input = session_data['user_id'].values\n",
    "    item_input = session_data['partnumber'].values\n",
    "    color_input = session_data['color_id'].values\n",
    "    family_input = session_data['family'].values\n",
    "    cod_section_input = session_data['cod_section'].values\n",
    "    discount_input = session_data['discount'].values\n",
    "    popularity_input = session_data['popularity'].values\n",
    "    session_input = session_data['session_interactions'].values\n",
    "    cluster_input = session_data['cluster'].values\n",
    "\n",
    "    # Predicciones del modelo\n",
    "    if len(user_input) > 0:\n",
    "        scores = model.predict([\n",
    "            user_input, item_input, color_input, family_input,\n",
    "            cod_section_input, discount_input, popularity_input,\n",
    "            session_input, cluster_input\n",
    "        ], verbose=0)\n",
    "        session_data['score'] = scores\n",
    "        top_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates().tolist()\n",
    "        )\n",
    "    else:\n",
    "        top_products = []\n",
    "\n",
    "    # Rellenar con historial personalizado\n",
    "    if len(top_products) < 5:\n",
    "        additional_products = (\n",
    "            session_data['partnumber']\n",
    "            .value_counts()\n",
    "            .index.tolist()\n",
    "        )\n",
    "        for product in additional_products:\n",
    "            if product not in top_products:\n",
    "                top_products.append(product)\n",
    "            if len(top_products) >= 5:\n",
    "                break\n",
    "\n",
    "    # Rellenar con productos populares globales\n",
    "    if len(top_products) < 5:\n",
    "        for product in global_popularity:\n",
    "            if product not in top_products:\n",
    "                top_products.append(product)\n",
    "            if len(top_products) >= 5:\n",
    "                break\n",
    "\n",
    "    # Asegurar predicciones únicas y exactas a 5 productos\n",
    "    predictions[str(session_id)] = top_products[:5]\n",
    "\n",
    "# --- Guardar el archivo predictions_3.json ---\n",
    "output = {\"target\": predictions}\n",
    "\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_v5.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "print(f\"Archivo predictions_3.json generado con éxito en {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Cargar el conjunto de prueba ---\n",
    "test_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl')\n",
    "\n",
    "# --- Mapear IDs disponibles ---\n",
    "test_df['user_id'] = test_df['user_id'].map(user_mapping).fillna(0).astype(int)\n",
    "test_df['partnumber'] = test_df['partnumber'].map(item_mapping).fillna(0).astype(int)\n",
    "\n",
    "# --- Añadir columnas faltantes con valores por defecto ---\n",
    "default_columns = ['color_id', 'family', 'cod_section', 'discount', 'popularity', 'session_interactions', 'cluster']\n",
    "for col in default_columns:\n",
    "    test_df[col] = 0\n",
    "\n",
    "# --- Obtener productos populares globales ---\n",
    "global_popularity = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "# --- Obtener las session_id únicas ---\n",
    "session_ids = test_df['session_id'].unique()\n",
    "\n",
    "# --- Cargar el modelo entrenado ---\n",
    "model = load_model('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/recommender_model_v5.keras')\n",
    "\n",
    "# --- Generar predicciones ---\n",
    "predictions = {}\n",
    "\n",
    "for session_id in session_ids:\n",
    "    # Filtrar los datos de la sesión actual\n",
    "    session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "\n",
    "    # Preparar las entradas para el modelo\n",
    "    user_input = session_data['user_id'].values\n",
    "    item_input = session_data['partnumber'].values\n",
    "    color_input = session_data['color_id'].values\n",
    "    family_input = session_data['family'].values\n",
    "    cod_section_input = session_data['cod_section'].values\n",
    "    discount_input = session_data['discount'].values\n",
    "    popularity_input = session_data['popularity'].values\n",
    "    session_input = session_data['session_interactions'].values\n",
    "    cluster_input = session_data['cluster'].values\n",
    "\n",
    "    # Predicciones del modelo\n",
    "    top_products = []\n",
    "    if len(user_input) > 0:\n",
    "        scores = model.predict([\n",
    "            user_input, item_input, color_input, family_input,\n",
    "            cod_section_input, discount_input, popularity_input,\n",
    "            session_input, cluster_input\n",
    "        ], verbose=0)\n",
    "        session_data['score'] = scores\n",
    "        top_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates().tolist()\n",
    "        )\n",
    "\n",
    "    # Rellenar con historial personalizado\n",
    "    if len(top_products) < 5:\n",
    "        additional_products = (\n",
    "            session_data['partnumber']\n",
    "            .value_counts()\n",
    "            .index.tolist()\n",
    "        )\n",
    "        for product in additional_products:\n",
    "            if product not in top_products:\n",
    "                top_products.append(product)\n",
    "            if len(top_products) >= 5:\n",
    "                break\n",
    "\n",
    "    # Rellenar con productos populares condicionados por país\n",
    "    user_country = session_data['country'].iloc[0] if not session_data.empty else None\n",
    "    if len(top_products) < 5 and user_country is not None:\n",
    "        country_popular = (\n",
    "            test_df[test_df['country'] == user_country]['partnumber']\n",
    "            .value_counts()\n",
    "            .index.tolist()\n",
    "        )\n",
    "        for product in country_popular:\n",
    "            if product not in top_products:\n",
    "                top_products.append(product)\n",
    "            if len(top_products) >= 5:\n",
    "                break\n",
    "\n",
    "    # Completar con productos globales\n",
    "    if len(top_products) < 5:\n",
    "        for product in global_popularity:\n",
    "            if product not in top_products:\n",
    "                top_products.append(product)\n",
    "            if len(top_products) >= 5:\n",
    "                break\n",
    "\n",
    "    # Asegurar predicciones únicas y exactas a 5 productos\n",
    "    predictions[str(session_id)] = top_products[:5]\n",
    "\n",
    "# --- Guardar el archivo predictions_3.json ---\n",
    "output = {\"target\": predictions}\n",
    "\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_optimized_v5.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "print(f\"Archivo predictions_3.json generado con éxito en {output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
