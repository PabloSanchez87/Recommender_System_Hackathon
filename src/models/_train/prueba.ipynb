{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp_local</th>\n",
       "      <th>add_to_cart</th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "      <th>partnumber</th>\n",
       "      <th>device_type</th>\n",
       "      <th>pagetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>2024-06-06</td>\n",
       "      <td>2024-06-06 16:43:17.389</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>14327</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117</td>\n",
       "      <td>2024-06-08</td>\n",
       "      <td>2024-06-08 15:11:02.782</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>38422</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117</td>\n",
       "      <td>2024-06-08</td>\n",
       "      <td>2024-06-08 15:11:44.797</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>19763</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>579</td>\n",
       "      <td>2024-06-05</td>\n",
       "      <td>2024-06-05 19:24:48.397</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>30253</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1220</td>\n",
       "      <td>2024-06-04</td>\n",
       "      <td>2024-06-04 08:21:13.476</td>\n",
       "      <td>0</td>\n",
       "      <td>480729</td>\n",
       "      <td>25</td>\n",
       "      <td>1592</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id       date         timestamp_local  add_to_cart  user_id  \\\n",
       "0          64 2024-06-06 2024-06-06 16:43:17.389            0       -1   \n",
       "1         117 2024-06-08 2024-06-08 15:11:02.782            0       -1   \n",
       "2         117 2024-06-08 2024-06-08 15:11:44.797            0       -1   \n",
       "3         579 2024-06-05 2024-06-05 19:24:48.397            0       -1   \n",
       "4        1220 2024-06-04 2024-06-04 08:21:13.476            0   480729   \n",
       "\n",
       "   country  partnumber  device_type  pagetype  \n",
       "0       29       14327            1        24  \n",
       "1       57       38422            1        24  \n",
       "2       57       19763            1        24  \n",
       "3       29       30253            1        24  \n",
       "4       25        1592            1        24  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model.txt'\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data.pkl'\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched_test.json'\n",
    "\n",
    "train_df = pd.read_pickle(train_path)\n",
    "\n",
    "train_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp_local</th>\n",
       "      <th>add_to_cart</th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "      <th>partnumber</th>\n",
       "      <th>device_type</th>\n",
       "      <th>pagetype</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week</th>\n",
       "      <th>popularity</th>\n",
       "      <th>session_interactions</th>\n",
       "      <th>cluster</th>\n",
       "      <th>discount_category</th>\n",
       "      <th>similar_products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>2024-06-06</td>\n",
       "      <td>2024-06-06 16:43:17.389</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>14327</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>912</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Full Price</td>\n",
       "      <td>[12119, 4507, 8141, 98, 22599]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117</td>\n",
       "      <td>2024-06-08</td>\n",
       "      <td>2024-06-08 15:11:02.782</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>38422</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>10370</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117</td>\n",
       "      <td>2024-06-08</td>\n",
       "      <td>2024-06-08 15:11:44.797</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>19763</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>6500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Full Price</td>\n",
       "      <td>[5013, -30211, 15224, 22437, 8102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>579</td>\n",
       "      <td>2024-06-05</td>\n",
       "      <td>2024-06-05 19:24:48.397</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>30253</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>15563</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Full Price</td>\n",
       "      <td>[14762, 12327, 11517, 6656, 21046]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1220</td>\n",
       "      <td>2024-06-04</td>\n",
       "      <td>2024-06-04 08:21:13.476</td>\n",
       "      <td>0</td>\n",
       "      <td>480729</td>\n",
       "      <td>25</td>\n",
       "      <td>1592</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>24918</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>Full Price</td>\n",
       "      <td>[1592, -27649, 14366, -22454, -28731]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id       date         timestamp_local  add_to_cart  user_id  \\\n",
       "0          64 2024-06-06 2024-06-06 16:43:17.389            0       -1   \n",
       "1         117 2024-06-08 2024-06-08 15:11:02.782            0       -1   \n",
       "2         117 2024-06-08 2024-06-08 15:11:44.797            0       -1   \n",
       "3         579 2024-06-05 2024-06-05 19:24:48.397            0       -1   \n",
       "4        1220 2024-06-04 2024-06-04 08:21:13.476            0   480729   \n",
       "\n",
       "   country  partnumber  device_type  pagetype  hour  day_of_week  week  \\\n",
       "0       29       14327            1        24    16            3    23   \n",
       "1       57       38422            1        24    15            5    23   \n",
       "2       57       19763            1        24    15            5    23   \n",
       "3       29       30253            1        24    19            2    23   \n",
       "4       25        1592            1        24     8            1    23   \n",
       "\n",
       "   popularity  session_interactions  cluster discount_category  \\\n",
       "0         912                     1        5        Full Price   \n",
       "1       10370                     2       -1           Unknown   \n",
       "2        6500                     2        0        Full Price   \n",
       "3       15563                     1        6        Full Price   \n",
       "4       24918                     3       19        Full Price   \n",
       "\n",
       "                        similar_products  \n",
       "0         [12119, 4507, 8141, 98, 22599]  \n",
       "1                                     []  \n",
       "2     [5013, -30211, 15224, 22437, 8102]  \n",
       "3     [14762, 12327, 11517, 6656, 21046]  \n",
       "4  [1592, -27649, 14366, -22454, -28731]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_enriched.pkl'\n",
    "\n",
    "train_df = pd.read_pickle(train_path)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          session_id       date         timestamp_local  add_to_cart  user_id  \\\n",
      "46551440     5170695 2024-06-07 2024-06-07 17:57:24.644            0       -1   \n",
      "46551441     5171109 2024-06-04 2024-06-04 12:34:05.430            0       -1   \n",
      "46551442     5171307 2024-06-07 2024-06-07 07:50:02.549            0       -1   \n",
      "46551443     5171603 2024-06-13 2024-06-13 17:56:44.477            0       -1   \n",
      "46551444     5171698 2024-06-11 2024-06-11 03:05:21.361            0       -1   \n",
      "\n",
      "          country  partnumber  device_type  pagetype  hour  ...  week  \\\n",
      "46551440       34       39901            3        24    17  ...    23   \n",
      "46551441       29       38638            1        24    12  ...    23   \n",
      "46551442       25       10883            1        24     7  ...    23   \n",
      "46551443       29        3769            1        24    17  ...    24   \n",
      "46551444       34       37323            1        24     3  ...    24   \n",
      "\n",
      "          session_interactions  discount_category  \\\n",
      "46551440              -2.07083            Unknown   \n",
      "46551441              -2.07083            Unknown   \n",
      "46551442              -2.07083         Full Price   \n",
      "46551443              -2.07083         Full Price   \n",
      "46551444              -2.07083            Unknown   \n",
      "\n",
      "                              similar_products color_id  family  cod_section  \\\n",
      "46551440                                    []      180      95            2   \n",
      "46551441                                    []       82      96            0   \n",
      "46551442  [14417, 27220, 23230, 24063, -24507]      470     155            0   \n",
      "46551443      [26865, 9245, 1482, 20716, 4431]      378     145            0   \n",
      "46551444                                    []       84      50            3   \n",
      "\n",
      "          discount  popularity  cluster  \n",
      "46551440         0    6.783325       16  \n",
      "46551441         0    9.513256        8  \n",
      "46551442         0    9.530683        8  \n",
      "46551443         0    8.206311        4  \n",
      "46551444         0    7.487174       12  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "   session_id       date         timestamp_local  add_to_cart  user_id  \\\n",
      "0          64 2024-06-06 2024-06-06 16:43:17.389            0       -1   \n",
      "1         117 2024-06-08 2024-06-08 15:11:02.782            0       -1   \n",
      "2         117 2024-06-08 2024-06-08 15:11:44.797            0       -1   \n",
      "3         579 2024-06-05 2024-06-05 19:24:48.397            0       -1   \n",
      "4        1220 2024-06-04 2024-06-04 08:21:13.476            0   480729   \n",
      "\n",
      "   country  partnumber  device_type  pagetype  hour  ...  week  \\\n",
      "0       29       14327            1        24    16  ...    23   \n",
      "1       57       38422            1        24    15  ...    23   \n",
      "2       57       19763            1        24    15  ...    23   \n",
      "3       29       30253            1        24    19  ...    23   \n",
      "4       25        1592            1        24     8  ...    23   \n",
      "\n",
      "   session_interactions  discount_category  \\\n",
      "0             -2.070830         Full Price   \n",
      "1             -1.758499            Unknown   \n",
      "2             -1.758499         Full Price   \n",
      "3             -2.070830         Full Price   \n",
      "4             -1.536897         Full Price   \n",
      "\n",
      "                        similar_products color_id  family  cod_section  \\\n",
      "0         [12119, 4507, 8141, 98, 22599]       73      21            0   \n",
      "1                                     []        2      72            0   \n",
      "2     [5013, -30211, 15224, 22437, 8102]        2     155            0   \n",
      "3     [14762, 12327, 11517, 6656, 21046]      180      52            0   \n",
      "4  [1592, -27649, 14366, -22454, -28731]       84      72            0   \n",
      "\n",
      "   discount  popularity  cluster  \n",
      "0         0    6.816736        7  \n",
      "1         0    9.246769       17  \n",
      "2         0    8.779711        0  \n",
      "3         0    9.652716       18  \n",
      "4         0   10.123386        6  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46551445 entries, 0 to 46551444\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Dtype         \n",
      "---  ------                -----         \n",
      " 0   session_id            int32         \n",
      " 1   date                  datetime64[ns]\n",
      " 2   timestamp_local       datetime64[ns]\n",
      " 3   add_to_cart           int8          \n",
      " 4   user_id               int32         \n",
      " 5   country               int16         \n",
      " 6   partnumber            int32         \n",
      " 7   device_type           int8          \n",
      " 8   pagetype              int16         \n",
      " 9   hour                  int32         \n",
      " 10  day_of_week           int32         \n",
      " 11  week                  UInt32        \n",
      " 12  session_interactions  float32       \n",
      " 13  discount_category     object        \n",
      " 14  similar_products      object        \n",
      " 15  color_id              int16         \n",
      " 16  family                int16         \n",
      " 17  cod_section           int8          \n",
      " 18  discount              int8          \n",
      " 19  popularity            float64       \n",
      " 20  cluster               int32         \n",
      "dtypes: UInt32(1), datetime64[ns](2), float32(1), float64(1), int16(4), int32(6), int8(4), object(2)\n",
      "memory usage: 3.7+ GB\n",
      "None\n",
      "   session_id       date         timestamp_local  user_id  country  \\\n",
      "0         746 2024-06-15 2024-06-15 18:36:47.390       -1       57   \n",
      "1         746 2024-06-15 2024-06-15 18:37:04.052       -1       57   \n",
      "2         746 2024-06-15 2024-06-15 18:37:48.159       -1       57   \n",
      "3         746 2024-06-15 2024-06-15 18:38:19.899       -1       57   \n",
      "4         746 2024-06-15 2024-06-15 18:38:46.492       -1       57   \n",
      "\n",
      "   partnumber  device_type  pagetype  \n",
      "0        1254            1        24  \n",
      "1       32544            1        24  \n",
      "2       12639            1        24  \n",
      "3       18048            1        24  \n",
      "4       13295            1        24  \n",
      "       session_id       date         timestamp_local  user_id  country  \\\n",
      "29270     5167195 2024-06-16 2024-06-16 13:13:10.152       -1       34   \n",
      "29271     5167272 2024-06-16 2024-06-16 18:58:15.562       -1       34   \n",
      "29272     5168338 2024-06-15 2024-06-15 21:36:15.931       -1       34   \n",
      "29273     5168492 2024-06-16 2024-06-16 04:06:03.528       -1       34   \n",
      "29274     5168933 2024-06-16 2024-06-16 04:53:26.427       -1       57   \n",
      "\n",
      "       partnumber  device_type  pagetype  \n",
      "29270       30113            1        24  \n",
      "29271       28922            1         8  \n",
      "29272       13467            1        24  \n",
      "29273       13732            1        24  \n",
      "29274        8969            1        24  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29275 entries, 0 to 29274\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   session_id       29275 non-null  int32         \n",
      " 1   date             29275 non-null  datetime64[ns]\n",
      " 2   timestamp_local  29275 non-null  datetime64[ns]\n",
      " 3   user_id          29275 non-null  int32         \n",
      " 4   country          29275 non-null  int16         \n",
      " 5   partnumber       29275 non-null  int32         \n",
      " 6   device_type      29275 non-null  int8          \n",
      " 7   pagetype         29275 non-null  int16         \n",
      "dtypes: datetime64[ns](2), int16(2), int32(3), int8(1)\n",
      "memory usage: 943.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl'\n",
    "\n",
    "train_df = pd.read_pickle(train_path)\n",
    "\n",
    "print(train_df.tail())\n",
    "print(train_df.head())\n",
    "print(train_df.info())\n",
    "\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "\n",
    "test_df = pd.read_pickle(test_path)\n",
    "\n",
    "print(test_df.head())\n",
    "print(test_df.tail())\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp_local</th>\n",
       "      <th>add_to_cart</th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "      <th>partnumber</th>\n",
       "      <th>device_type</th>\n",
       "      <th>pagetype</th>\n",
       "      <th>hour</th>\n",
       "      <th>...</th>\n",
       "      <th>week</th>\n",
       "      <th>session_interactions</th>\n",
       "      <th>discount_category</th>\n",
       "      <th>similar_products</th>\n",
       "      <th>color_id</th>\n",
       "      <th>family</th>\n",
       "      <th>cod_section</th>\n",
       "      <th>discount</th>\n",
       "      <th>popularity</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>2024-06-06</td>\n",
       "      <td>2024-06-06 16:43:17.389</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>14327</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>-2.070830</td>\n",
       "      <td>Full Price</td>\n",
       "      <td>[12119, 4507, 8141, 98, 22599]</td>\n",
       "      <td>73</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.816736</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117</td>\n",
       "      <td>2024-06-08</td>\n",
       "      <td>2024-06-08 15:11:02.782</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>38422</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.758499</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.246769</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117</td>\n",
       "      <td>2024-06-08</td>\n",
       "      <td>2024-06-08 15:11:44.797</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>19763</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.758499</td>\n",
       "      <td>Full Price</td>\n",
       "      <td>[5013, -30211, 15224, 22437, 8102]</td>\n",
       "      <td>2</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.779711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>579</td>\n",
       "      <td>2024-06-05</td>\n",
       "      <td>2024-06-05 19:24:48.397</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>30253</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>-2.070830</td>\n",
       "      <td>Full Price</td>\n",
       "      <td>[14762, 12327, 11517, 6656, 21046]</td>\n",
       "      <td>180</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.652716</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1220</td>\n",
       "      <td>2024-06-04</td>\n",
       "      <td>2024-06-04 08:21:13.476</td>\n",
       "      <td>0</td>\n",
       "      <td>480729</td>\n",
       "      <td>25</td>\n",
       "      <td>1592</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.536897</td>\n",
       "      <td>Full Price</td>\n",
       "      <td>[1592, -27649, 14366, -22454, -28731]</td>\n",
       "      <td>84</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.123386</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id       date         timestamp_local  add_to_cart  user_id  \\\n",
       "0          64 2024-06-06 2024-06-06 16:43:17.389            0       -1   \n",
       "1         117 2024-06-08 2024-06-08 15:11:02.782            0       -1   \n",
       "2         117 2024-06-08 2024-06-08 15:11:44.797            0       -1   \n",
       "3         579 2024-06-05 2024-06-05 19:24:48.397            0       -1   \n",
       "4        1220 2024-06-04 2024-06-04 08:21:13.476            0   480729   \n",
       "\n",
       "   country  partnumber  device_type  pagetype  hour  ...  week  \\\n",
       "0       29       14327            1        24    16  ...    23   \n",
       "1       57       38422            1        24    15  ...    23   \n",
       "2       57       19763            1        24    15  ...    23   \n",
       "3       29       30253            1        24    19  ...    23   \n",
       "4       25        1592            1        24     8  ...    23   \n",
       "\n",
       "   session_interactions  discount_category  \\\n",
       "0             -2.070830         Full Price   \n",
       "1             -1.758499            Unknown   \n",
       "2             -1.758499         Full Price   \n",
       "3             -2.070830         Full Price   \n",
       "4             -1.536897         Full Price   \n",
       "\n",
       "                        similar_products color_id  family  cod_section  \\\n",
       "0         [12119, 4507, 8141, 98, 22599]       73      21            0   \n",
       "1                                     []        2      72            0   \n",
       "2     [5013, -30211, 15224, 22437, 8102]        2     155            0   \n",
       "3     [14762, 12327, 11517, 6656, 21046]      180      52            0   \n",
       "4  [1592, -27649, 14366, -22454, -28731]       84      72            0   \n",
       "\n",
       "   discount  popularity  cluster  \n",
       "0         0    6.816736        7  \n",
       "1         0    9.246769       17  \n",
       "2         0    8.779711        0  \n",
       "3         0    9.652716       18  \n",
       "4         0   10.123386        6  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46551445 entries, 0 to 46551444\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Dtype         \n",
      "---  ------                -----         \n",
      " 0   session_id            int32         \n",
      " 1   date                  datetime64[ns]\n",
      " 2   timestamp_local       datetime64[ns]\n",
      " 3   add_to_cart           int8          \n",
      " 4   user_id               int32         \n",
      " 5   country               int16         \n",
      " 6   partnumber            int32         \n",
      " 7   device_type           int8          \n",
      " 8   pagetype              int16         \n",
      " 9   hour                  int32         \n",
      " 10  day_of_week           int32         \n",
      " 11  week                  UInt32        \n",
      " 12  session_interactions  float32       \n",
      " 13  discount_category     object        \n",
      " 14  similar_products      object        \n",
      " 15  color_id              int16         \n",
      " 16  family                int16         \n",
      " 17  cod_section           int8          \n",
      " 18  discount              int8          \n",
      " 19  popularity            float64       \n",
      " 20  cluster               int32         \n",
      "dtypes: UInt32(1), datetime64[ns](2), float32(1), float64(1), int16(4), int32(6), int8(4), object(2)\n",
      "memory usage: 3.7+ GB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp_local</th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "      <th>partnumber</th>\n",
       "      <th>device_type</th>\n",
       "      <th>pagetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>746</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>2024-06-15 18:36:47.390</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>1254</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>746</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>2024-06-15 18:37:04.052</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>32544</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>746</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>2024-06-15 18:37:48.159</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>12639</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>746</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>2024-06-15 18:38:19.899</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>18048</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>746</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>2024-06-15 18:38:46.492</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>13295</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id       date         timestamp_local  user_id  country  \\\n",
       "0         746 2024-06-15 2024-06-15 18:36:47.390       -1       57   \n",
       "1         746 2024-06-15 2024-06-15 18:37:04.052       -1       57   \n",
       "2         746 2024-06-15 2024-06-15 18:37:48.159       -1       57   \n",
       "3         746 2024-06-15 2024-06-15 18:38:19.899       -1       57   \n",
       "4         746 2024-06-15 2024-06-15 18:38:46.492       -1       57   \n",
       "\n",
       "   partnumber  device_type  pagetype  \n",
       "0        1254            1        24  \n",
       "1       32544            1        24  \n",
       "2       12639            1        24  \n",
       "3       18048            1        24  \n",
       "4       13295            1        24  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "\n",
    "test_df = pd.read_pickle(test_path)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29275 entries, 0 to 29274\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   session_id       29275 non-null  int32         \n",
      " 1   date             29275 non-null  datetime64[ns]\n",
      " 2   timestamp_local  29275 non-null  datetime64[ns]\n",
      " 3   user_id          29275 non-null  int32         \n",
      " 4   country          29275 non-null  int16         \n",
      " 5   partnumber       29275 non-null  int32         \n",
      " 6   device_type      29275 non-null  int8          \n",
      " 7   pagetype         29275 non-null  int16         \n",
      "dtypes: datetime64[ns](2), int16(2), int32(3), int8(1)\n",
      "memory usage: 943.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generando estadísticas globales del entrenamiento ---\n",
      "{'product_popularity': partnumber\n",
      "1           5\n",
      "2           7\n",
      "3        2289\n",
      "4        1634\n",
      "5         687\n",
      "         ... \n",
      "43688     231\n",
      "43689      13\n",
      "43690     210\n",
      "43691       5\n",
      "43692     344\n",
      "Name: session_id, Length: 41995, dtype: int64, 'discount_category': {'Full Price': 0.7094047456529008, 'Unknown': 0.23909893667103138, 'Discounted': 0.05149631767606784, 'default': 'Full Price'}, 'cluster': {18: 0.13827850026997013, 0: 0.09784218298701576, 12: 0.08534261396182224, 3: 0.07682414584552638, 2: 0.06590768557238126, 6: 0.05822689284940564, 16: 0.05331697007472056, 11: 0.050514586604132265, 13: 0.0503815509915965, 8: 0.048942132730788485, 14: 0.047250971478973426, 5: 0.04457296653197339, 10: 0.04112686512738756, 4: 0.04096347599950979, 17: 0.03720129418109363, 1: 0.037083617919916344, 19: 0.010216847189168887, 15: 0.008749223574048023, 7: 0.005925122195454943, 9: 0.0013323539151147725, 'default': 18}}\n",
      "\n",
      "--- Cargando datos de entrenamiento enriquecidos ---\n",
      "\n",
      "--- Preprocesando y generando características compartidas (train) ---\n",
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Preprocesando conjunto de prueba con enriquecimiento ---\n",
      "Agregando columna faltante: week\n",
      "Agregando columna faltante: session_interactions\n",
      "Agregando columna faltante: color_id\n",
      "Agregando columna faltante: family\n",
      "Agregando columna faltante: cod_section\n",
      "Agregando columna faltante: discount\n",
      "Agregando columna faltante: popularity\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Columnas con tipos de datos no válidos: country               int16\ndevice_type            int8\npagetype              int16\ndiscount_category    object\ndtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 202\u001b[0m\n\u001b[1;32m    199\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Generar predicciones\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m \u001b[43mgenerate_predictions_with_enrichment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Validar formato del JSON\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[1], line 149\u001b[0m, in \u001b[0;36mgenerate_predictions_with_enrichment\u001b[0;34m(model_path, test_path, output_path, global_stats, train_features)\u001b[0m\n\u001b[1;32m    147\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mBooster(model_file\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[1;32m    148\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(test_path)\n\u001b[0;32m--> 149\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_test_with_enrichment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m predictions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    152\u001b[0m popular_products \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartnumber\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n",
      "Cell \u001b[0;32mIn[1], line 106\u001b[0m, in \u001b[0;36mpreprocess_test_with_enrichment\u001b[0;34m(test_df, global_stats, train_features)\u001b[0m\n\u001b[1;32m    104\u001b[0m invalid_dtypes \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdtypes[\u001b[38;5;241m~\u001b[39mtest_df\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39misin([np\u001b[38;5;241m.\u001b[39mint32, np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mbool_])]\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m invalid_dtypes\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnas con tipos de datos no válidos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_dtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m train_features]\n",
      "\u001b[0;31mValueError\u001b[0m: Columnas con tipos de datos no válidos: country               int16\ndevice_type            int8\npagetype              int16\ndiscount_category    object\ndtype: object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "\n",
    "def preprocess_and_generate_shared_features(df, mode=\"train\"):\n",
    "    print(f\"\\n--- Preprocesando y generando características compartidas ({mode}) ---\")\n",
    "\n",
    "    # Convertir datetime a numérico (segundos desde el epoch)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).astype(int) / 10**9\n",
    "    if \"timestamp_local\" in df.columns:\n",
    "        df[\"timestamp_local\"] = pd.to_datetime(df[\"timestamp_local\"]).astype(int) / 10**9\n",
    "\n",
    "    # Generar características compartidas\n",
    "    if \"partnumber\" in df.columns:\n",
    "        df[\"product_popularity\"] = df.groupby(\"partnumber\")[\"session_id\"].transform(\"nunique\")\n",
    "    if \"session_id\" in df.columns:\n",
    "        df[\"session_length\"] = df.groupby(\"session_id\")[\"partnumber\"].transform(\"count\")\n",
    "\n",
    "    # Codificar `discount_category` como numérico\n",
    "    if \"discount_category\" in df.columns:\n",
    "        df[\"discount_category\"] = df[\"discount_category\"].astype(\"category\").cat.codes\n",
    "\n",
    "    # Procesar `similar_products` como longitud de lista\n",
    "    if \"similar_products\" in df.columns:\n",
    "        df[\"similar_products_count\"] = df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "\n",
    "    # Manejar columnas no compatibles en modo `test`\n",
    "    if mode == \"test\":\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype not in [np.int32, np.int64, np.float32, np.float64, np.bool_]:\n",
    "                print(f\"Eliminando columna no compatible: {col}\")\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_global_stats(train_df):\n",
    "    print(\"\\n--- Generando estadísticas globales del entrenamiento ---\")\n",
    "    stats = {}\n",
    "    stats['product_popularity'] = train_df.groupby('partnumber')['session_id'].nunique()\n",
    "    stats['discount_category'] = train_df['discount_category'].value_counts(normalize=True).to_dict()\n",
    "    stats['cluster'] = train_df['cluster'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    # Asignar valores por defecto para test\n",
    "    stats['discount_category']['default'] = max(stats['discount_category'], key=stats['discount_category'].get)\n",
    "    stats['cluster']['default'] = max(stats['cluster'], key=stats['cluster'].get)\n",
    "    return stats\n",
    "\n",
    "def preprocess_test_with_enrichment(test_df, global_stats, train_features):\n",
    "    print(\"\\n--- Preprocesando conjunto de prueba con enriquecimiento ---\")\n",
    "\n",
    "    if 'session_id' not in test_df.columns:\n",
    "        raise KeyError(\"El conjunto de prueba no contiene 'session_id' al inicio.\")\n",
    "\n",
    "    # Convertir columnas datetime a numérico\n",
    "    test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "    test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "    test_df['hour'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.hour\n",
    "    test_df['day_of_week'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "\n",
    "    # Generar características derivadas\n",
    "    test_df['session_length'] = test_df.groupby('session_id')['partnumber'].transform('count')\n",
    "    test_df['product_popularity'] = global_stats['product_popularity'].reindex(test_df['partnumber']).fillna(0).astype(float).values\n",
    "\n",
    "    # Manejar columna `discount_category`\n",
    "    if 'discount_category' not in test_df.columns:\n",
    "        test_df['discount_category'] = global_stats['discount_category']['default']\n",
    "    else:\n",
    "        test_df['discount_category'] = (\n",
    "            test_df['discount_category']\n",
    "            .map(global_stats['discount_category'])\n",
    "            .fillna(global_stats['discount_category']['default'])\n",
    "        ).astype(int)\n",
    "\n",
    "    # Manejar otras columnas faltantes o imputadas\n",
    "    if 'similar_products_count' not in test_df.columns:\n",
    "        test_df['similar_products_count'] = 0\n",
    "    if 'cluster' not in test_df.columns:\n",
    "        test_df['cluster'] = global_stats['cluster']['default']\n",
    "\n",
    "    # Agregar columnas faltantes\n",
    "    for feature in train_features:\n",
    "        if feature not in test_df.columns:\n",
    "            print(f\"Agregando columna faltante: {feature}\")\n",
    "            test_df[feature] = 0\n",
    "\n",
    "    # Convertir todas las columnas a tipos compatibles\n",
    "    for col in test_df.columns:\n",
    "        if col not in train_features + ['session_id']:\n",
    "            continue\n",
    "        if test_df[col].dtype == 'int64':\n",
    "            test_df[col] = test_df[col].astype(np.int32)\n",
    "        elif test_df[col].dtype == 'float64':\n",
    "            test_df[col] = test_df[col].astype(np.float32)\n",
    "\n",
    "    # Verificar que todas las columnas en `train_features` están presentes y tienen tipos válidos\n",
    "    missing_features = [f for f in train_features if f not in test_df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Faltan características en el conjunto de prueba: {missing_features}\")\n",
    "\n",
    "    # Validar tipos finales\n",
    "    invalid_dtypes = test_df.dtypes[~test_df.dtypes.isin([np.int32, np.float32, np.bool_])]\n",
    "    if not invalid_dtypes.empty:\n",
    "        raise ValueError(f\"Columnas con tipos de datos no válidos: {invalid_dtypes}\")\n",
    "\n",
    "    return test_df[['session_id'] + train_features]\n",
    "\n",
    "def train_lambdamart_with_enrichment(train_path, model_path):\n",
    "    print(\"\\n--- Cargando datos de entrenamiento enriquecidos ---\")\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    train_df = preprocess_and_generate_shared_features(train_df, mode=\"train\")\n",
    "    if \"add_to_cart\" not in train_df.columns:\n",
    "        raise ValueError(\"La columna 'add_to_cart' no está presente en el conjunto de datos.\")\n",
    "\n",
    "    X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y = train_df['add_to_cart']\n",
    "    groups = train_df['session_id'].value_counts().values\n",
    "\n",
    "    lgb_train = lgb.Dataset(X, label=y, group=groups)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [1, 3, 5],\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 70,\n",
    "        'max_bin': 255,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params, lgb_train,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[lgb_train],\n",
    "        valid_names=['train']\n",
    "    )\n",
    "\n",
    "    model.save_model(model_path)\n",
    "    return X.columns.tolist()\n",
    "\n",
    "def generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_test_with_enrichment(test_df, global_stats, train_features)\n",
    "\n",
    "    predictions = {}\n",
    "    popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "    print(\"\\n--- Generando predicciones para cada sesión ---\")\n",
    "    for session_id in test_df['session_id'].unique():\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl'\n",
    "\n",
    "# Cargar datos de entrenamiento\n",
    "train_df = pd.read_pickle(train_path)\n",
    "\n",
    "# Generar estadísticas globales\n",
    "global_stats = generate_global_stats(train_df)\n",
    "print(global_stats)\n",
    "\n",
    "\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_shared_model.txt'\n",
    "\n",
    "# Entrenar modelo y obtener características de entrenamiento\n",
    "train_features = train_lambdamart_with_enrichment(train_path, model_path)\n",
    "\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared.json'\n",
    "\n",
    "# Generar predicciones\n",
    "generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features)\n",
    "\n",
    "\n",
    "# Validar formato del JSON\n",
    "with open(output_path, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# Mostrar una parte del JSON para verificación\n",
    "print(\"\\n--- Validación del JSON generado ---\")\n",
    "print(json.dumps(predictions, indent=4)[:1000])  # Mostrar los primeros 1000 caracteres\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "datascience chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generando estadísticas globales del entrenamiento ---\n",
      "{'product_popularity': partnumber\n",
      "1           4\n",
      "2           5\n",
      "3        1943\n",
      "4        1359\n",
      "5         559\n",
      "         ... \n",
      "43688     187\n",
      "43689      11\n",
      "43690     179\n",
      "43691       5\n",
      "43692     281\n",
      "Name: session_id, Length: 40932, dtype: int64, 'discount_category': {'Full Price': 0.7093706489669654, 'Unknown': 0.23910962914255401, 'Discounted': 0.05151972189048052, 'default': 'Full Price'}, 'cluster': {18: 0.13828834958828884, 0: 0.09782634029942572, 12: 0.08535110993869256, 3: 0.07684176076596548, 2: 0.06590574148664988, 6: 0.05825281041222243, 16: 0.05331179837704286, 11: 0.050519833487445986, 13: 0.05037190574857558, 8: 0.0489641621221425, 14: 0.047246895343420595, 5: 0.04454252171978765, 10: 0.04112710679550334, 4: 0.0409505548109194, 17: 0.03719715360070992, 1: 0.03706963339161652, 19: 0.010215848294290328, 15: 0.008745620033921611, 7: 0.005935637443692672, 9: 0.0013352163396861258, 'default': 18}, 'discount_category_dtype': None}\n",
      "\n",
      "--- Cargando datos de entrenamiento enriquecidos ---\n",
      "\n",
      "--- Preprocesando y generando características compartidas (train) ---\n",
      "\n",
      "--- Preprocesando conjunto de prueba con enriquecimiento ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1305/2420315363.py:100: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .fillna(global_stats['discount_category']['default'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrigiendo tipo de dato de week de UInt32 a int32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Columnas con tipos de datos no válidos después de la conversión: date                    float32\ntimestamp_local         float32\ncountry                   int16\ndevice_type                int8\npagetype                  int16\nsession_interactions    float32\ncolor_id                  int16\nfamily                    int16\ncod_section                int8\ndiscount                   int8\npopularity              float32\nproduct_popularity      float32\ndtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 304\u001b[0m\n\u001b[1;32m    301\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mBooster(model_file\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Preprocesar datos de validación\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m val_df_processed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_test_with_enrichment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Generar predicciones en el conjunto de validación\u001b[39;00m\n\u001b[1;32m    307\u001b[0m val_predictions \u001b[38;5;241m=\u001b[39m generate_predictions(model, val_df_processed, global_stats, train_features)\n",
      "Cell \u001b[0;32mIn[1], line 134\u001b[0m, in \u001b[0;36mpreprocess_test_with_enrichment\u001b[0;34m(test_df, global_stats, train_features)\u001b[0m\n\u001b[1;32m    132\u001b[0m invalid_dtypes \u001b[38;5;241m=\u001b[39m test_subset\u001b[38;5;241m.\u001b[39mdtypes[\u001b[38;5;241m~\u001b[39mtest_subset\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39misin([np\u001b[38;5;241m.\u001b[39mint32, np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mbool_])]\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m invalid_dtypes\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnas con tipos de datos no válidos después de la conversión: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_dtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_subset\n",
      "\u001b[0;31mValueError\u001b[0m: Columnas con tipos de datos no válidos después de la conversión: date                    float32\ntimestamp_local         float32\ncountry                   int16\ndevice_type                int8\npagetype                  int16\nsession_interactions    float32\ncolor_id                  int16\nfamily                    int16\ncod_section                int8\ndiscount                   int8\npopularity              float32\nproduct_popularity      float32\ndtype: object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_and_generate_shared_features(df, mode=\"train\"):\n",
    "    print(f\"\\n--- Preprocesando y generando características compartidas ({mode}) ---\")\n",
    "\n",
    "    # Convertir datetime a numérico (segundos desde el epoch)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).astype(int) / 10**9\n",
    "    if \"timestamp_local\" in df.columns:\n",
    "        df[\"timestamp_local\"] = pd.to_datetime(df[\"timestamp_local\"]).astype(int) / 10**9\n",
    "\n",
    "    # Generar características compartidas\n",
    "    if \"partnumber\" in df.columns:\n",
    "        df[\"product_popularity\"] = df.groupby(\"partnumber\")[\"session_id\"].transform(\"nunique\")\n",
    "    if \"session_id\" in df.columns:\n",
    "        df[\"session_length\"] = df.groupby(\"session_id\")[\"partnumber\"].transform(\"count\")\n",
    "\n",
    "    # Codificar `discount_category` como numérico y almacenar dtype para consistencia\n",
    "    if \"discount_category\" in df.columns:\n",
    "        cat_dtype = CategoricalDtype(categories=df[\"discount_category\"].unique(), ordered=True)\n",
    "        df[\"discount_category\"] = df[\"discount_category\"].astype(cat_dtype).cat.codes\n",
    "        df.attrs[\"discount_category_dtype\"] = cat_dtype\n",
    "\n",
    "    # Procesar `similar_products` como longitud de lista\n",
    "    if \"similar_products\" in df.columns:\n",
    "        df[\"similar_products_count\"] = df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_global_stats(train_df):\n",
    "    print(\"\\n--- Generando estadísticas globales del entrenamiento ---\")\n",
    "    stats = {}\n",
    "    stats['product_popularity'] = train_df.groupby('partnumber')['session_id'].nunique()\n",
    "    stats['discount_category'] = train_df['discount_category'].value_counts(normalize=True).to_dict()\n",
    "    stats['cluster'] = train_df['cluster'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    # Almacenar dtype categórico de discount_category para reutilizar en test\n",
    "    stats['discount_category_dtype'] = train_df.attrs.get(\"discount_category_dtype\", None)\n",
    "\n",
    "    # Asignar valores por defecto para test\n",
    "    stats['discount_category']['default'] = max(stats['discount_category'], key=stats['discount_category'].get)\n",
    "    stats['cluster']['default'] = max(stats['cluster'], key=stats['cluster'].get)\n",
    "    return stats\n",
    "\n",
    "def preprocess_test_with_enrichment(test_df, global_stats, train_features):\n",
    "    print(\"\\n--- Preprocesando conjunto de prueba con enriquecimiento ---\")\n",
    "\n",
    "    if 'session_id' not in test_df.columns:\n",
    "        raise KeyError(\"El conjunto de prueba no contiene 'session_id' al inicio.\")\n",
    "\n",
    "    # Convertir columnas datetime a numérico\n",
    "    test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "    test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "    # Procesar similar_products si existe\n",
    "    if \"similar_products\" in test_df.columns:\n",
    "        test_df[\"similar_products_count\"] = test_df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        test_df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "    elif 'similar_products_count' not in test_df.columns:\n",
    "        test_df['similar_products_count'] = 0\n",
    "\n",
    "    # Extraer características temporales\n",
    "    test_df['hour'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.hour\n",
    "    test_df['day_of_week'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "\n",
    "    # Generar características derivadas\n",
    "    test_df['session_length'] = test_df.groupby('session_id')['partnumber'].transform('count')\n",
    "    test_df['product_popularity'] = test_df['partnumber'].map(global_stats['product_popularity']).fillna(0).astype(np.float32)\n",
    "\n",
    "    # Manejar columna `discount_category` con dtype consistente\n",
    "    cat_dtype = global_stats.get(\"discount_category_dtype\", None)\n",
    "    if cat_dtype is not None:\n",
    "        if 'discount_category' in test_df.columns:\n",
    "            test_df['discount_category'] = test_df['discount_category'].astype(cat_dtype).cat.codes\n",
    "        else:\n",
    "            default_category = global_stats['discount_category']['default']\n",
    "            test_df['discount_category'] = pd.Series([default_category] * len(test_df), dtype=cat_dtype).cat.codes\n",
    "    else:\n",
    "        test_df['discount_category'] = 0  # Valor predeterminado si no hay dtype\n",
    "\n",
    "    # Agregar columnas faltantes\n",
    "    for feature in train_features:\n",
    "        if feature not in test_df.columns:\n",
    "            print(f\"Agregando columna faltante: {feature}\")\n",
    "            test_df[feature] = 0\n",
    "\n",
    "    # Corrección exhaustiva de tipos\n",
    "    for col in train_features:\n",
    "        if test_df[col].dtype == 'UInt32':\n",
    "            print(f\"Corrigiendo tipo de dato de {col} de UInt32 a int32\")\n",
    "            test_df[col] = test_df[col].astype(np.int32)\n",
    "        elif test_df[col].dtype == 'int64':\n",
    "            test_df[col] = test_df[col].astype(np.int32)\n",
    "        elif test_df[col].dtype == 'float64':\n",
    "            test_df[col] = test_df[col].astype(np.float32)\n",
    "\n",
    "    # Validar que todas las columnas tienen tipos válidos\n",
    "    invalid_dtypes = test_df[train_features].dtypes[~test_df[train_features].dtypes.isin([np.int32, np.float32, np.bool_])]\n",
    "    if not invalid_dtypes.empty:\n",
    "        raise ValueError(f\"Columnas con tipos de datos no válidos después de la conversión: {invalid_dtypes}\")\n",
    "\n",
    "    return test_df[['session_id'] + train_features]\n",
    "\n",
    "def train_lambdamart_with_enrichment(train_path, model_path):\n",
    "    print(\"\\n--- Cargando datos de entrenamiento enriquecidos ---\")\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    train_df = preprocess_and_generate_shared_features(train_df, mode=\"train\")\n",
    "\n",
    "    if \"add_to_cart\" not in train_df.columns:\n",
    "        raise ValueError(\"La columna 'add_to_cart' no está presente en el conjunto de datos.\")\n",
    "\n",
    "    X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y = train_df['add_to_cart']\n",
    "    groups = train_df['session_id'].value_counts().values\n",
    "\n",
    "    lgb_train = lgb.Dataset(X, label=y, group=groups)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [1, 3, 5],\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 70,\n",
    "        'max_bin': 255,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params, lgb_train,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[lgb_train],\n",
    "        valid_names=['train']\n",
    "    )\n",
    "\n",
    "    model.save_model(model_path)\n",
    "    return X.columns.tolist()\n",
    "\n",
    "def generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_test_with_enrichment(test_df, global_stats, train_features)\n",
    "\n",
    "    predictions = {}\n",
    "    if 'partnumber' in test_df.columns:\n",
    "        popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    print(\"\\n--- Generando predicciones para cada sesión ---\")\n",
    "    for session_id in test_df['session_id'].unique():\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "\n",
    "def compute_metrics(predictions, ground_truth, k=5):\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for session_id, predicted in predictions.items():\n",
    "        true_products = ground_truth.get(session_id, set())\n",
    "        if not true_products:\n",
    "            continue\n",
    "\n",
    "        pred_set = set(predicted[:k])\n",
    "        precision = len(pred_set & true_products) / k\n",
    "        recall = len(pred_set & true_products) / len(true_products)\n",
    "\n",
    "        dcg = 0.0\n",
    "        for i, prod in enumerate(predicted[:k]):\n",
    "            rel = 1 if prod in true_products else 0\n",
    "            dcg += (2**rel - 1) / np.log2(i + 2)\n",
    "        ideal_rels = [1] * min(len(true_products), k) + [0] * (k - min(len(true_products), k))\n",
    "        idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_rels))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    avg_ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "\n",
    "    return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "def generate_predictions(model, df, global_stats, train_features):\n",
    "    predictions = {}\n",
    "    if 'partnumber' in df.columns:\n",
    "        popular_products = df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    for session_id in df['session_id'].unique():\n",
    "        session_data = df[df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "    return predictions\n",
    "\n",
    "# Rutas de archivos\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl'\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_shared_model.txt'\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared.json'\n",
    "\n",
    "# Cargar datos de entrenamiento completos\n",
    "full_train_df = pd.read_pickle(train_path)\n",
    "\n",
    "# Dividir en entrenamiento y validación\n",
    "train_df, val_df = train_test_split(full_train_df, test_size=0.2, random_state=42, stratify=full_train_df['add_to_cart'])\n",
    "\n",
    "\n",
    "# Generar estadísticas globales usando todo el conjunto de entrenamiento o solo la parte de entrenamiento\n",
    "global_stats = generate_global_stats(train_df)\n",
    "print(global_stats)\n",
    "\n",
    "# Guardar subconjunto de entrenamiento para utilizar la función existente\n",
    "train_subset_path = '/tmp/train_subset.pkl'\n",
    "train_df.to_pickle(train_subset_path)\n",
    "\n",
    "# Entrenar modelo y obtener características de entrenamiento usando el subconjunto\n",
    "train_features = train_lambdamart_with_enrichment(train_subset_path, model_path)\n",
    "\n",
    "# Cargar el modelo entrenado para usarlo en predicciones\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# Preprocesar datos de validación\n",
    "val_df_processed = preprocess_test_with_enrichment(val_df.copy(), global_stats, train_features)\n",
    "\n",
    "# Generar predicciones en el conjunto de validación\n",
    "val_predictions = generate_predictions(model, val_df_processed, global_stats, train_features)\n",
    "\n",
    "# Preparar ground truth para el conjunto de validación\n",
    "ground_truth = {}\n",
    "for _, row in val_df.iterrows():\n",
    "    if row['add_to_cart'] == 1:\n",
    "        sid = str(row['session_id'])\n",
    "        ground_truth.setdefault(sid, set()).add(row['partnumber'])\n",
    "\n",
    "# Calcular métricas en el conjunto de validación\n",
    "avg_precision, avg_recall, avg_ndcg = compute_metrics(val_predictions, ground_truth, k=5)\n",
    "print(f\"Precision@5: {avg_precision:.4f}, Recall@5: {avg_recall:.4f}, NDCG@5: {avg_ndcg:.4f}\")\n",
    "\n",
    "# Generar predicciones para el conjunto de prueba final (si es necesario)\n",
    "generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features)\n",
    "\n",
    "# Validar formato del JSON generado\n",
    "with open(output_path, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "print(\"\\n--- Validación del JSON generado ---\")\n",
    "print(json.dumps(predictions, indent=4)[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chat 01-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_and_generate_shared_features(df, mode=\"train\"):\n",
    "    print(f\"\\n--- Preprocesando y generando características compartidas ({mode}) ---\")\n",
    "\n",
    "    # Convertir datetime a numérico (segundos desde el epoch)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).astype(int) / 10**9\n",
    "    if \"timestamp_local\" in df.columns:\n",
    "        df[\"timestamp_local\"] = pd.to_datetime(df[\"timestamp_local\"]).astype(int) / 10**9\n",
    "\n",
    "    # Generar características compartidas\n",
    "    if \"partnumber\" in df.columns:\n",
    "        df[\"product_popularity\"] = df.groupby(\"partnumber\")[\"session_id\"].transform(\"nunique\")\n",
    "    if \"session_id\" in df.columns:\n",
    "        df[\"session_length\"] = df.groupby(\"session_id\")[\"partnumber\"].transform(\"count\")\n",
    "\n",
    "    # Codificar `discount_category` como numérico y almacenar dtype para consistencia\n",
    "    if \"discount_category\" in df.columns:\n",
    "        cat_dtype = df[\"discount_category\"].astype(\"category\").dtype\n",
    "        df[\"discount_category\"] = df[\"discount_category\"].astype(cat_dtype).cat.codes\n",
    "        df.attrs[\"discount_category_dtype\"] = cat_dtype\n",
    "\n",
    "    # Procesar `similar_products` como longitud de lista\n",
    "    if \"similar_products\" in df.columns:\n",
    "        df[\"similar_products_count\"] = df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "\n",
    "    # Manejar columnas no compatibles en modo `test`\n",
    "    if mode == \"test\":\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype not in [np.int32, np.int64, np.float32, np.float64, np.bool_]:\n",
    "                print(f\"Eliminando columna no compatible: {col}\")\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_global_stats(train_df):\n",
    "    print(\"\\n--- Generando estadísticas globales del entrenamiento ---\")\n",
    "    stats = {}\n",
    "    stats['product_popularity'] = train_df.groupby('partnumber')['session_id'].nunique()\n",
    "    stats['discount_category'] = train_df['discount_category'].value_counts(normalize=True).to_dict()\n",
    "    stats['cluster'] = train_df['cluster'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    # Almacenar dtype categórico de discount_category para reutilizar en test\n",
    "    stats['discount_category_dtype'] = train_df.attrs.get(\"discount_category_dtype\", None)\n",
    "\n",
    "    # Asignar valores por defecto para test\n",
    "    stats['discount_category']['default'] = max(stats['discount_category'], key=stats['discount_category'].get)\n",
    "    stats['cluster']['default'] = max(stats['cluster'], key=stats['cluster'].get)\n",
    "    return stats\n",
    "\n",
    "def preprocess_test_with_enrichment(test_df, global_stats, train_features):\n",
    "    print(\"\\n--- Preprocesando conjunto de prueba con enriquecimiento ---\")\n",
    "\n",
    "    if 'session_id' not in test_df.columns:\n",
    "        raise KeyError(\"El conjunto de prueba no contiene 'session_id' al inicio.\")\n",
    "\n",
    "    # Convertir columnas datetime a numérico\n",
    "    test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "    test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "    # Procesar similar_products si existe\n",
    "    if \"similar_products\" in test_df.columns:\n",
    "        test_df[\"similar_products_count\"] = test_df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        test_df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "    elif 'similar_products_count' not in test_df.columns:\n",
    "        test_df['similar_products_count'] = 0\n",
    "\n",
    "    # Extraer características temporales\n",
    "    test_df['hour'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.hour\n",
    "    test_df['day_of_week'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "\n",
    "    # Generar características derivadas\n",
    "    test_df['session_length'] = test_df.groupby('session_id')['partnumber'].transform('count')\n",
    "    test_df['product_popularity'] = test_df['partnumber'].map(global_stats['product_popularity']).fillna(0).astype(float)\n",
    "\n",
    "    # Manejar columna `discount_category` con dtype consistente\n",
    "    cat_dtype = global_stats.get(\"discount_category_dtype\", None)\n",
    "    if cat_dtype is not None:\n",
    "        if 'discount_category' in test_df.columns:\n",
    "            test_df['discount_category'] = test_df['discount_category'].astype(cat_dtype).cat.codes\n",
    "        else:\n",
    "            default_category = global_stats['discount_category']['default']\n",
    "            temp_series = pd.Series([default_category], dtype=cat_dtype)\n",
    "            default_code = temp_series.cat.codes.iloc[0]\n",
    "            test_df['discount_category'] = default_code\n",
    "    else:\n",
    "        if 'discount_category' not in test_df.columns:\n",
    "            test_df['discount_category'] = global_stats['discount_category']['default']\n",
    "        else:\n",
    "            test_df['discount_category'] = (\n",
    "                test_df['discount_category']\n",
    "                .map(global_stats['discount_category'])\n",
    "                .fillna(global_stats['discount_category']['default'])\n",
    "            ).astype(int)\n",
    "\n",
    "    if 'cluster' not in test_df.columns:\n",
    "        test_df['cluster'] = global_stats['cluster']['default']\n",
    "\n",
    "    for feature in train_features:\n",
    "        if feature not in test_df.columns:\n",
    "            print(f\"Agregando columna faltante: {feature}\")\n",
    "            test_df[feature] = 0\n",
    "\n",
    "    for col in ['session_id'] + train_features:\n",
    "        if col in test_df.columns:\n",
    "            if test_df[col].dtype == 'int64':\n",
    "                test_df[col] = test_df[col].astype(np.int32)\n",
    "            elif test_df[col].dtype == 'float64':\n",
    "                test_df[col] = test_df[col].astype(np.float32)\n",
    "\n",
    "    missing_features = [f for f in train_features if f not in test_df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Faltan características en el conjunto de prueba: {missing_features}\")\n",
    "\n",
    "    valid_cols = ['session_id'] + train_features\n",
    "    test_subset = test_df[valid_cols]\n",
    "\n",
    "    # Ampliar la lista de tipos permitidos\n",
    "    allowed_types = {\n",
    "        np.dtype('int32'),\n",
    "        np.dtype('float32'),\n",
    "        np.dtype('bool_'),\n",
    "        np.dtype('int16'),\n",
    "        np.dtype('int8'),\n",
    "        np.dtype('uint32'),\n",
    "        pd.UInt32Dtype()\n",
    "    }\n",
    "\n",
    "    # Verificar individualmente los tipos de cada columna\n",
    "    invalid_columns = {col: dt for col, dt in test_subset.dtypes.items() if dt not in allowed_types}\n",
    "    if invalid_columns:\n",
    "        raise ValueError(f\"Columnas con tipos de datos no válidos: {invalid_columns}\")\n",
    "\n",
    "    return test_subset\n",
    "\n",
    "def train_lambdamart_with_enrichment(train_path, model_path):\n",
    "    print(\"\\n--- Cargando datos de entrenamiento enriquecidos ---\")\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    train_df = preprocess_and_generate_shared_features(train_df, mode=\"train\")\n",
    "\n",
    "    if \"add_to_cart\" not in train_df.columns:\n",
    "        raise ValueError(\"La columna 'add_to_cart' no está presente en el conjunto de datos.\")\n",
    "\n",
    "    X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y = train_df['add_to_cart']\n",
    "    groups = train_df['session_id'].value_counts().values\n",
    "\n",
    "    lgb_train = lgb.Dataset(X, label=y, group=groups)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [1, 3, 5],\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 70,\n",
    "        'max_bin': 255,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params, lgb_train,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[lgb_train],\n",
    "        valid_names=['train']\n",
    "    )\n",
    "\n",
    "    model.save_model(model_path)\n",
    "    return X.columns.tolist()\n",
    "\n",
    "def generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_test_with_enrichment(test_df, global_stats, train_features)\n",
    "\n",
    "    predictions = {}\n",
    "    if 'partnumber' in test_df.columns:\n",
    "        popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    print(\"\\n--- Generando predicciones para cada sesión ---\")\n",
    "    for session_id in test_df['session_id'].unique():\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "\n",
    "def compute_metrics(predictions, ground_truth, k=5):\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for session_id, predicted in predictions.items():\n",
    "        true_products = ground_truth.get(session_id, set())\n",
    "        if not true_products:\n",
    "            continue\n",
    "\n",
    "        pred_set = set(predicted[:k])\n",
    "        precision = len(pred_set & true_products) / k\n",
    "        recall = len(pred_set & true_products) / len(true_products)\n",
    "\n",
    "        dcg = 0.0\n",
    "        for i, prod in enumerate(predicted[:k]):\n",
    "            rel = 1 if prod in true_products else 0\n",
    "            dcg += (2**rel - 1) / np.log2(i + 2)\n",
    "        ideal_rels = [1] * min(len(true_products), k) + [0] * (k - min(len(true_products), k))\n",
    "        idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_rels))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    avg_ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "\n",
    "    return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "def generate_predictions(model, df, global_stats, train_features):\n",
    "    predictions = {}\n",
    "    if 'partnumber' in df.columns:\n",
    "        popular_products = df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    for session_id in df['session_id'].unique():\n",
    "        session_data = df[df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "    return predictions\n",
    "\n",
    "# Rutas de archivos\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl'\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_shared_model_stratify.txt'\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared_stratify.json'\n",
    "\n",
    "# Cargar datos de entrenamiento completos\n",
    "full_train_df = pd.read_pickle(train_path)\n",
    "\n",
    "# Dividir en entrenamiento y validación\n",
    "train_df, val_df = train_test_split(full_train_df, test_size=0.2, random_state=42, stratify=full_train_df['add_to_cart'])\n",
    "\n",
    "# Generar estadísticas globales usando todo el conjunto de entrenamiento o solo la parte de entrenamiento\n",
    "global_stats = generate_global_stats(train_df)\n",
    "print(global_stats)\n",
    "\n",
    "# Guardar subconjunto de entrenamiento para utilizar la función existente\n",
    "train_subset_path = '/tmp/train_subset.pkl'\n",
    "train_df.to_pickle(train_subset_path)\n",
    "\n",
    "# Entrenar modelo y obtener características de entrenamiento usando el subconjunto\n",
    "train_features = train_lambdamart_with_enrichment(train_subset_path, model_path)\n",
    "\n",
    "# Cargar el modelo entrenado para usarlo en predicciones\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# Preprocesar datos de validación\n",
    "val_df_processed = preprocess_test_with_enrichment(val_df.copy(), global_stats, train_features)\n",
    "\n",
    "# Generar predicciones en el conjunto de validación\n",
    "val_predictions = generate_predictions(model, val_df_processed, global_stats, train_features)\n",
    "\n",
    "# Preparar ground truth para el conjunto de validación\n",
    "ground_truth = {}\n",
    "for _, row in val_df.iterrows():\n",
    "    if row['add_to_cart'] == 1:\n",
    "        sid = str(row['session_id'])\n",
    "        ground_truth.setdefault(sid, set()).add(row['partnumber'])\n",
    "\n",
    "# Calcular métricas en el conjunto de validación\n",
    "avg_precision, avg_recall, avg_ndcg = compute_metrics(val_predictions, ground_truth, k=5)\n",
    "print(f\"Precision@5: {avg_precision:.4f}, Recall@5: {avg_recall:.4f}, NDCG@5: {avg_ndcg:.4f}\")\n",
    "\n",
    "# Generar predicciones para el conjunto de prueba final (si es necesario)\n",
    "generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features)\n",
    "\n",
    "# Validar formato del JSON generado\n",
    "with open(output_path, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "print(\"\\n--- Validación del JSON generado ---\")\n",
    "print(json.dumps(predictions, indent=4)[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuración para silenciar advertencias de downcasting\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "def preprocess_and_generate_shared_features(df, mode=\"train\"):\n",
    "    print(f\"\\n--- Preprocesando y generando características compartidas ({mode}) ---\")\n",
    "\n",
    "    # Convertir datetime a numérico (segundos desde el epoch)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).astype(int) / 10**9\n",
    "    if \"timestamp_local\" in df.columns:\n",
    "        df[\"timestamp_local\"] = pd.to_datetime(df[\"timestamp_local\"]).astype(int) / 10**9\n",
    "\n",
    "    # Generar características compartidas\n",
    "    if \"partnumber\" in df.columns:\n",
    "        df[\"product_popularity\"] = df.groupby(\"partnumber\")[\"session_id\"].transform(\"nunique\")\n",
    "    if \"session_id\" in df.columns:\n",
    "        df[\"session_length\"] = df.groupby(\"session_id\")[\"partnumber\"].transform(\"count\")\n",
    "\n",
    "    # Utilizar variables categóricas directamente para discount_category\n",
    "    if \"discount_category\" in df.columns:\n",
    "        df[\"discount_category\"] = df[\"discount_category\"].astype(\"category\")\n",
    "\n",
    "    # Procesar similar_products como longitud de lista\n",
    "    if \"similar_products\" in df.columns:\n",
    "        df[\"similar_products_count\"] = df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "\n",
    "    # Manejar columnas no compatibles en modo `test`\n",
    "    if mode == \"test\":\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype not in [np.int32, np.int64, np.float32, np.float64, np.bool_]:\n",
    "                print(f\"Eliminando columna no compatible: {col}\")\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_global_stats(train_df):\n",
    "    print(\"\\n--- Generando estadísticas globales del entrenamiento ---\")\n",
    "    stats = {}\n",
    "    stats['product_popularity'] = train_df.groupby('partnumber')['session_id'].nunique()\n",
    "    stats['discount_category'] = train_df['discount_category'].value_counts(normalize=True).to_dict()\n",
    "    stats['cluster'] = train_df['cluster'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    # Asignar valores por defecto para test\n",
    "    stats['discount_category']['default'] = max(stats['discount_category'], key=stats['discount_category'].get)\n",
    "    stats['cluster']['default'] = max(stats['cluster'], key=stats['cluster'].get)\n",
    "    return stats\n",
    "\n",
    "def preprocess_test_with_enrichment(test_df, global_stats, train_features):\n",
    "    print(\"\\n--- Preprocesando conjunto de prueba con enriquecimiento ---\")\n",
    "\n",
    "    if 'session_id' not in test_df.columns:\n",
    "        raise KeyError(\"El conjunto de prueba no contiene 'session_id' al inicio.\")\n",
    "\n",
    "    # Convertir columnas datetime a numérico\n",
    "    test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "    test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "    # Procesar similar_products si existe\n",
    "    if \"similar_products\" in test_df.columns:\n",
    "        test_df[\"similar_products_count\"] = test_df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        test_df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "    elif 'similar_products_count' not in test_df.columns:\n",
    "        test_df['similar_products_count'] = 0\n",
    "\n",
    "    # Extraer características temporales\n",
    "    test_df['hour'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.hour\n",
    "    test_df['day_of_week'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "\n",
    "    # Generar características derivadas\n",
    "    test_df['session_length'] = test_df.groupby('session_id')['partnumber'].transform('count')\n",
    "    test_df['product_popularity'] = test_df['partnumber'].map(global_stats['product_popularity']).fillna(0).astype(float)\n",
    "\n",
    "    # Manejar discount_category como categórico\n",
    "    if 'discount_category' in test_df.columns:\n",
    "        test_df['discount_category'] = test_df['discount_category'].astype('category')\n",
    "    else:\n",
    "        default_cat = global_stats['discount_category']['default']\n",
    "        test_df['discount_category'] = pd.Series([default_cat] * len(test_df), index=test_df.index).astype('category')\n",
    "\n",
    "    if 'cluster' not in test_df.columns:\n",
    "        test_df['cluster'] = global_stats['cluster']['default']\n",
    "\n",
    "    for feature in train_features:\n",
    "        if feature not in test_df.columns:\n",
    "            print(f\"Agregando columna faltante: {feature}\")\n",
    "            test_df[feature] = 0\n",
    "\n",
    "    for col in ['session_id'] + train_features:\n",
    "        if col in test_df.columns:\n",
    "            if test_df[col].dtype == 'int64':\n",
    "                test_df[col] = test_df[col].astype(np.int32)\n",
    "            elif test_df[col].dtype == 'float64':\n",
    "                test_df[col] = test_df[col].astype(np.float32)\n",
    "\n",
    "    missing_features = [f for f in train_features if f not in test_df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Faltan características en el conjunto de prueba: {missing_features}\")\n",
    "\n",
    "    valid_cols = ['session_id'] + train_features\n",
    "    test_subset = test_df[valid_cols]\n",
    "\n",
    "    # Ampliar la lista de tipos permitidos para incluir categóricos\n",
    "    allowed_types = {\n",
    "        np.dtype('int32'),\n",
    "        np.dtype('float32'),\n",
    "        np.dtype('bool_'),\n",
    "        np.dtype('int16'),\n",
    "        np.dtype('int8'),\n",
    "        np.dtype('uint32'),\n",
    "        pd.UInt32Dtype()\n",
    "    }\n",
    "\n",
    "    # Verificar individualmente los tipos de cada columna, permitiendo tipos categóricos\n",
    "    invalid_columns = {}\n",
    "    for col, dt in test_subset.dtypes.items():\n",
    "        if dt not in allowed_types and not isinstance(dt, pd.CategoricalDtype):\n",
    "            invalid_columns[col] = dt\n",
    "\n",
    "    if invalid_columns:\n",
    "        raise ValueError(f\"Columnas con tipos de datos no válidos: {invalid_columns}\")\n",
    "\n",
    "    return test_subset\n",
    "\n",
    "def train_lambdamart_with_enrichment(train_path, model_path):\n",
    "    print(\"\\n--- Cargando datos de entrenamiento enriquecidos ---\")\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    train_df = preprocess_and_generate_shared_features(train_df, mode=\"train\")\n",
    "\n",
    "    if \"add_to_cart\" not in train_df.columns:\n",
    "        raise ValueError(\"La columna 'add_to_cart' no está presente en el conjunto de datos.\")\n",
    "\n",
    "    X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y = train_df['add_to_cart']\n",
    "    groups = train_df['session_id'].value_counts().values\n",
    "\n",
    "    # Asegurar que discount_category es categórico\n",
    "    if 'discount_category' in X.columns:\n",
    "        X['discount_category'] = X['discount_category'].astype('category')\n",
    "\n",
    "    categorical_features = []\n",
    "    if 'discount_category' in X.columns and str(X['discount_category'].dtype).startswith('category'):\n",
    "        categorical_features.append('discount_category')\n",
    "\n",
    "    lgb_train = lgb.Dataset(X, label=y, group=groups, categorical_feature=categorical_features)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [1, 3, 5],\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 70,\n",
    "        'max_bin': 255,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- Entrenando el modelo Lambdamart ---\")\n",
    "    model = lgb.train(\n",
    "        params, lgb_train,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[lgb_train],\n",
    "        valid_names=['train']\n",
    "    )\n",
    "\n",
    "    model.save_model(model_path)\n",
    "    return X.columns.tolist()\n",
    "\n",
    "def generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_test_with_enrichment(test_df, global_stats, train_features)\n",
    "\n",
    "    predictions = {}\n",
    "    if 'partnumber' in test_df.columns:\n",
    "        popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    print(\"\\n--- Generando predicciones para cada sesión ---\")\n",
    "    for session_id in test_df['session_id'].unique():\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "\n",
    "def compute_metrics(predictions, ground_truth, k=5):\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for session_id, predicted in predictions.items():\n",
    "        true_products = ground_truth.get(session_id, set())\n",
    "        if not true_products:\n",
    "            continue\n",
    "\n",
    "        pred_set = set(predicted[:k])\n",
    "        precision = len(pred_set & true_products) / k\n",
    "        recall = len(pred_set & true_products) / len(true_products)\n",
    "\n",
    "        dcg = 0.0\n",
    "        for i, prod in enumerate(predicted[:k]):\n",
    "            rel = 1 if prod in true_products else 0\n",
    "            dcg += (2**rel - 1) / np.log2(i + 2)\n",
    "        ideal_rels = [1] * min(len(true_products), k) + [0] * (k - min(len(true_products), k))\n",
    "        idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_rels))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    avg_ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "\n",
    "    return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "def generate_predictions(model, df, global_stats, train_features):\n",
    "    predictions = {}\n",
    "    if 'partnumber' in df.columns:\n",
    "        popular_products = df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    for session_id in df['session_id'].unique():\n",
    "        session_data = df[df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "    return predictions\n",
    "\n",
    "# Rutas de archivos\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl'\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_shared_model_stratify.txt'\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared_stratify.json'\n",
    "\n",
    "# Cargar datos de entrenamiento completos\n",
    "full_train_df = pd.read_pickle(train_path)\n",
    "\n",
    "# Dividir en entrenamiento y validación estratificando por add_to_cart\n",
    "train_df, val_df = train_test_split(full_train_df, test_size=0.2, random_state=42, stratify=full_train_df['add_to_cart'])\n",
    "\n",
    "# Generar estadísticas globales usando el conjunto de entrenamiento\n",
    "global_stats = generate_global_stats(train_df)\n",
    "\n",
    "# Guardar subconjunto de entrenamiento para utilizar la función existente\n",
    "train_subset_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_subset.pkl'\n",
    "train_df.to_pickle(train_subset_path)\n",
    "\n",
    "# Entrenar modelo y obtener características de entrenamiento\n",
    "train_features = train_lambdamart_with_enrichment(train_subset_path, model_path)\n",
    "\n",
    "# Cargar el modelo entrenado para usarlo en predicciones\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# Preprocesar datos de validación\n",
    "val_df_processed = preprocess_test_with_enrichment(val_df.copy(), global_stats, train_features)\n",
    "\n",
    "# Generar predicciones en el conjunto de validación\n",
    "val_predictions = generate_predictions(model, val_df_processed, global_stats, train_features)\n",
    "\n",
    "# Preparar ground truth para el conjunto de validación\n",
    "ground_truth = {}\n",
    "for _, row in val_df.iterrows():\n",
    "    if row['add_to_cart'] == 1:\n",
    "        sid = str(row['session_id'])\n",
    "        ground_truth.setdefault(sid, set()).add(row['partnumber'])\n",
    "\n",
    "# Calcular métricas en el conjunto de validación\n",
    "avg_precision, avg_recall, avg_ndcg = compute_metrics(val_predictions, ground_truth, k=5)\n",
    "print(f\"Precision@5: {avg_precision:.4f}, Recall@5: {avg_recall:.4f}, NDCG@5: {avg_ndcg:.4f}\")\n",
    "\n",
    "# Generar predicciones para el conjunto de prueba final\n",
    "generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features)\n",
    "\n",
    "# Validar formato del JSON generado\n",
    "with open(output_path, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "print(\"\\n--- Validación del JSON generado ---\")\n",
    "print(json.dumps(predictions, indent=4)[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame procesado en un archivo pickle\n",
    "val_df_processed.to_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_enriched_lambdamart_stratify.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "233/900 puntos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuevos parámetros v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generando estadísticas globales del entrenamiento ---\n",
      "\n",
      "--- Cargando datos de entrenamiento enriquecidos ---\n",
      "\n",
      "--- Preprocesando y generando características compartidas (train) ---\n",
      "\n",
      "--- Entrenando el modelo Lambdamart ---\n",
      "Precision@5: 0.2285, Recall@5: 0.8652, NDCG@5: 0.7165\n",
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando datos de prueba ---\n",
      "\n",
      "--- Preprocesando conjunto de prueba con enriquecimiento ---\n",
      "Agregando columna faltante: week\n",
      "Agregando columna faltante: session_interactions\n",
      "Agregando columna faltante: color_id\n",
      "Agregando columna faltante: family\n",
      "Agregando columna faltante: cod_section\n",
      "Agregando columna faltante: discount\n",
      "Agregando columna faltante: popularity\n",
      "\n",
      "--- Generando predicciones para cada sesión ---\n",
      "\n",
      "--- Guardando predicciones ---\n",
      "\n",
      "--- Validación del JSON generado ---\n",
      "{\n",
      "    \"target\": {\n",
      "        \"746\": [\n",
      "            13295,\n",
      "            9377,\n",
      "            9467,\n",
      "            12639,\n",
      "            1254\n",
      "        ],\n",
      "        \"1306\": [\n",
      "            27437,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"1364\": [\n",
      "            19693,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"1377\": [\n",
      "            3610,\n",
      "            6409,\n",
      "            42854,\n",
      "            10205,\n",
      "            32845\n",
      "        ],\n",
      "        \"2251\": [\n",
      "            11024,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"2344\": [\n",
      "            20175,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"3125\": [\n",
      "            22202,\n",
      "            11695,\n",
      "            19733,\n",
      "            20434,\n",
      "            2993\n",
      "        ],\n",
      "        \"4422\": [\n",
      "            22357,\n",
      "            11496,\n",
      "            43269,\n",
      "            16482,\n",
      "            14362\n",
      "        ],\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuración para silenciar advertencias de downcasting\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "def preprocess_and_generate_shared_features(df, mode=\"train\"):\n",
    "    print(f\"\\n--- Preprocesando y generando características compartidas ({mode}) ---\")\n",
    "\n",
    "    # Convertir datetime a numérico (segundos desde el epoch)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).astype(int) / 10**9\n",
    "    if \"timestamp_local\" in df.columns:\n",
    "        df[\"timestamp_local\"] = pd.to_datetime(df[\"timestamp_local\"]).astype(int) / 10**9\n",
    "\n",
    "    # Generar características compartidas\n",
    "    if \"partnumber\" in df.columns:\n",
    "        df[\"product_popularity\"] = df.groupby(\"partnumber\")[\"session_id\"].transform(\"nunique\")\n",
    "    if \"session_id\" in df.columns:\n",
    "        df[\"session_length\"] = df.groupby(\"session_id\")[\"partnumber\"].transform(\"count\")\n",
    "\n",
    "    # Utilizar variables categóricas directamente para discount_category\n",
    "    if \"discount_category\" in df.columns:\n",
    "        df[\"discount_category\"] = df[\"discount_category\"].astype(\"category\")\n",
    "\n",
    "    # Procesar similar_products como longitud de lista\n",
    "    if \"similar_products\" in df.columns:\n",
    "        df[\"similar_products_count\"] = df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "\n",
    "    # Manejar columnas no compatibles en modo `test`\n",
    "    if mode == \"test\":\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype not in [np.int32, np.int64, np.float32, np.float64, np.bool_]:\n",
    "                print(f\"Eliminando columna no compatible: {col}\")\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_global_stats(train_df):\n",
    "    print(\"\\n--- Generando estadísticas globales del entrenamiento ---\")\n",
    "    stats = {}\n",
    "    stats['product_popularity'] = train_df.groupby('partnumber')['session_id'].nunique()\n",
    "    stats['discount_category'] = train_df['discount_category'].value_counts(normalize=True).to_dict()\n",
    "    stats['cluster'] = train_df['cluster'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    # Asignar valores por defecto para test\n",
    "    stats['discount_category']['default'] = max(stats['discount_category'], key=stats['discount_category'].get)\n",
    "    stats['cluster']['default'] = max(stats['cluster'], key=stats['cluster'].get)\n",
    "    return stats\n",
    "\n",
    "def preprocess_test_with_enrichment(test_df, global_stats, train_features):\n",
    "    print(\"\\n--- Preprocesando conjunto de prueba con enriquecimiento ---\")\n",
    "\n",
    "    if 'session_id' not in test_df.columns:\n",
    "        raise KeyError(\"El conjunto de prueba no contiene 'session_id' al inicio.\")\n",
    "\n",
    "    # Convertir columnas datetime a numérico\n",
    "    test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "    test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "    # Procesar similar_products si existe\n",
    "    if \"similar_products\" in test_df.columns:\n",
    "        test_df[\"similar_products_count\"] = test_df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        test_df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "    elif 'similar_products_count' not in test_df.columns:\n",
    "        test_df['similar_products_count'] = 0\n",
    "\n",
    "    # Extraer características temporales\n",
    "    test_df['hour'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.hour\n",
    "    test_df['day_of_week'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "\n",
    "    # Generar características derivadas\n",
    "    test_df['session_length'] = test_df.groupby('session_id')['partnumber'].transform('count')\n",
    "    test_df['product_popularity'] = test_df['partnumber'].map(global_stats['product_popularity']).fillna(0).astype(float)\n",
    "\n",
    "    # Manejar discount_category como categórico\n",
    "    if 'discount_category' in test_df.columns:\n",
    "        test_df['discount_category'] = test_df['discount_category'].astype('category')\n",
    "    else:\n",
    "        default_cat = global_stats['discount_category']['default']\n",
    "        test_df['discount_category'] = pd.Series([default_cat] * len(test_df), index=test_df.index).astype('category')\n",
    "\n",
    "    if 'cluster' not in test_df.columns:\n",
    "        test_df['cluster'] = global_stats['cluster']['default']\n",
    "\n",
    "    for feature in train_features:\n",
    "        if feature not in test_df.columns:\n",
    "            print(f\"Agregando columna faltante: {feature}\")\n",
    "            test_df[feature] = 0\n",
    "\n",
    "    for col in ['session_id'] + train_features:\n",
    "        if col in test_df.columns:\n",
    "            if test_df[col].dtype == 'int64':\n",
    "                test_df[col] = test_df[col].astype(np.int32)\n",
    "            elif test_df[col].dtype == 'float64':\n",
    "                test_df[col] = test_df[col].astype(np.float32)\n",
    "\n",
    "    missing_features = [f for f in train_features if f not in test_df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Faltan características en el conjunto de prueba: {missing_features}\")\n",
    "\n",
    "    valid_cols = ['session_id'] + train_features\n",
    "    test_subset = test_df[valid_cols]\n",
    "\n",
    "    # Ampliar la lista de tipos permitidos para incluir categóricos\n",
    "    allowed_types = {\n",
    "        np.dtype('int32'),\n",
    "        np.dtype('float32'),\n",
    "        np.dtype('bool_'),\n",
    "        np.dtype('int16'),\n",
    "        np.dtype('int8'),\n",
    "        np.dtype('uint32'),\n",
    "        pd.UInt32Dtype()\n",
    "    }\n",
    "\n",
    "    # Verificar individualmente los tipos de cada columna, permitiendo tipos categóricos\n",
    "    invalid_columns = {}\n",
    "    for col, dt in test_subset.dtypes.items():\n",
    "        if dt not in allowed_types and not isinstance(dt, pd.CategoricalDtype):\n",
    "            invalid_columns[col] = dt\n",
    "\n",
    "    if invalid_columns:\n",
    "        raise ValueError(f\"Columnas con tipos de datos no válidos: {invalid_columns}\")\n",
    "\n",
    "    return test_subset\n",
    "\n",
    "def train_lambdamart_with_enrichment(train_path, model_path):\n",
    "    print(\"\\n--- Cargando datos de entrenamiento enriquecidos ---\")\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    train_df = preprocess_and_generate_shared_features(train_df, mode=\"train\")\n",
    "\n",
    "    if \"add_to_cart\" not in train_df.columns:\n",
    "        raise ValueError(\"La columna 'add_to_cart' no está presente en el conjunto de datos.\")\n",
    "\n",
    "    X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y = train_df['add_to_cart']\n",
    "    groups = train_df['session_id'].value_counts().values\n",
    "\n",
    "    # Asegurar que discount_category es categórico\n",
    "    if 'discount_category' in X.columns:\n",
    "        X['discount_category'] = X['discount_category'].astype('category')\n",
    "\n",
    "    categorical_features = []\n",
    "    if 'discount_category' in X.columns and str(X['discount_category'].dtype).startswith('category'):\n",
    "        categorical_features.append('discount_category')\n",
    "\n",
    "    lgb_train = lgb.Dataset(X, label=y, group=groups, categorical_feature=categorical_features)\n",
    "\n",
    "    params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.01,  # Reducido para aprendizaje más gradual\n",
    "    'num_leaves': 80,       # Aumento moderado\n",
    "    'max_bin': 255,\n",
    "    'min_data_in_leaf': 30, # Ligeramente más alto para mayor robustez\n",
    "    'boosting_type': 'gbdt',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "    print(\"\\n--- Entrenando el modelo Lambdamart ---\")\n",
    "    model = lgb.train(\n",
    "        params, lgb_train,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[lgb_train],\n",
    "        valid_names=['train']\n",
    "    )\n",
    "\n",
    "    model.save_model(model_path)\n",
    "    return X.columns.tolist()\n",
    "\n",
    "def generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_test_with_enrichment(test_df, global_stats, train_features)\n",
    "\n",
    "    predictions = {}\n",
    "    if 'partnumber' in test_df.columns:\n",
    "        popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    print(\"\\n--- Generando predicciones para cada sesión ---\")\n",
    "    for session_id in test_df['session_id'].unique():\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "\n",
    "def compute_metrics(predictions, ground_truth, k=5):\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for session_id, predicted in predictions.items():\n",
    "        true_products = ground_truth.get(session_id, set())\n",
    "        if not true_products:\n",
    "            continue\n",
    "\n",
    "        pred_set = set(predicted[:k])\n",
    "        precision = len(pred_set & true_products) / k\n",
    "        recall = len(pred_set & true_products) / len(true_products)\n",
    "\n",
    "        dcg = 0.0\n",
    "        for i, prod in enumerate(predicted[:k]):\n",
    "            rel = 1 if prod in true_products else 0\n",
    "            dcg += (2**rel - 1) / np.log2(i + 2)\n",
    "        ideal_rels = [1] * min(len(true_products), k) + [0] * (k - min(len(true_products), k))\n",
    "        idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_rels))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    avg_ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "\n",
    "    return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "def generate_predictions(model, df, global_stats, train_features):\n",
    "    predictions = {}\n",
    "    if 'partnumber' in df.columns:\n",
    "        popular_products = df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    for session_id in df['session_id'].unique():\n",
    "        session_data = df[df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "    return predictions\n",
    "\n",
    "# Rutas de archivos\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl'\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_shared_model_stratify_v2.txt'\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared_stratify_v2.json'\n",
    "\n",
    "# Cargar datos de entrenamiento completos\n",
    "full_train_df = pd.read_pickle(train_path)\n",
    "\n",
    "# Dividir en entrenamiento y validación estratificando por add_to_cart\n",
    "train_df, val_df = train_test_split(full_train_df, test_size=0.2, random_state=42, stratify=full_train_df['add_to_cart'])\n",
    "\n",
    "# Generar estadísticas globales usando el conjunto de entrenamiento\n",
    "global_stats = generate_global_stats(train_df)\n",
    "\n",
    "# Guardar subconjunto de entrenamiento para utilizar la función existente\n",
    "train_subset_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_subset.pkl'\n",
    "train_df.to_pickle(train_subset_path)\n",
    "\n",
    "# Entrenar modelo y obtener características de entrenamiento\n",
    "train_features = train_lambdamart_with_enrichment(train_subset_path, model_path)\n",
    "\n",
    "# Cargar el modelo entrenado para usarlo en predicciones\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# Preprocesar datos de validación\n",
    "# val_df_processed = preprocess_test_with_enrichment(val_df.copy(), global_stats, train_features)\n",
    "val_df_processed = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_enriched_lambdamart_stratify.pkl')\n",
    "\n",
    "# Generar predicciones en el conjunto de validación\n",
    "val_predictions = generate_predictions(model, val_df_processed, global_stats, train_features)\n",
    "\n",
    "# Preparar ground truth para el conjunto de validación\n",
    "ground_truth = {}\n",
    "for _, row in val_df.iterrows():\n",
    "    if row['add_to_cart'] == 1:\n",
    "        sid = str(row['session_id'])\n",
    "        ground_truth.setdefault(sid, set()).add(row['partnumber'])\n",
    "\n",
    "# Calcular métricas en el conjunto de validación\n",
    "avg_precision, avg_recall, avg_ndcg = compute_metrics(val_predictions, ground_truth, k=5)\n",
    "print(f\"Precision@5: {avg_precision:.4f}, Recall@5: {avg_recall:.4f}, NDCG@5: {avg_ndcg:.4f}\")\n",
    "\n",
    "# Generar predicciones para el conjunto de prueba final\n",
    "generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features)\n",
    "\n",
    "# Validar formato del JSON generado\n",
    "with open(output_path, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "print(\"\\n--- Validación del JSON generado ---\")\n",
    "print(json.dumps(predictions, indent=4)[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "232/900 puntos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicamos configuración de la version 436 con el modelo enriquecido. v3\n",
    "\n",
    "Se usa las mismas características tanto en train como en test, cambiamos params, early_stops,... simulando un modelo más sencillo que resulto tener mejor resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generando estadísticas globales del entrenamiento ---\n",
      "\n",
      "--- Entrenando el modelo Lambdamart ---\n",
      "\n",
      "--- Preprocesando y generando características compartidas (train) ---\n",
      "\n",
      "--- Preprocesando y generando características compartidas (train) ---\n",
      "\n",
      "--- Generando estadísticas globales del entrenamiento ---\n",
      "\n",
      "--- Calculado scale_pos_weight: 15.80 ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.847923\ttrain's ndcg@3: 0.863149\ttrain's ndcg@5: 0.874072\tval's ndcg@1: 0.847075\tval's ndcg@3: 0.862345\tval's ndcg@5: 0.873273\n",
      "[20]\ttrain's ndcg@1: 0.848909\ttrain's ndcg@3: 0.864172\ttrain's ndcg@5: 0.875095\tval's ndcg@1: 0.846909\tval's ndcg@3: 0.862093\tval's ndcg@5: 0.873055\n",
      "[30]\ttrain's ndcg@1: 0.849209\ttrain's ndcg@3: 0.864542\ttrain's ndcg@5: 0.875465\tval's ndcg@1: 0.847059\tval's ndcg@3: 0.862269\tval's ndcg@5: 0.873255\n",
      "[40]\ttrain's ndcg@1: 0.849616\ttrain's ndcg@3: 0.86499\ttrain's ndcg@5: 0.875863\tval's ndcg@1: 0.847167\tval's ndcg@3: 0.862407\tval's ndcg@5: 0.873367\n",
      "[50]\ttrain's ndcg@1: 0.849976\ttrain's ndcg@3: 0.865356\ttrain's ndcg@5: 0.876201\tval's ndcg@1: 0.847209\tval's ndcg@3: 0.8625\tval's ndcg@5: 0.873496\n",
      "[60]\ttrain's ndcg@1: 0.850105\ttrain's ndcg@3: 0.865546\ttrain's ndcg@5: 0.876406\tval's ndcg@1: 0.847201\tval's ndcg@3: 0.862542\tval's ndcg@5: 0.873548\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttrain's ndcg@1: 0.848351\ttrain's ndcg@3: 0.863544\ttrain's ndcg@5: 0.874468\tval's ndcg@1: 0.847405\tval's ndcg@3: 0.862572\tval's ndcg@5: 0.87357\n",
      "\n",
      "--- Modelo guardado en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_shared_model_stratify_v3.txt ---\n",
      "Precision@5: 0.1580, Recall@5: 0.3475, NDCG@5: 0.3391\n",
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando datos de prueba ---\n",
      "\n",
      "--- Preprocesando conjunto de prueba con enriquecimiento ---\n",
      "Agregando columna faltante: week\n",
      "Agregando columna faltante: session_interactions\n",
      "Agregando columna faltante: color_id\n",
      "Agregando columna faltante: family\n",
      "Agregando columna faltante: cod_section\n",
      "Agregando columna faltante: discount\n",
      "Agregando columna faltante: popularity\n",
      "\n",
      "--- Generando predicciones para cada sesión ---\n",
      "\n",
      "--- Guardando predicciones ---\n",
      "Predicciones guardadas en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared_stratify_v3.json\n",
      "\n",
      "--- Validación del JSON generado ---\n",
      "{\n",
      "    \"target\": {\n",
      "        \"746\": [\n",
      "            12639,\n",
      "            13295,\n",
      "            9377,\n",
      "            9467,\n",
      "            32544\n",
      "        ],\n",
      "        \"1306\": [\n",
      "            27437,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"1364\": [\n",
      "            19693,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"1377\": [\n",
      "            42854,\n",
      "            10205,\n",
      "            3610,\n",
      "            6409,\n",
      "            32845\n",
      "        ],\n",
      "        \"2251\": [\n",
      "            11024,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"2344\": [\n",
      "            20175,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"3125\": [\n",
      "            11695,\n",
      "            4774,\n",
      "            19733,\n",
      "            2993,\n",
      "            22202\n",
      "        ],\n",
      "        \"4422\": [\n",
      "            43269,\n",
      "            16482,\n",
      "            11496,\n",
      "            22357,\n",
      "            14362\n",
      "        ],\n",
      "     \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# Configuración para silenciar advertencias de downcasting\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "def preprocess_and_generate_shared_features(df, mode=\"train\"):\n",
    "    print(f\"\\n--- Preprocesando y generando características compartidas ({mode}) ---\")\n",
    "    df = df.copy()  # Crear una copia explícita para evitar SettingWithCopyWarning\n",
    "\n",
    "    # Convertir datetime a numérico (segundos desde el epoch)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).astype(int) / 10**9\n",
    "    if \"timestamp_local\" in df.columns:\n",
    "        df[\"timestamp_local\"] = pd.to_datetime(df[\"timestamp_local\"]).astype(int) / 10**9\n",
    "\n",
    "    # Generar características compartidas\n",
    "    if \"partnumber\" in df.columns:\n",
    "        df[\"product_popularity\"] = df.groupby(\"partnumber\")[\"session_id\"].transform(\"nunique\")\n",
    "    if \"session_id\" in df.columns:\n",
    "        df[\"session_length\"] = df.groupby(\"session_id\")[\"partnumber\"].transform(\"count\")\n",
    "\n",
    "    # Utilizar variables categóricas para discount_category\n",
    "    if \"discount_category\" in df.columns:\n",
    "        df[\"discount_category\"] = df[\"discount_category\"].astype(\"category\")\n",
    "\n",
    "    # Procesar similar_products como longitud de lista\n",
    "    if \"similar_products\" in df.columns:\n",
    "        df[\"similar_products_count\"] = df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "\n",
    "    # Manejar columnas no compatibles en modo `test`\n",
    "    if mode == \"test\":\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype not in [np.int32, np.int64, np.float32, np.float64, np.bool_]:\n",
    "                print(f\"Eliminando columna no compatible: {col}\")\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_global_stats(train_df):\n",
    "    print(\"\\n--- Generando estadísticas globales del entrenamiento ---\")\n",
    "    stats = {}\n",
    "    stats['product_popularity'] = train_df.groupby('partnumber')['session_id'].nunique()\n",
    "    stats['discount_category'] = train_df['discount_category'].value_counts(normalize=True).to_dict()\n",
    "    stats['cluster'] = train_df['cluster'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    # Asignar valores por defecto para test\n",
    "    stats['discount_category']['default'] = max(stats['discount_category'], key=stats['discount_category'].get)\n",
    "    stats['cluster']['default'] = max(stats['cluster'], key=stats['cluster'].get)\n",
    "    return stats\n",
    "\n",
    "def preprocess_test_with_enrichment(test_df, global_stats, train_features):\n",
    "    print(\"\\n--- Preprocesando conjunto de prueba con enriquecimiento ---\")\n",
    "    test_df = test_df.copy()  # Evitar SettingWithCopyWarning\n",
    "\n",
    "    if 'session_id' not in test_df.columns:\n",
    "        raise KeyError(\"El conjunto de prueba no contiene 'session_id' al inicio.\")\n",
    "\n",
    "    # Convertir columnas datetime a numérico\n",
    "    test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "    test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "    # Procesar similar_products si existe\n",
    "    if \"similar_products\" in test_df.columns:\n",
    "        test_df[\"similar_products_count\"] = test_df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        test_df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "    elif 'similar_products_count' not in test_df.columns:\n",
    "        test_df['similar_products_count'] = 0\n",
    "\n",
    "    # Extraer características temporales\n",
    "    test_df['hour'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.hour\n",
    "    test_df['day_of_week'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "\n",
    "    # Generar características derivadas\n",
    "    test_df['session_length'] = test_df.groupby('session_id')['partnumber'].transform('count')\n",
    "    test_df['product_popularity'] = test_df['partnumber'].map(global_stats['product_popularity']).fillna(0).astype(float)\n",
    "\n",
    "    # Manejar discount_category como categórico\n",
    "    if 'discount_category' in test_df.columns:\n",
    "        test_df['discount_category'] = test_df['discount_category'].astype('category')\n",
    "    else:\n",
    "        default_cat = global_stats['discount_category']['default']\n",
    "        test_df['discount_category'] = pd.Series([default_cat] * len(test_df), index=test_df.index).astype('category')\n",
    "\n",
    "    if 'cluster' not in test_df.columns:\n",
    "        test_df['cluster'] = global_stats['cluster']['default']\n",
    "\n",
    "    for feature in train_features:\n",
    "        if feature not in test_df.columns:\n",
    "            print(f\"Agregando columna faltante: {feature}\")\n",
    "            test_df[feature] = 0\n",
    "\n",
    "    for col in ['session_id'] + train_features:\n",
    "        if col in test_df.columns:\n",
    "            if test_df[col].dtype == 'int64':\n",
    "                test_df[col] = test_df[col].astype(np.int32)\n",
    "            elif test_df[col].dtype == 'float64':\n",
    "                test_df[col] = test_df[col].astype(np.float32)\n",
    "\n",
    "    missing_features = [f for f in train_features if f not in test_df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Faltan características en el conjunto de prueba: {missing_features}\")\n",
    "\n",
    "    valid_cols = ['session_id'] + train_features\n",
    "    test_subset = test_df[valid_cols]\n",
    "\n",
    "    # Ampliar la lista de tipos permitidos para incluir categóricos\n",
    "    allowed_types = {\n",
    "        np.dtype('int32'),\n",
    "        np.dtype('float32'),\n",
    "        np.dtype('bool_'),\n",
    "        np.dtype('int16'),\n",
    "        np.dtype('int8'),\n",
    "        np.dtype('uint32'),\n",
    "        pd.UInt32Dtype()\n",
    "    }\n",
    "\n",
    "    # Verificar individualmente los tipos de cada columna, permitiendo tipos categóricos\n",
    "    invalid_columns = {}\n",
    "    for col, dt in test_subset.dtypes.items():\n",
    "        if dt not in allowed_types and not isinstance(dt, pd.CategoricalDtype):\n",
    "            invalid_columns[col] = dt\n",
    "\n",
    "    if invalid_columns:\n",
    "        raise ValueError(f\"Columnas con tipos de datos no válidos: {invalid_columns}\")\n",
    "\n",
    "    return test_subset\n",
    "\n",
    "def train_lambdamart_with_enrichment(train_df, val_df, model_path):\n",
    "    print(\"\\n--- Entrenando el modelo Lambdamart ---\")\n",
    "\n",
    "    # Preprocesar datos de entrenamiento y validación\n",
    "    train_df = preprocess_and_generate_shared_features(train_df, mode=\"train\")\n",
    "    val_df = preprocess_and_generate_shared_features(val_df, mode=\"train\")\n",
    "\n",
    "    if \"add_to_cart\" not in train_df.columns:\n",
    "        raise ValueError(\"La columna 'add_to_cart' no está presente en el conjunto de datos.\")\n",
    "\n",
    "    # Definir características y etiquetas\n",
    "    X_train = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y_train = train_df['add_to_cart']\n",
    "    groups_train = train_df['session_id'].value_counts().values\n",
    "\n",
    "    X_val = val_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y_val = val_df['add_to_cart']\n",
    "    groups_val = val_df['session_id'].value_counts().values\n",
    "\n",
    "    # Generar estadísticas globales usando el conjunto de entrenamiento\n",
    "    global_stats = generate_global_stats(train_df)\n",
    "\n",
    "    # Asegurar que discount_category es categórico en ambos conjuntos\n",
    "    for dataset in [X_train, X_val]:\n",
    "        if 'discount_category' in dataset.columns:\n",
    "            dataset['discount_category'] = dataset['discount_category'].astype('category')\n",
    "\n",
    "    # Identificar características categóricas\n",
    "    categorical_features = []\n",
    "    if 'discount_category' in X_train.columns and str(X_train['discount_category'].dtype).startswith('category'):\n",
    "        categorical_features.append('discount_category')\n",
    "\n",
    "    # Crear datasets de LightGBM para entrenamiento y validación\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train, categorical_feature=categorical_features)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train, categorical_feature=categorical_features)\n",
    "    \n",
    "    # Configuración de hiperparámetros ajustados\n",
    "    # params = {\n",
    "    #     'objective': 'lambdarank',\n",
    "    #     'metric': 'ndcg',\n",
    "    #     'ndcg_eval_at': [1, 3, 5],\n",
    "    #     'learning_rate': 0.03,        # Aumentado ligeramente\n",
    "    #     'num_leaves': 40,             # Reducido para simplicidad\n",
    "    #     'max_bin': 255,\n",
    "    #     'min_data_in_leaf': 15,       # Reducido para mayor generalización\n",
    "    #     'boosting_type': 'gbdt',\n",
    "    #     'feature_fraction': 0.8,\n",
    "    #     'bagging_fraction': 0.8,\n",
    "    #     'bagging_freq': 5,\n",
    "    #     'lambda_l1': 0.1,\n",
    "    #     'lambda_l2': 0.1,\n",
    "    #     'verbose': -1,\n",
    "    #     'n_jobs': -1\n",
    "    # }\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [1, 3, 5],\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 40,\n",
    "        'min_data_in_leaf': 15,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1,\n",
    "        'device': 'cpu',\n",
    "    }\n",
    "\n",
    "    # Definir callbacks para early stopping y logueo\n",
    "    callbacks = [\n",
    "        early_stopping(stopping_rounds=50, verbose=True),\n",
    "        log_evaluation(period=10)\n",
    "    ]\n",
    "\n",
    "    # Entrenar el modelo con validación\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Guardar el modelo entrenado\n",
    "    model.save_model(model_path)\n",
    "    print(f\"\\n--- Modelo guardado en {model_path} ---\")\n",
    "\n",
    "    return X_train.columns.tolist(), global_stats\n",
    "\n",
    "def generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_test_with_enrichment(test_df, global_stats, train_features)\n",
    "\n",
    "    predictions = {}\n",
    "    if 'partnumber' in test_df.columns:\n",
    "        popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    print(\"\\n--- Generando predicciones para cada sesión ---\")\n",
    "    for session_id in test_df['session_id'].unique():\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # Completar con productos populares si es necesario\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "    print(f\"Predicciones guardadas en {output_path}\")\n",
    "\n",
    "def compute_metrics(predictions, ground_truth, k=5):\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for session_id, predicted in predictions.items():\n",
    "        true_products = ground_truth.get(session_id, set())\n",
    "        if not true_products:\n",
    "            continue\n",
    "\n",
    "        pred_set = set(predicted[:k])\n",
    "        precision = len(pred_set & true_products) / k\n",
    "        recall = len(pred_set & true_products) / len(true_products)\n",
    "\n",
    "        dcg = 0.0\n",
    "        for i, prod in enumerate(predicted[:k]):\n",
    "            rel = 1 if prod in true_products else 0\n",
    "            dcg += (2**rel - 1) / np.log2(i + 2)\n",
    "        ideal_rels = [1] * min(len(true_products), k) + [0] * (k - min(len(true_products), k))\n",
    "        idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_rels))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    avg_ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "\n",
    "    return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "def generate_predictions(model, df, global_stats, train_features):\n",
    "    predictions = {}\n",
    "    if 'partnumber' in df.columns:\n",
    "        popular_products = df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    for session_id in df['session_id'].unique():\n",
    "        session_data = df[df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "    return predictions\n",
    "\n",
    "# Rutas de archivos\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl'\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_shared_model_stratify_v3.txt'\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared_stratify_v3.json'\n",
    "\n",
    "# Cargar datos de entrenamiento completos\n",
    "full_train_df = pd.read_pickle(train_path)\n",
    "\n",
    "# Usar GroupShuffleSplit para dividir preservando sesiones\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(full_train_df, groups=full_train_df['session_id']))\n",
    "train_df = full_train_df.iloc[train_idx]\n",
    "val_df = full_train_df.iloc[val_idx]\n",
    "\n",
    "# Generar estadísticas globales usando el conjunto de entrenamiento\n",
    "global_stats = generate_global_stats(train_df)\n",
    "\n",
    "# Guardar subconjunto de entrenamiento para utilizar la función existente\n",
    "train_subset_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_subset.pkl'\n",
    "train_df.to_pickle(train_subset_path)\n",
    "\n",
    "# Entrenar modelo y obtener características de entrenamiento\n",
    "train_features, _ = train_lambdamart_with_enrichment(train_df, val_df, model_path)\n",
    "\n",
    "# Cargar el modelo entrenado para usarlo en predicciones\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# Preprocesar datos de validación desde un archivo preprocesado\n",
    "val_df_processed = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_enriched_lambdamart_stratify.pkl')\n",
    "\n",
    "# Generar predicciones en el conjunto de validación\n",
    "val_predictions = generate_predictions(model, val_df_processed, global_stats, train_features)\n",
    "\n",
    "# Preparar ground truth para el conjunto de validación\n",
    "ground_truth = {}\n",
    "for _, row in val_df.iterrows():\n",
    "    if row['add_to_cart'] == 1:\n",
    "        sid = str(row['session_id'])\n",
    "        ground_truth.setdefault(sid, set()).add(row['partnumber'])\n",
    "\n",
    "# Calcular métricas en el conjunto de validación\n",
    "avg_precision, avg_recall, avg_ndcg = compute_metrics(val_predictions, ground_truth, k=5)\n",
    "print(f\"Precision@5: {avg_precision:.4f}, Recall@5: {avg_recall:.4f}, NDCG@5: {avg_ndcg:.4f}\")\n",
    "\n",
    "# Generar predicciones para el conjunto de prueba final\n",
    "generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features)\n",
    "\n",
    "# Validar formato del JSON generado\n",
    "with open(output_path, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "print(\"\\n--- Validación del JSON generado ---\")\n",
    "print(json.dumps(predictions, indent=4)[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilizar **GroupShuffleSplit** para preservar sesiones en la división de datos.\n",
    "- Ajustar hiperparámetros basados en el modelo sencillo.\n",
    "- Incorporar **callbacks** para early stopping y registro de evaluación en el entrenamiento.\n",
    "- Utilizar un conjunto de validación para early stopping.\n",
    "\n",
    "Este código reemplaza la estrategia de `train_test_split` por `GroupShuffleSplit`, configura un conjunto de validación y añade callbacks para early stopping:\n",
    "\n",
    "### Ajustes Clave Realizados:\n",
    "\n",
    "1. **División de Datos con GroupShuffleSplit:**\n",
    "   - Se utiliza `GroupShuffleSplit` para dividir el conjunto de datos preservando las sesiones. Aunque esto no garantiza estratificación, mantiene la integridad de las sesiones en los conjuntos de entrenamiento y validación.\n",
    "\n",
    "2. **Ajuste de Hiperparámetros:**\n",
    "   - Se han cambiado algunos hiperparámetros basados en el modelo sencillo:\n",
    "     - `learning_rate`: 0.03\n",
    "     - `num_leaves`: 40\n",
    "     - `min_data_in_leaf`: 15\n",
    "   - Se mantienen otros parámetros como `feature_fraction`, `bagging_fraction`, etc.\n",
    "\n",
    "3. **Incorporación de Callbacks para Early Stopping y Logueo:**\n",
    "   - Se han añadido callbacks (`early_stopping` y `log_evaluation`) al entrenamiento para detener el proceso si no se observan mejoras y para registrar el progreso.\n",
    "\n",
    "4. **Uso de un Conjunto de Validación:**\n",
    "   - Se crea un conjunto de validación (`lgb_val`) con sus correspondientes datos (`X_val`, `y_val`, `groups_val`) para que los callbacks de early stopping funcionen adecuadamente.\n",
    "   - La función `train_lambdamart_with_enrichment` se ajustó para incorporar valid_sets con entrenamiento y validación.\n",
    "\n",
    "### Nota:\n",
    "- Este ajuste emplea GroupShuffleSplit sin estratificación debido a la limitación de combinar estratificación y agrupación en sklearn.\n",
    "- La configuración de early stopping ahora utiliza un conjunto de validación para monitorear el desempeño y detener el entrenamiento si no mejora.\n",
    "\n",
    "Con estos ajustes, el código debería entrenar con early stopping, preservar sesiones en la división de datos y registrar la evaluación periódicamente, siguiendo las recomendaciones que derivan del modelo más sencillo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "236/900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Probamos nuevos parámetros.` v4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_stats cargado desde 'global_stats.pkl'\n",
      "\n",
      "--- Entrenando el modelo Lambdamart ---\n",
      "\n",
      "--- Preprocesando y generando características compartidas (train) ---\n",
      "\n",
      "--- Preprocesando y generando características compartidas (train) ---\n",
      "\n",
      "--- Generando estadísticas globales del entrenamiento ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.848201\ttrain's ndcg@3: 0.86335\ttrain's ndcg@5: 0.874271\tval's ndcg@1: 0.847027\tval's ndcg@3: 0.862238\tval's ndcg@5: 0.873252\n",
      "[20]\ttrain's ndcg@1: 0.84887\ttrain's ndcg@3: 0.864084\ttrain's ndcg@5: 0.874973\tval's ndcg@1: 0.847247\tval's ndcg@3: 0.862585\tval's ndcg@5: 0.873601\n",
      "[30]\ttrain's ndcg@1: 0.849067\ttrain's ndcg@3: 0.86432\ttrain's ndcg@5: 0.875239\tval's ndcg@1: 0.847434\tval's ndcg@3: 0.862739\tval's ndcg@5: 0.873753\n",
      "[40]\ttrain's ndcg@1: 0.849439\ttrain's ndcg@3: 0.864764\ttrain's ndcg@5: 0.875661\tval's ndcg@1: 0.847267\tval's ndcg@3: 0.86264\tval's ndcg@5: 0.873627\n",
      "[50]\ttrain's ndcg@1: 0.84965\ttrain's ndcg@3: 0.864974\ttrain's ndcg@5: 0.875867\tval's ndcg@1: 0.847329\tval's ndcg@3: 0.862742\tval's ndcg@5: 0.87375\n",
      "[60]\ttrain's ndcg@1: 0.849903\ttrain's ndcg@3: 0.865225\ttrain's ndcg@5: 0.876102\tval's ndcg@1: 0.847546\tval's ndcg@3: 0.862596\tval's ndcg@5: 0.873659\n",
      "[70]\ttrain's ndcg@1: 0.85018\ttrain's ndcg@3: 0.865523\ttrain's ndcg@5: 0.876387\tval's ndcg@1: 0.847718\tval's ndcg@3: 0.86286\tval's ndcg@5: 0.873862\n",
      "[80]\ttrain's ndcg@1: 0.850314\ttrain's ndcg@3: 0.86561\ttrain's ndcg@5: 0.876489\tval's ndcg@1: 0.847775\tval's ndcg@3: 0.862902\tval's ndcg@5: 0.873904\n",
      "[90]\ttrain's ndcg@1: 0.850428\ttrain's ndcg@3: 0.865704\ttrain's ndcg@5: 0.8766\tval's ndcg@1: 0.847698\tval's ndcg@3: 0.862835\tval's ndcg@5: 0.873874\n",
      "[100]\ttrain's ndcg@1: 0.850567\ttrain's ndcg@3: 0.865873\ttrain's ndcg@5: 0.876769\tval's ndcg@1: 0.84774\tval's ndcg@3: 0.862925\tval's ndcg@5: 0.873914\n",
      "[110]\ttrain's ndcg@1: 0.850697\ttrain's ndcg@3: 0.866004\ttrain's ndcg@5: 0.876876\tval's ndcg@1: 0.847771\tval's ndcg@3: 0.862973\tval's ndcg@5: 0.873956\n",
      "[120]\ttrain's ndcg@1: 0.850888\ttrain's ndcg@3: 0.866178\ttrain's ndcg@5: 0.877039\tval's ndcg@1: 0.847825\tval's ndcg@3: 0.86301\tval's ndcg@5: 0.874019\n",
      "[130]\ttrain's ndcg@1: 0.851005\ttrain's ndcg@3: 0.866221\ttrain's ndcg@5: 0.877095\tval's ndcg@1: 0.847842\tval's ndcg@3: 0.863032\tval's ndcg@5: 0.87404\n",
      "[140]\ttrain's ndcg@1: 0.851178\ttrain's ndcg@3: 0.866408\ttrain's ndcg@5: 0.877261\tval's ndcg@1: 0.847845\tval's ndcg@3: 0.863138\tval's ndcg@5: 0.874132\n",
      "[150]\ttrain's ndcg@1: 0.851213\ttrain's ndcg@3: 0.866479\ttrain's ndcg@5: 0.877336\tval's ndcg@1: 0.847961\tval's ndcg@3: 0.863173\tval's ndcg@5: 0.874212\n",
      "[160]\ttrain's ndcg@1: 0.851292\ttrain's ndcg@3: 0.86658\ttrain's ndcg@5: 0.877454\tval's ndcg@1: 0.847846\tval's ndcg@3: 0.86312\tval's ndcg@5: 0.87417\n",
      "[170]\ttrain's ndcg@1: 0.851481\ttrain's ndcg@3: 0.866752\ttrain's ndcg@5: 0.877609\tval's ndcg@1: 0.847888\tval's ndcg@3: 0.863172\tval's ndcg@5: 0.874195\n",
      "[180]\ttrain's ndcg@1: 0.851489\ttrain's ndcg@3: 0.866771\ttrain's ndcg@5: 0.877639\tval's ndcg@1: 0.847878\tval's ndcg@3: 0.863153\tval's ndcg@5: 0.874155\n",
      "[190]\ttrain's ndcg@1: 0.851583\ttrain's ndcg@3: 0.866859\ttrain's ndcg@5: 0.877733\tval's ndcg@1: 0.847928\tval's ndcg@3: 0.863174\tval's ndcg@5: 0.874193\n",
      "[200]\ttrain's ndcg@1: 0.851674\ttrain's ndcg@3: 0.866953\ttrain's ndcg@5: 0.877832\tval's ndcg@1: 0.848043\tval's ndcg@3: 0.863239\tval's ndcg@5: 0.874258\n",
      "[210]\ttrain's ndcg@1: 0.85183\ttrain's ndcg@3: 0.867074\ttrain's ndcg@5: 0.877942\tval's ndcg@1: 0.847774\tval's ndcg@3: 0.86308\tval's ndcg@5: 0.874104\n",
      "[220]\ttrain's ndcg@1: 0.851907\ttrain's ndcg@3: 0.867152\ttrain's ndcg@5: 0.878023\tval's ndcg@1: 0.847738\tval's ndcg@3: 0.863081\tval's ndcg@5: 0.874085\n",
      "[230]\ttrain's ndcg@1: 0.851952\ttrain's ndcg@3: 0.86719\ttrain's ndcg@5: 0.87806\tval's ndcg@1: 0.847763\tval's ndcg@3: 0.863104\tval's ndcg@5: 0.874091\n",
      "[240]\ttrain's ndcg@1: 0.85207\ttrain's ndcg@3: 0.867295\ttrain's ndcg@5: 0.878142\tval's ndcg@1: 0.847823\tval's ndcg@3: 0.863175\tval's ndcg@5: 0.874149\n",
      "Early stopping, best iteration is:\n",
      "[197]\ttrain's ndcg@1: 0.851657\ttrain's ndcg@3: 0.866922\ttrain's ndcg@5: 0.877804\tval's ndcg@1: 0.848048\tval's ndcg@3: 0.863221\tval's ndcg@5: 0.874248\n",
      "\n",
      "--- Modelo guardado en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_shared_model_stratify_v4.txt ---\n",
      "Precision@5: 0.1594, Recall@5: 0.3495, NDCG@5: 0.3438\n",
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando datos de prueba ---\n",
      "\n",
      "--- Preprocesando conjunto de prueba con enriquecimiento ---\n",
      "Agregando columna faltante: week\n",
      "Agregando columna faltante: session_interactions\n",
      "Agregando columna faltante: color_id\n",
      "Agregando columna faltante: family\n",
      "Agregando columna faltante: cod_section\n",
      "Agregando columna faltante: discount\n",
      "Agregando columna faltante: popularity\n",
      "\n",
      "--- Generando predicciones para cada sesión ---\n",
      "\n",
      "--- Guardando predicciones ---\n",
      "Predicciones guardadas en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared_stratify_v4.json\n",
      "\n",
      "--- Validación del JSON generado ---\n",
      "{\n",
      "    \"target\": {\n",
      "        \"746\": [\n",
      "            12639,\n",
      "            13295,\n",
      "            9377,\n",
      "            9467,\n",
      "            1254\n",
      "        ],\n",
      "        \"1306\": [\n",
      "            27437,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"1364\": [\n",
      "            19693,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"1377\": [\n",
      "            3610,\n",
      "            6409,\n",
      "            10205,\n",
      "            42854,\n",
      "            32845\n",
      "        ],\n",
      "        \"2251\": [\n",
      "            11024,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"2344\": [\n",
      "            20175,\n",
      "            32845,\n",
      "            11563,\n",
      "            43608,\n",
      "            11798\n",
      "        ],\n",
      "        \"3125\": [\n",
      "            22202,\n",
      "            2993,\n",
      "            2049,\n",
      "            4774,\n",
      "            11695\n",
      "        ],\n",
      "        \"4422\": [\n",
      "            11496,\n",
      "            16482,\n",
      "            22357,\n",
      "            43269,\n",
      "            14362\n",
      "        ],\n",
      "       \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# Configuración para silenciar advertencias de downcasting\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "def preprocess_and_generate_shared_features(df, mode=\"train\"):\n",
    "    print(f\"\\n--- Preprocesando y generando características compartidas ({mode}) ---\")\n",
    "    df = df.copy()  # Crear una copia explícita para evitar SettingWithCopyWarning\n",
    "\n",
    "    # Convertir datetime a numérico (segundos desde el epoch)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).astype(int) / 10**9\n",
    "    if \"timestamp_local\" in df.columns:\n",
    "        df[\"timestamp_local\"] = pd.to_datetime(df[\"timestamp_local\"]).astype(int) / 10**9\n",
    "\n",
    "    # Generar características compartidas\n",
    "    if \"partnumber\" in df.columns:\n",
    "        df[\"product_popularity\"] = df.groupby(\"partnumber\")[\"session_id\"].transform(\"nunique\")\n",
    "    if \"session_id\" in df.columns:\n",
    "        df[\"session_length\"] = df.groupby(\"session_id\")[\"partnumber\"].transform(\"count\")\n",
    "\n",
    "    # Utilizar variables categóricas para discount_category\n",
    "    if \"discount_category\" in df.columns:\n",
    "        df[\"discount_category\"] = df[\"discount_category\"].astype(\"category\")\n",
    "\n",
    "    # Procesar similar_products como longitud de lista\n",
    "    if \"similar_products\" in df.columns:\n",
    "        df[\"similar_products_count\"] = df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "\n",
    "    # Manejar columnas no compatibles en modo `test`\n",
    "    if mode == \"test\":\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype not in [np.int32, np.int64, np.float32, np.float64, np.bool_]:\n",
    "                print(f\"Eliminando columna no compatible: {col}\")\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_global_stats(train_df):\n",
    "    print(\"\\n--- Generando estadísticas globales del entrenamiento ---\")\n",
    "    stats = {}\n",
    "    stats['product_popularity'] = train_df.groupby('partnumber')['session_id'].nunique()\n",
    "    stats['discount_category'] = train_df['discount_category'].value_counts(normalize=True).to_dict()\n",
    "    stats['cluster'] = train_df['cluster'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    # Asignar valores por defecto para test\n",
    "    stats['discount_category']['default'] = max(stats['discount_category'], key=stats['discount_category'].get)\n",
    "    stats['cluster']['default'] = max(stats['cluster'], key=stats['cluster'].get)\n",
    "    return stats\n",
    "\n",
    "def preprocess_test_with_enrichment(test_df, global_stats, train_features):\n",
    "    print(\"\\n--- Preprocesando conjunto de prueba con enriquecimiento ---\")\n",
    "    test_df = test_df.copy()  # Evitar SettingWithCopyWarning\n",
    "\n",
    "    if 'session_id' not in test_df.columns:\n",
    "        raise KeyError(\"El conjunto de prueba no contiene 'session_id' al inicio.\")\n",
    "\n",
    "    # Convertir columnas datetime a numérico\n",
    "    test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "    test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "    # Procesar similar_products si existe\n",
    "    if \"similar_products\" in test_df.columns:\n",
    "        test_df[\"similar_products_count\"] = test_df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        test_df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "    elif 'similar_products_count' not in test_df.columns:\n",
    "        test_df['similar_products_count'] = 0\n",
    "\n",
    "    # Extraer características temporales\n",
    "    test_df['hour'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.hour\n",
    "    test_df['day_of_week'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "\n",
    "    # Generar características derivadas\n",
    "    test_df['session_length'] = test_df.groupby('session_id')['partnumber'].transform('count')\n",
    "    test_df['product_popularity'] = test_df['partnumber'].map(global_stats['product_popularity']).fillna(0).astype(float)\n",
    "\n",
    "    # Manejar discount_category como categórico\n",
    "    if 'discount_category' in test_df.columns:\n",
    "        test_df['discount_category'] = test_df['discount_category'].astype('category')\n",
    "    else:\n",
    "        default_cat = global_stats['discount_category']['default']\n",
    "        test_df['discount_category'] = pd.Series([default_cat] * len(test_df), index=test_df.index).astype('category')\n",
    "\n",
    "    if 'cluster' not in test_df.columns:\n",
    "        test_df['cluster'] = global_stats['cluster']['default']\n",
    "\n",
    "    for feature in train_features:\n",
    "        if feature not in test_df.columns:\n",
    "            print(f\"Agregando columna faltante: {feature}\")\n",
    "            test_df[feature] = 0\n",
    "\n",
    "    for col in ['session_id'] + train_features:\n",
    "        if col in test_df.columns:\n",
    "            if test_df[col].dtype == 'int64':\n",
    "                test_df[col] = test_df[col].astype(np.int32)\n",
    "            elif test_df[col].dtype == 'float64':\n",
    "                test_df[col] = test_df[col].astype(np.float32)\n",
    "\n",
    "    missing_features = [f for f in train_features if f not in test_df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Faltan características en el conjunto de prueba: {missing_features}\")\n",
    "\n",
    "    valid_cols = ['session_id'] + train_features\n",
    "    test_subset = test_df[valid_cols]\n",
    "\n",
    "    # Ampliar la lista de tipos permitidos para incluir categóricos\n",
    "    allowed_types = {\n",
    "        np.dtype('int32'),\n",
    "        np.dtype('float32'),\n",
    "        np.dtype('bool_'),\n",
    "        np.dtype('int16'),\n",
    "        np.dtype('int8'),\n",
    "        np.dtype('uint32'),\n",
    "        pd.UInt32Dtype()\n",
    "    }\n",
    "\n",
    "    # Verificar individualmente los tipos de cada columna, permitiendo tipos categóricos\n",
    "    invalid_columns = {}\n",
    "    for col, dt in test_subset.dtypes.items():\n",
    "        if dt not in allowed_types and not isinstance(dt, pd.CategoricalDtype):\n",
    "            invalid_columns[col] = dt\n",
    "\n",
    "    if invalid_columns:\n",
    "        raise ValueError(f\"Columnas con tipos de datos no válidos: {invalid_columns}\")\n",
    "\n",
    "    return test_subset\n",
    "\n",
    "def train_lambdamart_with_enrichment(train_df, val_df, model_path):\n",
    "    print(\"\\n--- Entrenando el modelo Lambdamart ---\")\n",
    "\n",
    "    # Preprocesar datos de entrenamiento y validación\n",
    "    train_df = preprocess_and_generate_shared_features(train_df, mode=\"train\")\n",
    "    val_df = preprocess_and_generate_shared_features(val_df, mode=\"train\")\n",
    "\n",
    "    if \"add_to_cart\" not in train_df.columns:\n",
    "        raise ValueError(\"La columna 'add_to_cart' no está presente en el conjunto de datos.\")\n",
    "\n",
    "    # Definir características y etiquetas\n",
    "    X_train = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y_train = train_df['add_to_cart']\n",
    "    groups_train = train_df['session_id'].value_counts().values\n",
    "\n",
    "    X_val = val_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y_val = val_df['add_to_cart']\n",
    "    groups_val = val_df['session_id'].value_counts().values\n",
    "\n",
    "    # Generar estadísticas globales usando el conjunto de entrenamiento\n",
    "    global_stats = generate_global_stats(train_df)\n",
    "\n",
    "    # Asegurar que discount_category es categórico en ambos conjuntos\n",
    "    for dataset in [X_train, X_val]:\n",
    "        if 'discount_category' in dataset.columns:\n",
    "            dataset['discount_category'] = dataset['discount_category'].astype('category')\n",
    "\n",
    "    # Identificar características categóricas\n",
    "    categorical_features = []\n",
    "    if 'discount_category' in X_train.columns and str(X_train['discount_category'].dtype).startswith('category'):\n",
    "        categorical_features.append('discount_category')\n",
    "\n",
    "    # Crear datasets de LightGBM para entrenamiento y validación\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train, categorical_feature=categorical_features)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train, categorical_feature=categorical_features)\n",
    "    \n",
    "    # Configuración de hiperparámetros ajustados\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [1, 3, 5],\n",
    "        'learning_rate': 0.01,        # Aumentado ligeramente\n",
    "        'num_leaves': 60,             # Reducido para simplicidad\n",
    "        'max_bin': 255,\n",
    "        'min_data_in_leaf': 30,       # Reducido para mayor generalización\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # params = {\n",
    "    #     'objective': 'lambdarank',\n",
    "    #     'metric': 'ndcg',\n",
    "    #     'ndcg_eval_at': [1, 3, 5],\n",
    "    #     'learning_rate': 0.03,\n",
    "    #     'num_leaves': 40,\n",
    "    #     'min_data_in_leaf': 15,\n",
    "    #     'boosting_type': 'gbdt',\n",
    "    #     'verbose': -1,\n",
    "    #     'device': 'cpu',\n",
    "    # }\n",
    "\n",
    "    # Definir callbacks para early stopping y logueo\n",
    "    callbacks = [\n",
    "        early_stopping(stopping_rounds=50, verbose=True),\n",
    "        log_evaluation(period=10)\n",
    "    ]\n",
    "\n",
    "    # Entrenar el modelo con validación\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Guardar el modelo entrenado\n",
    "    model.save_model(model_path)\n",
    "    print(f\"\\n--- Modelo guardado en {model_path} ---\")\n",
    "\n",
    "    return X_train.columns.tolist(), global_stats\n",
    "\n",
    "def generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_test_with_enrichment(test_df, global_stats, train_features)\n",
    "\n",
    "    predictions = {}\n",
    "    if 'partnumber' in test_df.columns:\n",
    "        popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    print(\"\\n--- Generando predicciones para cada sesión ---\")\n",
    "    for session_id in test_df['session_id'].unique():\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # Completar con productos populares si es necesario\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "    print(f\"Predicciones guardadas en {output_path}\")\n",
    "\n",
    "def compute_metrics(predictions, ground_truth, k=5):\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for session_id, predicted in predictions.items():\n",
    "        true_products = ground_truth.get(session_id, set())\n",
    "        if not true_products:\n",
    "            continue\n",
    "\n",
    "        pred_set = set(predicted[:k])\n",
    "        precision = len(pred_set & true_products) / k\n",
    "        recall = len(pred_set & true_products) / len(true_products)\n",
    "\n",
    "        dcg = 0.0\n",
    "        for i, prod in enumerate(predicted[:k]):\n",
    "            rel = 1 if prod in true_products else 0\n",
    "            dcg += (2**rel - 1) / np.log2(i + 2)\n",
    "        ideal_rels = [1] * min(len(true_products), k) + [0] * (k - min(len(true_products), k))\n",
    "        idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_rels))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    avg_ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "\n",
    "    return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "def generate_predictions(model, df, global_stats, train_features):\n",
    "    predictions = {}\n",
    "    if 'partnumber' in df.columns:\n",
    "        popular_products = df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    print('-- Calcular puntajes para todas las filas de una vez --')\n",
    "    # Calcular puntajes para todas las filas de una vez\n",
    "    df['score'] = model.predict(df[train_features])\n",
    "    \n",
    "    print('-- FOR - Calcular puntajes para todas las filas de una vez --')\n",
    "    for session_id in df['session_id'].unique():\n",
    "        session_data = df[df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        # Utilizar nlargest para obtener los mejores productos\n",
    "        top_candidates = session_data.nlargest(10, 'score')['partnumber'].drop_duplicates().tolist()\n",
    "\n",
    "        # Completar con productos populares si hay menos de 5 únicos\n",
    "        recommended_products = top_candidates[:5]\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Rutas de archivos\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl'\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_shared_model_stratify_v4.txt'\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared_stratify_v4.json'\n",
    "\n",
    "# Cargar datos de entrenamiento completos\n",
    "full_train_df = pd.read_pickle(train_path)\n",
    "\n",
    "# Usar GroupShuffleSplit para dividir preservando sesiones\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(full_train_df, groups=full_train_df['session_id']))\n",
    "train_df = full_train_df.iloc[train_idx]\n",
    "val_df = full_train_df.iloc[val_idx]\n",
    "\n",
    "# Generar estadísticas globales usando el conjunto de entrenamiento\n",
    "# global_stats = generate_global_stats(train_df)\n",
    "import pickle\n",
    "with open('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/global_stats.pkl', 'rb') as f:\n",
    "    global_stats = pickle.load(f)\n",
    "print(\"global_stats cargado desde 'global_stats.pkl'\")\n",
    "\n",
    "# Guardar subconjunto de entrenamiento para utilizar la función existente\n",
    "train_subset_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_subset.pkl'\n",
    "train_df.to_pickle(train_subset_path)\n",
    "\n",
    "# Entrenar modelo y obtener características de entrenamiento\n",
    "train_features, _ = train_lambdamart_with_enrichment(train_df, val_df, model_path)\n",
    "\n",
    "# Cargar el modelo entrenado para usarlo en predicciones\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# Preprocesar datos de validación desde un archivo preprocesado\n",
    "val_df_processed = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_enriched_lambdamart_stratify.pkl')\n",
    "\n",
    "# Generar predicciones en el conjunto de validación\n",
    "val_predictions = generate_predictions(model, val_df_processed, global_stats, train_features)\n",
    "\n",
    "# Preparar ground truth para el conjunto de validación\n",
    "ground_truth = {}\n",
    "for _, row in val_df.iterrows():\n",
    "    if row['add_to_cart'] == 1:\n",
    "        sid = str(row['session_id'])\n",
    "        ground_truth.setdefault(sid, set()).add(row['partnumber'])\n",
    "\n",
    "# Calcular métricas en el conjunto de validación\n",
    "avg_precision, avg_recall, avg_ndcg = compute_metrics(val_predictions, ground_truth, k=5)\n",
    "print(f\"Precision@5: {avg_precision:.4f}, Recall@5: {avg_recall:.4f}, NDCG@5: {avg_ndcg:.4f}\")\n",
    "\n",
    "# Generar predicciones para el conjunto de prueba final\n",
    "generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features)\n",
    "\n",
    "# Validar formato del JSON generado\n",
    "with open(output_path, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "print(\"\\n--- Validación del JSON generado ---\")\n",
    "print(json.dumps(predictions, indent=4)[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_stats guardado en 'global_stats.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Guardado global_stats\n",
    "import pickle\n",
    "# Suponiendo que global_stats ya está calculado\n",
    "with open('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/global_stats.pkl', 'wb') as f:\n",
    "    pickle.dump(global_stats, f)\n",
    "print(\"global_stats guardado en 'global_stats.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sí, es posible mantener sesiones completas mientras se estratifica por la presencia de `add_to_cart` dentro de cada sesión. La idea es realizar la estratificación a nivel de sesión, usando como criterio si la sesión contiene al menos un evento `add_to_cart = 1`. A continuación, te explico cómo puedes hacerlo:\n",
    "\n",
    "### Pasos para estratificar sesiones completas por `add_to_cart`:\n",
    "\n",
    "1. **Agrega una etiqueta a nivel de sesión**:  \n",
    "   Agrupa tus datos por `session_id` y determina si en esa sesión ocurrió al menos un `add_to_cart = 1`. Esto te dará una variable binaria por sesión que indica la presencia de un evento de carrito.\n",
    "\n",
    "2. **Divide las sesiones en entrenamiento y validación**:  \n",
    "   Utiliza `train_test_split` en el conjunto de sesiones, estratificando según la etiqueta binaria obtenida. Esto garantiza que la proporción de sesiones con y sin `add_to_cart` se mantenga similar en ambos conjuntos.\n",
    "\n",
    "3. **Filtra el DataFrame original por las sesiones asignadas a entrenamiento y validación**:  \n",
    "   Una vez que tienes listas de `session_id` para entrenamiento y validación, filtra tu DataFrame original para incluir solo las sesiones correspondientes en cada conjunto.\n",
    "\n",
    "### Implementación en código:\n",
    "\n",
    "Agrega el siguiente bloque antes de dividir tus datos, sustituyendo la parte donde usas `train_test_split` directamente sobre el DataFrame completo:\n",
    "\n",
    "```python\n",
    "# 1. Agregar etiqueta a nivel de sesión\n",
    "# Agrupar por session_id para determinar si la sesión tuvo algún add_to_cart\n",
    "session_labels = full_train_df.groupby('session_id')['add_to_cart'].max().reset_index()\n",
    "session_labels['label'] = session_labels['add_to_cart']  # 1 si hubo al menos un add_to_cart, 0 si no\n",
    "\n",
    "# 2. Dividir sesiones en entrenamiento y validación\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_sessions, val_sessions = train_test_split(\n",
    "    session_labels['session_id'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=session_labels['label']\n",
    ")\n",
    "\n",
    "# 3. Filtrar el DataFrame original para mantener sesiones completas en cada conjunto\n",
    "train_df = full_train_df[full_train_df['session_id'].isin(train_sessions)]\n",
    "val_df = full_train_df[full_train_df['session_id'].isin(val_sessions)]\n",
    "```\n",
    "\n",
    "### Integración con tu flujo actual:\n",
    "\n",
    "Reemplaza la sección de partición de datos en tu código por el bloque anterior. Por ejemplo:\n",
    "\n",
    "```python\n",
    "# Cargar datos de entrenamiento completos\n",
    "full_train_df = pd.read_pickle(train_path)\n",
    "\n",
    "# 1. Agregar etiqueta a nivel de sesión\n",
    "session_labels = full_train_df.groupby('session_id')['add_to_cart'].max().reset_index()\n",
    "session_labels['label'] = session_labels['add_to_cart']\n",
    "\n",
    "# 2. Dividir sesiones en entrenamiento y validación\n",
    "train_sessions, val_sessions = train_test_split(\n",
    "    session_labels['session_id'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=session_labels['label']\n",
    ")\n",
    "\n",
    "# 3. Filtrar el DataFrame original para mantener sesiones completas en cada conjunto\n",
    "train_df = full_train_df[full_train_df['session_id'].isin(train_sessions)]\n",
    "val_df = full_train_df[full_train_df['session_id'].isin(val_sessions)]\n",
    "\n",
    "# Generar estadísticas globales usando el conjunto de entrenamiento\n",
    "global_stats = generate_global_stats(train_df)\n",
    "print(global_stats)\n",
    "\n",
    "# ... continuar con el flujo: entrenamiento, evaluación, etc.\n",
    "```\n",
    "\n",
    "### Consideraciones:\n",
    "\n",
    "- **Distribución de clases**:  \n",
    "  Con este enfoque, estratificas por si una sesión contiene algún `add_to_cart`. La proporción de sesiones que tienen al menos un `add_to_cart` se mantendrá similar entre entrenamiento y validación.\n",
    "\n",
    "- **Integridad de la sesión**:  \n",
    "  Todas las entradas de una misma sesión permanecerán juntas en el conjunto de entrenamiento o en el de validación, preservando la información completa de cada sesión.\n",
    "\n",
    "- **Compatibilidad con tu flujo existente**:  \n",
    "  Esta estrategia no altera el resto de tu flujo. El modelo se entrenará y validará con sesiones completas, respetando la estratificación deseada.\n",
    "\n",
    "Al seguir estos pasos, mantendrás la integridad de las sesiones y al mismo tiempo estratificarás por la variable objetivo `add_to_cart`, mejorando la consistencia y representatividad de tus conjuntos de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga global_stats\n",
    "# import pickle\n",
    "\n",
    "# with open('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/global_stats.pkl', 'rb') as f:\n",
    "#     global_stats = pickle.load(f)\n",
    "# print(\"global_stats cargado desde 'global_stats.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f13ad7036d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# Configuración para silenciar advertencias de downcasting\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "def preprocess_and_generate_shared_features(df, mode=\"train\"):\n",
    "    df = df.copy()  # Evitar SettingWithCopyWarning\n",
    "    print(f\"\\n--- Preprocesando y generando características compartidas ({mode}) ---\")\n",
    "\n",
    "    # Convertir datetime a numérico (segundos desde el epoch)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).astype(int) / 10**9\n",
    "    if \"timestamp_local\" in df.columns:\n",
    "        df[\"timestamp_local\"] = pd.to_datetime(df[\"timestamp_local\"]).astype(int) / 10**9\n",
    "\n",
    "    # Generar características compartidas\n",
    "    if \"partnumber\" in df.columns:\n",
    "        df[\"product_popularity\"] = df.groupby(\"partnumber\")[\"session_id\"].transform(\"nunique\")\n",
    "    if \"session_id\" in df.columns:\n",
    "        df[\"session_length\"] = df.groupby(\"session_id\")[\"partnumber\"].transform(\"count\")\n",
    "\n",
    "    # Utilizar variables categóricas directamente para discount_category\n",
    "    if \"discount_category\" in df.columns:\n",
    "        df[\"discount_category\"] = df[\"discount_category\"].astype(\"category\")\n",
    "\n",
    "    # Procesar similar_products como longitud de lista\n",
    "    if \"similar_products\" in df.columns:\n",
    "        df[\"similar_products_count\"] = df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "\n",
    "    # Manejar columnas no compatibles en modo `test`\n",
    "    if mode == \"test\":\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype not in [np.int32, np.int64, np.float32, np.float64, np.bool_]:\n",
    "                print(f\"Eliminando columna no compatible: {col}\")\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_global_stats(train_df):\n",
    "    print(\"\\n--- Generando estadísticas globales del entrenamiento ---\")\n",
    "    stats = {}\n",
    "    stats['product_popularity'] = train_df.groupby('partnumber')['session_id'].nunique()\n",
    "    stats['discount_category'] = train_df['discount_category'].value_counts(normalize=True).to_dict()\n",
    "    stats['cluster'] = train_df['cluster'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    # Asignar valores por defecto para test\n",
    "    stats['discount_category']['default'] = max(stats['discount_category'], key=stats['discount_category'].get)\n",
    "    stats['cluster']['default'] = max(stats['cluster'], key=stats['cluster'].get)\n",
    "    return stats\n",
    "\n",
    "def preprocess_test_with_enrichment(test_df, global_stats, train_features):\n",
    "    test_df = test_df.copy()  # Evitar SettingWithCopyWarning\n",
    "    print(\"\\n--- Preprocesando conjunto de prueba con enriquecimiento ---\")\n",
    "\n",
    "    if 'session_id' not in test_df.columns:\n",
    "        raise KeyError(\"El conjunto de prueba no contiene 'session_id' al inicio.\")\n",
    "\n",
    "    # Convertir columnas datetime a numérico\n",
    "    test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "    test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "    # Procesar similar_products si existe\n",
    "    if \"similar_products\" in test_df.columns:\n",
    "        test_df[\"similar_products_count\"] = test_df[\"similar_products\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        test_df.drop(columns=[\"similar_products\"], inplace=True)\n",
    "    elif 'similar_products_count' not in test_df.columns:\n",
    "        test_df['similar_products_count'] = 0\n",
    "\n",
    "    # Extraer características temporales\n",
    "    test_df['hour'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.hour\n",
    "    test_df['day_of_week'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "\n",
    "    # Generar características derivadas\n",
    "    test_df['session_length'] = test_df.groupby('session_id')['partnumber'].transform('count')\n",
    "    test_df['product_popularity'] = test_df['partnumber'].map(global_stats['product_popularity']).fillna(0).astype(float)\n",
    "\n",
    "    # Manejar discount_category como categórico\n",
    "    if 'discount_category' in test_df.columns:\n",
    "        test_df['discount_category'] = test_df['discount_category'].astype('category')\n",
    "    else:\n",
    "        default_cat = global_stats['discount_category']['default']\n",
    "        test_df['discount_category'] = pd.Series([default_cat] * len(test_df), index=test_df.index).astype('category')\n",
    "\n",
    "    if 'cluster' not in test_df.columns:\n",
    "        test_df['cluster'] = global_stats['cluster']['default']\n",
    "\n",
    "    for feature in train_features:\n",
    "        if feature not in test_df.columns:\n",
    "            print(f\"Agregando columna faltante: {feature}\")\n",
    "            test_df[feature] = 0\n",
    "\n",
    "    for col in ['session_id'] + train_features:\n",
    "        if col in test_df.columns:\n",
    "            if test_df[col].dtype == 'int64':\n",
    "                test_df[col] = test_df[col].astype(np.int32)\n",
    "            elif test_df[col].dtype == 'float64':\n",
    "                test_df[col] = test_df[col].astype(np.float32)\n",
    "\n",
    "    missing_features = [f for f in train_features if f not in test_df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Faltan características en el conjunto de prueba: {missing_features}\")\n",
    "\n",
    "    valid_cols = ['session_id'] + train_features\n",
    "    test_subset = test_df[valid_cols]\n",
    "\n",
    "    # Ampliar la lista de tipos permitidos para incluir categóricos\n",
    "    allowed_types = {\n",
    "        np.dtype('int32'),\n",
    "        np.dtype('float32'),\n",
    "        np.dtype('bool_'),\n",
    "        np.dtype('int16'),\n",
    "        np.dtype('int8'),\n",
    "        np.dtype('uint32'),\n",
    "        pd.UInt32Dtype()\n",
    "    }\n",
    "\n",
    "    # Verificar individualmente los tipos de cada columna, permitiendo tipos categóricos\n",
    "    invalid_columns = {}\n",
    "    for col, dt in test_subset.dtypes.items():\n",
    "        if dt not in allowed_types and not isinstance(dt, pd.CategoricalDtype):\n",
    "            invalid_columns[col] = dt\n",
    "\n",
    "    if invalid_columns:\n",
    "        raise ValueError(f\"Columnas con tipos de datos no válidos: {invalid_columns}\")\n",
    "\n",
    "    return test_subset\n",
    "\n",
    "def train_lambdamart_with_enrichment(train_df, val_df, model_path):\n",
    "    print(\"\\n--- Entrenando el modelo Lambdamart ---\")\n",
    "\n",
    "    # Preprocesar datos de entrenamiento y validación\n",
    "    train_df = preprocess_and_generate_shared_features(train_df, mode=\"train\")\n",
    "    val_df = preprocess_and_generate_shared_features(val_df, mode=\"train\")  # Usamos modo \"train\" para mantener etiquetas\n",
    "\n",
    "    if \"add_to_cart\" not in train_df.columns:\n",
    "        raise ValueError(\"La columna 'add_to_cart' no está presente en el conjunto de datos.\")\n",
    "\n",
    "    # Definir características y etiquetas para entrenamiento y validación\n",
    "    X_train = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y_train = train_df['add_to_cart']\n",
    "    groups_train = train_df['session_id'].value_counts().values\n",
    "\n",
    "    X_val = val_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "    y_val = val_df['add_to_cart']\n",
    "    groups_val = val_df['session_id'].value_counts().values\n",
    "\n",
    "    # Generar estadísticas globales usando el conjunto de entrenamiento\n",
    "    global_stats = generate_global_stats(train_df)\n",
    "\n",
    "    # Asegurar que discount_category es categórico en ambos conjuntos\n",
    "    for dataset in [X_train, X_val]:\n",
    "        if 'discount_category' in dataset.columns:\n",
    "            dataset['discount_category'] = dataset['discount_category'].astype('category')\n",
    "\n",
    "    # Identificar características categóricas\n",
    "    categorical_features = []\n",
    "    if 'discount_category' in X_train.columns and str(X_train['discount_category'].dtype).startswith('category'):\n",
    "        categorical_features.append('discount_category')\n",
    "\n",
    "    # Crear datasets de LightGBM para entrenamiento y validación\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train, categorical_feature=categorical_features)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train, categorical_feature=categorical_features)\n",
    "\n",
    "    # Configuración de hiperparámetros ajustados\n",
    "    # params = {\n",
    "    #     'objective': 'lambdarank',\n",
    "    #     'metric': 'ndcg',\n",
    "    #     'ndcg_eval_at': [1, 3, 5],\n",
    "    #     'learning_rate': 0.03,\n",
    "    #     'num_leaves': 40,\n",
    "    #     'max_bin': 255,\n",
    "    #     'min_data_in_leaf': 15,\n",
    "    #     'boosting_type': 'gbdt',\n",
    "    #     'feature_fraction': 0.8,\n",
    "    #     'bagging_fraction': 0.8,\n",
    "    #     'bagging_freq': 5,\n",
    "    #     'lambda_l1': 0.1,\n",
    "    #     'lambda_l2': 0.1,\n",
    "    #     'verbose': -1,\n",
    "    #     'n_jobs': -1\n",
    "    # }\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [1, 3, 5],\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 70,\n",
    "        'max_bin': 255,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    # Definir callbacks para early stopping y logueo\n",
    "    callbacks = [\n",
    "        early_stopping(stopping_rounds=50, verbose=True),\n",
    "        log_evaluation(period=10)\n",
    "    ]\n",
    "\n",
    "    # Entrenar el modelo con validación\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Guardar el modelo entrenado\n",
    "    model.save_model(model_path)\n",
    "    print(f\"\\n--- Modelo guardado en {model_path} ---\")\n",
    "\n",
    "    return X_train.columns.tolist(), global_stats\n",
    "\n",
    "def generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "    print(\"\\n--- Cargando datos de prueba ---\")\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    test_df = preprocess_test_with_enrichment(test_df, global_stats, train_features)\n",
    "\n",
    "    predictions = {}\n",
    "    if 'partnumber' in test_df.columns:\n",
    "        popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    print(\"\\n--- Generando predicciones para cada sesión ---\")\n",
    "    for session_id in test_df['session_id'].unique():\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    print(\"\\n--- Guardando predicciones ---\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\"target\": predictions}, f, indent=4)\n",
    "    print(f\"Predicciones guardadas en {output_path}\")\n",
    "\n",
    "def compute_metrics(predictions, ground_truth, k=5):\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for session_id, predicted in predictions.items():\n",
    "        true_products = ground_truth.get(session_id, set())\n",
    "        if not true_products:\n",
    "            continue\n",
    "\n",
    "        pred_set = set(predicted[:k])\n",
    "        precision = len(pred_set & true_products) / k\n",
    "        recall = len(pred_set & true_products) / len(true_products)\n",
    "\n",
    "        dcg = 0.0\n",
    "        for i, prod in enumerate(predicted[:k]):\n",
    "            rel = 1 if prod in true_products else 0\n",
    "            dcg += (2**rel - 1) / np.log2(i + 2)\n",
    "        ideal_rels = [1] * min(len(true_products), k) + [0] * (k - min(len(true_products), k))\n",
    "        idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_rels))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    avg_ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "\n",
    "    return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "def generate_predictions(model, df, global_stats, train_features):\n",
    "    predictions = {}\n",
    "    if 'partnumber' in df.columns:\n",
    "        popular_products = df['partnumber'].value_counts().index.tolist()\n",
    "    else:\n",
    "        popular_products = []\n",
    "\n",
    "    for session_id in df['session_id'].unique():\n",
    "        session_data = df[df['session_id'] == session_id].copy()\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = popular_products[:5]\n",
    "            continue\n",
    "\n",
    "        session_features = session_data[train_features]\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        # Obtener los 10 productos con mayor score para asegurar diversidad\n",
    "        recommended_products = (\n",
    "            session_data.nlargest(10, 'score')['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # Si obtenemos más de 5 productos únicos, tomar sólo los primeros 5\n",
    "        recommended_products = recommended_products[:5]\n",
    "\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "    return predictions\n",
    "\n",
    "# Rutas de archivos\n",
    "train_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl'\n",
    "test_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl'\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_shared_model_stratify_v5.txt'\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_shared_stratify_v5.json'\n",
    "\n",
    "# Cargar datos de entrenamiento completos\n",
    "full_train_df = pd.read_pickle(train_path)\n",
    "\n",
    "# 1. Agregar etiqueta a nivel de sesión\n",
    "session_labels = full_train_df.groupby('session_id')['add_to_cart'].max().reset_index()\n",
    "session_labels['label'] = session_labels['add_to_cart']\n",
    "\n",
    "# 2. Dividir sesiones en entrenamiento y validación estratificando por 'label'\n",
    "train_sessions, val_sessions = train_test_split(\n",
    "    session_labels['session_id'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=session_labels['label']\n",
    ")\n",
    "\n",
    "# 3. Filtrar el DataFrame original para mantener sesiones completas en cada conjunto\n",
    "train_df = full_train_df[full_train_df['session_id'].isin(train_sessions)]\n",
    "val_df = full_train_df[full_train_df['session_id'].isin(val_sessions)]\n",
    "\n",
    "# Generar estadísticas globales usando el conjunto de entrenamiento\n",
    "#global_stats = generate_global_stats(train_df)\n",
    "import pickle\n",
    "with open('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/global_stats.pkl', 'rb') as f:\n",
    "    global_stats = pickle.load(f)\n",
    "print(\"global_stats cargado desde 'global_stats.pkl'\")\n",
    "\n",
    "print('train_lambdamart_with_enrichment')\n",
    "# Entrenar modelo y obtener características de entrenamiento junto con estadísticas globales\n",
    "train_features, global_stats = train_lambdamart_with_enrichment(train_df, val_df, model_path)\n",
    "\n",
    "print('model')\n",
    "# Cargar el modelo entrenado para usarlo en predicciones (opcional si ya lo cargaste dentro de generate_predictions_with_enrichment)\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "print('val_df_processed')\n",
    "# Preprocesar datos de validación\n",
    "# val_df_processed = preprocess_test_with_enrichment(val_df.copy(), global_stats, train_features)\n",
    "val_df_processed = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_enriched_lambdamart_stratify.pkl')\n",
    "\n",
    "print('generate_predictions')\n",
    "# Generar predicciones en el conjunto de validación\n",
    "val_predictions = generate_predictions(model, val_df_processed, global_stats, train_features)\n",
    "\n",
    "print('ground_truth')\n",
    "# Preparar ground truth para el conjunto de validación\n",
    "ground_truth = {}\n",
    "for _, row in val_df.iterrows():\n",
    "    if row['add_to_cart'] == 1:\n",
    "        sid = str(row['session_id'])\n",
    "        ground_truth.setdefault(sid, set()).add(row['partnumber'])\n",
    "\n",
    "print('compute_metrics')\n",
    "# Calcular métricas en el conjunto de validación\n",
    "avg_precision, avg_recall, avg_ndcg = compute_metrics(val_predictions, ground_truth, k=5)\n",
    "print(f\"Precision@5: {avg_precision:.4f}, Recall@5: {avg_recall:.4f}, NDCG@5: {avg_ndcg:.4f}\")\n",
    "\n",
    "print('generate_predictions_with_enrichment')\n",
    "# Generar predicciones para el conjunto de prueba final\n",
    "generate_predictions_with_enrichment(model_path, test_path, output_path, global_stats, train_features)\n",
    "\n",
    "print('JSON')\n",
    "# Validar formato del JSON generado\n",
    "with open(output_path, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "print(\"\\n--- Validación del JSON generado ---\")\n",
    "print(json.dumps(predictions, indent=4)[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
