{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU configurada para LightGBM.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# Configurar LightGBM para uso dinámico de memoria en GPU\n",
    "def configure_gpu():\n",
    "    try:\n",
    "        import os\n",
    "        os.environ['LIGHTGBM_DEVICE'] = 'gpu'\n",
    "        os.environ['LIGHTGBM_GPU_PLATFORM_ID'] = '0'\n",
    "        os.environ['LIGHTGBM_GPU_DEVICE_ID'] = '0'\n",
    "        print(\"GPU configurada para LightGBM.\")\n",
    "    except ImportError:\n",
    "        print(\"Error al configurar GPU. Continuando con CPU.\")\n",
    "\n",
    "configure_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando datos ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Cargando datos ---\")\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_final.pkl')\n",
    "test_df= pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['session_id', 'date', 'timestamp_local', 'user_id', 'country',\n",
      "       'partnumber', 'device_type', 'pagetype'],\n",
      "      dtype='object')\n",
      "Index(['session_id', 'date', 'timestamp_local', 'add_to_cart', 'user_id',\n",
      "       'country', 'partnumber', 'device_type', 'pagetype', 'hour',\n",
      "       'day_of_week', 'week', 'session_interactions', 'discount_category',\n",
      "       'similar_products', 'color_id', 'family', 'cod_section', 'discount',\n",
      "       'popularity', 'cluster'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(test_df.columns)\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocesando datos ---\n",
      "\n",
      "--- Dividiendo datos ---\n",
      "\n",
      "--- Configurando y entrenando el modelo LambdaMART ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.847607\ttrain's ndcg@3: 0.862816\ttrain's ndcg@5: 0.873727\tval's ndcg@1: 0.846946\tval's ndcg@3: 0.862283\tval's ndcg@5: 0.873344\n",
      "[20]\ttrain's ndcg@1: 0.848607\ttrain's ndcg@3: 0.863873\ttrain's ndcg@5: 0.874765\tval's ndcg@1: 0.847769\tval's ndcg@3: 0.863228\tval's ndcg@5: 0.874127\n",
      "[30]\ttrain's ndcg@1: 0.849251\ttrain's ndcg@3: 0.86456\ttrain's ndcg@5: 0.875424\tval's ndcg@1: 0.848557\tval's ndcg@3: 0.863929\tval's ndcg@5: 0.874886\n",
      "[40]\ttrain's ndcg@1: 0.849653\ttrain's ndcg@3: 0.864986\ttrain's ndcg@5: 0.875834\tval's ndcg@1: 0.848723\tval's ndcg@3: 0.864267\tval's ndcg@5: 0.87516\n",
      "[50]\ttrain's ndcg@1: 0.849938\ttrain's ndcg@3: 0.865301\ttrain's ndcg@5: 0.876147\tval's ndcg@1: 0.849026\tval's ndcg@3: 0.86453\tval's ndcg@5: 0.875442\n",
      "[60]\ttrain's ndcg@1: 0.850298\ttrain's ndcg@3: 0.865639\ttrain's ndcg@5: 0.876479\tval's ndcg@1: 0.849165\tval's ndcg@3: 0.864781\tval's ndcg@5: 0.8757\n",
      "[70]\ttrain's ndcg@1: 0.850516\ttrain's ndcg@3: 0.865864\ttrain's ndcg@5: 0.876697\tval's ndcg@1: 0.849361\tval's ndcg@3: 0.864928\tval's ndcg@5: 0.875827\n",
      "[80]\ttrain's ndcg@1: 0.850678\ttrain's ndcg@3: 0.866021\ttrain's ndcg@5: 0.876857\tval's ndcg@1: 0.849581\tval's ndcg@3: 0.865064\tval's ndcg@5: 0.875978\n",
      "[90]\ttrain's ndcg@1: 0.850868\ttrain's ndcg@3: 0.866188\ttrain's ndcg@5: 0.877\tval's ndcg@1: 0.849849\tval's ndcg@3: 0.865252\tval's ndcg@5: 0.87614\n",
      "[100]\ttrain's ndcg@1: 0.851071\ttrain's ndcg@3: 0.866373\ttrain's ndcg@5: 0.877177\tval's ndcg@1: 0.849933\tval's ndcg@3: 0.865325\tval's ndcg@5: 0.876247\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttrain's ndcg@1: 0.851\ttrain's ndcg@3: 0.866321\ttrain's ndcg@5: 0.877134\tval's ndcg@1: 0.849934\tval's ndcg@3: 0.865329\tval's ndcg@5: 0.876238\n",
      "\n",
      "--- Guardando el modelo ---\n",
      "\n",
      "--- Modelo entrenado y guardado con éxito ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    print(\"\\n--- Preprocesando datos ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "    categorical_columns = ['discount_category', 'similar_products']\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocesar los datos\n",
    "train_df = preprocess_data(train_df)\n",
    "\n",
    "# Asegurar que todas las columnas sean numéricas o booleanas\n",
    "compatible_columns = train_df.select_dtypes(include=['int64', 'float64', 'bool', 'int32', 'int16', 'int8']).columns\n",
    "train_df = train_df[compatible_columns]\n",
    "\n",
    "# Separar las características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Dividir datos en entrenamiento y validación respetando los grupos\n",
    "print(\"\\n--- Dividiendo datos ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "groups_train = groups.iloc[train_idx].value_counts().values\n",
    "groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "print(\"\\n--- Configurando y entrenando el modelo LambdaMART ---\")\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu'  # Cambiado para CPU\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping(stopping_rounds=50, verbose=True),\n",
    "    log_evaluation(period=10)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\"\\n--- Guardando el modelo ---\")\n",
    "model.save_model('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_model.txt')\n",
    "\n",
    "print(\"\\n--- Modelo entrenado y guardado con éxito ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocesando datos ---\n",
      "\n",
      "--- Dividiendo datos ---\n",
      "\n",
      "--- Configurando y entrenando el modelo LambdaMART ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.847609\ttrain's ndcg@3: 0.862797\ttrain's ndcg@5: 0.873717\tval's ndcg@1: 0.847182\tval's ndcg@3: 0.862263\tval's ndcg@5: 0.873228\n",
      "[20]\ttrain's ndcg@1: 0.848629\ttrain's ndcg@3: 0.863958\ttrain's ndcg@5: 0.874863\tval's ndcg@1: 0.848183\tval's ndcg@3: 0.863404\tval's ndcg@5: 0.87431\n",
      "[30]\ttrain's ndcg@1: 0.849208\ttrain's ndcg@3: 0.864619\ttrain's ndcg@5: 0.875463\tval's ndcg@1: 0.848535\tval's ndcg@3: 0.863813\tval's ndcg@5: 0.874794\n",
      "[40]\ttrain's ndcg@1: 0.849541\ttrain's ndcg@3: 0.864961\ttrain's ndcg@5: 0.875813\tval's ndcg@1: 0.848947\tval's ndcg@3: 0.864223\tval's ndcg@5: 0.875164\n",
      "[50]\ttrain's ndcg@1: 0.849929\ttrain's ndcg@3: 0.865359\ttrain's ndcg@5: 0.876201\tval's ndcg@1: 0.849245\tval's ndcg@3: 0.864519\tval's ndcg@5: 0.875475\n",
      "[60]\ttrain's ndcg@1: 0.850132\ttrain's ndcg@3: 0.865593\ttrain's ndcg@5: 0.876416\tval's ndcg@1: 0.849415\tval's ndcg@3: 0.864659\tval's ndcg@5: 0.875609\n",
      "[70]\ttrain's ndcg@1: 0.85037\ttrain's ndcg@3: 0.865793\ttrain's ndcg@5: 0.87659\tval's ndcg@1: 0.849666\tval's ndcg@3: 0.864815\tval's ndcg@5: 0.875795\n",
      "[80]\ttrain's ndcg@1: 0.850507\ttrain's ndcg@3: 0.865913\ttrain's ndcg@5: 0.876709\tval's ndcg@1: 0.849679\tval's ndcg@3: 0.864876\tval's ndcg@5: 0.875835\n",
      "[90]\ttrain's ndcg@1: 0.850769\ttrain's ndcg@3: 0.866192\ttrain's ndcg@5: 0.876966\tval's ndcg@1: 0.849919\tval's ndcg@3: 0.865167\tval's ndcg@5: 0.876094\n",
      "[100]\ttrain's ndcg@1: 0.851038\ttrain's ndcg@3: 0.866466\ttrain's ndcg@5: 0.877246\tval's ndcg@1: 0.850124\tval's ndcg@3: 0.86538\tval's ndcg@5: 0.876259\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttrain's ndcg@1: 0.851038\ttrain's ndcg@3: 0.866466\ttrain's ndcg@5: 0.877246\tval's ndcg@1: 0.850124\tval's ndcg@3: 0.86538\tval's ndcg@5: 0.876259\n",
      "\n",
      "--- Guardando el modelo ---\n",
      "\n",
      "--- Modelo entrenado y guardado con éxito ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    print(\"\\n--- Preprocesando datos ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "    categorical_columns = ['discount_category', 'similar_products']\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocesar los datos\n",
    "train_df = preprocess_data(train_df)\n",
    "\n",
    "# Asegurar que todas las columnas sean numéricas o booleanas\n",
    "compatible_columns = train_df.select_dtypes(include=['int64', 'float64', 'bool', 'int32', 'int16', 'int8']).columns\n",
    "train_df = train_df[compatible_columns]\n",
    "\n",
    "# Separar las características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Dividir datos en entrenamiento y validación respetando los grupos\n",
    "print(\"\\n--- Dividiendo datos ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "groups_train = groups.iloc[train_idx].value_counts().values\n",
    "groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "print(\"\\n--- Configurando y entrenando el modelo LambdaMART ---\")\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.03,  # Reducir tasa de aprendizaje\n",
    "    'num_leaves': 40,      # Aumentar el número de hojas\n",
    "    'min_data_in_leaf': 15,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu'        # Cambiar a GPU\n",
    "}\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping(stopping_rounds=50, verbose=True),\n",
    "    log_evaluation(period=10)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\"\\n--- Guardando el modelo ---\")\n",
    "model.save_model('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_model.txt')\n",
    "\n",
    "print(\"\\n--- Modelo entrenado y guardado con éxito ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocesando datos ---\n",
      "\n",
      "--- Dividiendo datos ---\n",
      "\n",
      "--- Configurando y entrenando el modelo LambdaMART ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.848265\ttrain's ndcg@3: 0.863583\ttrain's ndcg@5: 0.874437\tval's ndcg@1: 0.84776\tval's ndcg@3: 0.863032\tval's ndcg@5: 0.874029\n",
      "[20]\ttrain's ndcg@1: 0.849301\ttrain's ndcg@3: 0.864663\ttrain's ndcg@5: 0.875493\tval's ndcg@1: 0.848581\tval's ndcg@3: 0.863982\tval's ndcg@5: 0.874905\n",
      "[30]\ttrain's ndcg@1: 0.849852\ttrain's ndcg@3: 0.865224\ttrain's ndcg@5: 0.876051\tval's ndcg@1: 0.849229\tval's ndcg@3: 0.864419\tval's ndcg@5: 0.875388\n",
      "[40]\ttrain's ndcg@1: 0.850244\ttrain's ndcg@3: 0.865644\ttrain's ndcg@5: 0.876474\tval's ndcg@1: 0.849721\tval's ndcg@3: 0.864789\tval's ndcg@5: 0.875769\n",
      "[50]\ttrain's ndcg@1: 0.850557\ttrain's ndcg@3: 0.865971\ttrain's ndcg@5: 0.87679\tval's ndcg@1: 0.849843\tval's ndcg@3: 0.864944\tval's ndcg@5: 0.876002\n",
      "[60]\ttrain's ndcg@1: 0.850779\ttrain's ndcg@3: 0.866218\ttrain's ndcg@5: 0.877004\tval's ndcg@1: 0.849883\tval's ndcg@3: 0.865175\tval's ndcg@5: 0.876145\n",
      "[70]\ttrain's ndcg@1: 0.851065\ttrain's ndcg@3: 0.866455\ttrain's ndcg@5: 0.877251\tval's ndcg@1: 0.85013\tval's ndcg@3: 0.865389\tval's ndcg@5: 0.87632\n",
      "[80]\ttrain's ndcg@1: 0.851187\ttrain's ndcg@3: 0.866572\ttrain's ndcg@5: 0.877357\tval's ndcg@1: 0.850201\tval's ndcg@3: 0.86547\tval's ndcg@5: 0.876379\n",
      "[90]\ttrain's ndcg@1: 0.851399\ttrain's ndcg@3: 0.866773\ttrain's ndcg@5: 0.877561\tval's ndcg@1: 0.850293\tval's ndcg@3: 0.8656\tval's ndcg@5: 0.876486\n",
      "[100]\ttrain's ndcg@1: 0.851534\ttrain's ndcg@3: 0.866917\ttrain's ndcg@5: 0.877682\tval's ndcg@1: 0.850423\tval's ndcg@3: 0.865767\tval's ndcg@5: 0.876663\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttrain's ndcg@1: 0.851534\ttrain's ndcg@3: 0.866917\ttrain's ndcg@5: 0.877682\tval's ndcg@1: 0.850423\tval's ndcg@3: 0.865767\tval's ndcg@5: 0.876663\n",
      "\n",
      "--- Guardando el modelo ---\n",
      "\n",
      "--- Modelo entrenado y guardado con éxito ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    print(\"\\n--- Preprocesando datos ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "\n",
    "    categorical_columns = ['discount_category', 'similar_products']\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocesar los datos\n",
    "train_df = preprocess_data(train_df)\n",
    "\n",
    "# Asegurar que todas las columnas sean numéricas o booleanas\n",
    "compatible_columns = train_df.select_dtypes(include=['int64', 'float64', 'bool', 'int32', 'int16', 'int8']).columns\n",
    "train_df = train_df[compatible_columns]\n",
    "\n",
    "# Separar las características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Dividir datos en entrenamiento y validación respetando los grupos\n",
    "print(\"\\n--- Dividiendo datos ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "groups_train = groups.iloc[train_idx].value_counts().values\n",
    "groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "print(\"\\n--- Configurando y entrenando el modelo LambdaMART ---\")\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.02,  # Reducir tasa de aprendizaje para un ajuste más fino\n",
    "    'num_leaves': 50,      # Capturar más patrones\n",
    "    'min_data_in_leaf': 10,  # Más flexibilidad en las hojas\n",
    "    'max_bin': 255,  # Granularidad en las características\n",
    "    'feature_fraction': 0.8,  # Submuestreo de características\n",
    "    'bagging_fraction': 0.8,  # Submuestreo de datos\n",
    "    'bagging_freq': 5,  # Frecuencia de muestreo\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu'  # Cambiar a 'gpu' si es posible\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping(stopping_rounds=50, verbose=True),\n",
    "    log_evaluation(period=10)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\"\\n--- Guardando el modelo ---\")\n",
    "model.save_model('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_model_v2.txt')\n",
    "\n",
    "print(\"\\n--- Modelo entrenado y guardado con éxito ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo utilizando únicamente las características alineadas con el dataset de test(evaluación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocesando datos ---\n",
      "\n",
      "--- Dividiendo datos ---\n",
      "\n",
      "--- Configurando y entrenando el modelo LambdaMART ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.842234\ttrain's ndcg@3: 0.8571\ttrain's ndcg@5: 0.86801\tval's ndcg@1: 0.841675\tval's ndcg@3: 0.856382\tval's ndcg@5: 0.867438\n",
      "[20]\ttrain's ndcg@1: 0.842289\ttrain's ndcg@3: 0.857207\ttrain's ndcg@5: 0.868128\tval's ndcg@1: 0.841882\tval's ndcg@3: 0.856652\tval's ndcg@5: 0.86765\n",
      "[30]\ttrain's ndcg@1: 0.842335\ttrain's ndcg@3: 0.85724\ttrain's ndcg@5: 0.868167\tval's ndcg@1: 0.841814\tval's ndcg@3: 0.856495\tval's ndcg@5: 0.867429\n",
      "[40]\ttrain's ndcg@1: 0.842551\ttrain's ndcg@3: 0.857358\ttrain's ndcg@5: 0.868279\tval's ndcg@1: 0.841843\tval's ndcg@3: 0.856529\tval's ndcg@5: 0.867586\n",
      "[50]\ttrain's ndcg@1: 0.842476\ttrain's ndcg@3: 0.857344\ttrain's ndcg@5: 0.868283\tval's ndcg@1: 0.841838\tval's ndcg@3: 0.856515\tval's ndcg@5: 0.867532\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's ndcg@1: 0.842482\ttrain's ndcg@3: 0.857505\ttrain's ndcg@5: 0.868481\tval's ndcg@1: 0.841742\tval's ndcg@3: 0.856876\tval's ndcg@5: 0.867907\n",
      "\n",
      "--- Guardando el modelo ---\n",
      "\n",
      "--- Modelo entrenado y guardado con éxito ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# --- Cargar y preprocesar datos ---\n",
    "def preprocess_data(df, columns_to_keep):\n",
    "    print(\"\\n--- Preprocesando datos ---\")\n",
    "    df = df[columns_to_keep]  # Seleccionar solo las columnas permitidas\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    return df\n",
    "\n",
    "# Columnas permitidas (comunes entre `train` y `test`)\n",
    "columns_to_keep = [\n",
    "    'session_id', 'date', 'timestamp_local', 'user_id', \n",
    "    'country', 'partnumber', 'device_type', 'pagetype', 'add_to_cart'\n",
    "]\n",
    "\n",
    "# Cargar y preprocesar el dataset de entrenamiento\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data.pkl')\n",
    "train_df = preprocess_data(train_df, columns_to_keep)\n",
    "\n",
    "# Separar características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Dividir datos en entrenamiento y validación respetando los grupos\n",
    "print(\"\\n--- Dividiendo datos ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "groups_train = groups.iloc[train_idx].value_counts().values\n",
    "groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "# --- Configurar y entrenar el modelo ---\n",
    "print(\"\\n--- Configurando y entrenando el modelo LambdaMART ---\")\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 40,\n",
    "    'min_data_in_leaf': 15,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping(stopping_rounds=50, verbose=True),\n",
    "    log_evaluation(period=10)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "print(\"\\n--- Guardando el modelo ---\")\n",
    "model.save_model('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_aligned_model.txt')\n",
    "print(\"\\n--- Modelo entrenado y guardado con éxito ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando el conjunto de validación ---\n",
      "\n",
      "--- Generando predicciones ---\n",
      "\n",
      "--- Calculando métricas de evaluación ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Métricas de Evaluación ---\n",
      "ROC AUC Score: 0.6503\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Accuracy: 0.9405\n",
      "\n",
      "--- Classification Report ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97   8741049\n",
      "           1       0.00      0.00      0.00    553207\n",
      "\n",
      "    accuracy                           0.94   9294256\n",
      "   macro avg       0.47      0.50      0.48   9294256\n",
      "weighted avg       0.88      0.94      0.91   9294256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/.env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- Cargar el modelo entrenado ---\n",
    "print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_aligned_model.txt'\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# --- Cargar y preprocesar el conjunto de validación ---\n",
    "print(\"\\n--- Cargando el conjunto de validación ---\")\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data.pkl')\n",
    "\n",
    "# Preprocesamiento básico\n",
    "def preprocess_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    return df\n",
    "\n",
    "train_df = preprocess_data(train_df)\n",
    "\n",
    "# Separar características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Dividir datos en entrenamiento y validación respetando los grupos\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_val = X.iloc[val_idx]\n",
    "y_val = y.iloc[val_idx]\n",
    "\n",
    "# --- Generar predicciones ---\n",
    "print(\"\\n--- Generando predicciones ---\")\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Convertir las probabilidades en etiquetas binarias\n",
    "y_val_pred_binary = np.where(y_val_pred >= 0.5, 1, 0)\n",
    "\n",
    "# --- Cálculo de métricas adicionales ---\n",
    "print(\"\\n--- Calculando métricas de evaluación ---\")\n",
    "roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred_binary)\n",
    "recall = recall_score(y_val, y_val_pred_binary)\n",
    "f1 = f1_score(y_val, y_val_pred_binary)\n",
    "accuracy = accuracy_score(y_val, y_val_pred_binary)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n--- Métricas de Evaluación ---\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Reporte completo\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_val, y_val_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando el conjunto de prueba ---\n",
      "Columnas en test_df: Index(['session_id', 'date', 'timestamp_local', 'user_id', 'country',\n",
      "       'partnumber', 'device_type', 'pagetype'],\n",
      "      dtype='object')\n",
      "\n",
      "--- Calculando productos populares ---\n",
      "\n",
      "--- Preprocesando `test_df` ---\n",
      "Columnas procesadas en test_df: Index(['session_id', 'date', 'timestamp_local', 'user_id', 'country',\n",
      "       'partnumber', 'device_type', 'pagetype'],\n",
      "      dtype='object')\n",
      "\n",
      "--- Generando predicciones ---\n",
      "\n",
      "--- Guardando el archivo predictions_3_final.json ---\n",
      "Archivo predictions_3_final.json generado con éxito en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_final.json.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "\n",
    "# --- Cargar el modelo entrenado ---\n",
    "print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_aligned_model.txt'\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# --- Cargar el conjunto de prueba ---\n",
    "print(\"\\n--- Cargando el conjunto de prueba ---\")\n",
    "test_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl')\n",
    "print(\"Columnas en test_df:\", test_df.columns)\n",
    "\n",
    "# --- Obtener productos populares ---\n",
    "print(\"\\n--- Calculando productos populares ---\")\n",
    "popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "# --- Preprocesar el conjunto de prueba ---\n",
    "def preprocess_test_data(test_df):\n",
    "    print(\"\\n--- Preprocesando `test_df` ---\")\n",
    "    test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "    test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "    feature_columns = [\n",
    "        'session_id', 'date', 'timestamp_local', 'user_id', 'country',\n",
    "        'partnumber', 'device_type', 'pagetype'\n",
    "    ]\n",
    "    return test_df[feature_columns]\n",
    "\n",
    "test_df = preprocess_test_data(test_df)\n",
    "print(\"Columnas procesadas en test_df:\", test_df.columns)\n",
    "\n",
    "# --- Generar predicciones para cada session_id ---\n",
    "print(\"\\n--- Generando predicciones ---\")\n",
    "session_ids = test_df['session_id'].unique()\n",
    "predictions = {}\n",
    "\n",
    "for session_id in session_ids:\n",
    "    session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "\n",
    "    if session_data.empty:\n",
    "        predictions[str(session_id)] = []\n",
    "        continue\n",
    "\n",
    "    session_features = session_data.drop(['session_id'], axis=1)\n",
    "\n",
    "    if session_features.shape[1] != model.num_feature():\n",
    "        raise ValueError(f\"El número de características no coincide con el modelo. \"\n",
    "                         f\"Esperado: {model.num_feature()}, Actual: {session_features.shape[1]}\")\n",
    "\n",
    "    session_data['score'] = model.predict(session_features)\n",
    "\n",
    "    recommended_products = (\n",
    "        session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "        .drop_duplicates()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # Completar con productos populares si faltan\n",
    "    for product in popular_products:\n",
    "        if len(recommended_products) >= 5:\n",
    "            break\n",
    "        if product not in recommended_products:\n",
    "            recommended_products.append(product)\n",
    "\n",
    "    # Garantizar exactamente 5 valores únicos\n",
    "    predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "# --- Guardar el archivo predictions_3.json ---\n",
    "print(\"\\n--- Guardando el archivo predictions_3_final.json ---\")\n",
    "output = {\"target\": predictions}\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_final.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "print(f\"Archivo predictions_3_final.json generado con éxito en {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 3: 26% completada\n",
    "\n",
    "235/900 puntos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocesando datos ---\n",
      "\n",
      "--- Dividiendo datos ---\n",
      "\n",
      "--- Configurando y entrenando el modelo LambdaMART ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.843253\ttrain's ndcg@3: 0.858145\ttrain's ndcg@5: 0.869074\tval's ndcg@1: 0.842437\tval's ndcg@3: 0.857323\tval's ndcg@5: 0.868362\n",
      "[20]\ttrain's ndcg@1: 0.843383\ttrain's ndcg@3: 0.858346\ttrain's ndcg@5: 0.869241\tval's ndcg@1: 0.842444\tval's ndcg@3: 0.857375\tval's ndcg@5: 0.868239\n",
      "[30]\ttrain's ndcg@1: 0.843541\ttrain's ndcg@3: 0.858482\ttrain's ndcg@5: 0.869385\tval's ndcg@1: 0.842645\tval's ndcg@3: 0.857444\tval's ndcg@5: 0.868347\n",
      "[40]\ttrain's ndcg@1: 0.843909\ttrain's ndcg@3: 0.858738\ttrain's ndcg@5: 0.869665\tval's ndcg@1: 0.842535\tval's ndcg@3: 0.857529\tval's ndcg@5: 0.868446\n",
      "[50]\ttrain's ndcg@1: 0.844293\ttrain's ndcg@3: 0.859177\ttrain's ndcg@5: 0.870055\tval's ndcg@1: 0.842964\tval's ndcg@3: 0.857816\tval's ndcg@5: 0.868718\n",
      "[60]\ttrain's ndcg@1: 0.844481\ttrain's ndcg@3: 0.859532\ttrain's ndcg@5: 0.870347\tval's ndcg@1: 0.843056\tval's ndcg@3: 0.857832\tval's ndcg@5: 0.868753\n",
      "[70]\ttrain's ndcg@1: 0.844944\ttrain's ndcg@3: 0.859923\ttrain's ndcg@5: 0.870739\tval's ndcg@1: 0.842981\tval's ndcg@3: 0.857854\tval's ndcg@5: 0.868835\n",
      "[80]\ttrain's ndcg@1: 0.845433\ttrain's ndcg@3: 0.860322\ttrain's ndcg@5: 0.871146\tval's ndcg@1: 0.843194\tval's ndcg@3: 0.858047\tval's ndcg@5: 0.868971\n",
      "[90]\ttrain's ndcg@1: 0.845827\ttrain's ndcg@3: 0.860685\ttrain's ndcg@5: 0.871484\tval's ndcg@1: 0.843279\tval's ndcg@3: 0.858078\tval's ndcg@5: 0.869014\n",
      "[100]\ttrain's ndcg@1: 0.846149\ttrain's ndcg@3: 0.861009\ttrain's ndcg@5: 0.871824\tval's ndcg@1: 0.843396\tval's ndcg@3: 0.858179\tval's ndcg@5: 0.869097\n",
      "[110]\ttrain's ndcg@1: 0.846536\ttrain's ndcg@3: 0.861341\ttrain's ndcg@5: 0.87211\tval's ndcg@1: 0.84332\tval's ndcg@3: 0.858093\tval's ndcg@5: 0.869044\n",
      "[120]\ttrain's ndcg@1: 0.846831\ttrain's ndcg@3: 0.86165\ttrain's ndcg@5: 0.872414\tval's ndcg@1: 0.843318\tval's ndcg@3: 0.858125\tval's ndcg@5: 0.869087\n",
      "[130]\ttrain's ndcg@1: 0.847119\ttrain's ndcg@3: 0.861975\ttrain's ndcg@5: 0.872721\tval's ndcg@1: 0.843336\tval's ndcg@3: 0.858146\tval's ndcg@5: 0.869129\n",
      "[140]\ttrain's ndcg@1: 0.847427\ttrain's ndcg@3: 0.862232\ttrain's ndcg@5: 0.872953\tval's ndcg@1: 0.843312\tval's ndcg@3: 0.858139\tval's ndcg@5: 0.869059\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttrain's ndcg@1: 0.846071\ttrain's ndcg@3: 0.860938\ttrain's ndcg@5: 0.871735\tval's ndcg@1: 0.843356\tval's ndcg@3: 0.858177\tval's ndcg@5: 0.869138\n",
      "\n",
      "--- Guardando el modelo ---\n",
      "\n",
      "--- Modelo guardado en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model.txt ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# --- Preprocesamiento de datos ---\n",
    "def preprocess_data(df):\n",
    "    print(\"\\n--- Preprocesando datos ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    return df\n",
    "\n",
    "# Cargar y preprocesar datos\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data.pkl')\n",
    "train_df = preprocess_data(train_df)\n",
    "\n",
    "# Separar características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Dividir datos\n",
    "print(\"\\n--- Dividiendo datos ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "groups_train = groups.iloc[train_idx].value_counts().values\n",
    "groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "# Configurar y entrenar modelo\n",
    "print(\"\\n--- Configurando y entrenando el modelo LambdaMART ---\")\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.005,\n",
    "    'num_leaves': 100,\n",
    "    'min_data_in_leaf': 25,\n",
    "    'max_bin': 200,\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping(stopping_rounds=50, verbose=True),\n",
    "    log_evaluation(period=10)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=200,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Guardar modelo\n",
    "print(\"\\n--- Guardando el modelo ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model.txt'\n",
    "model.save_model(model_path)\n",
    "print(f\"\\n--- Modelo guardado en {model_path} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluando el modelo ---\n",
      "\n",
      "--- Métricas de Evaluación ---\n",
      "ROC AUC Score: 0.6537\n",
      "Precision: 0.9990\n",
      "Recall: 0.1081\n",
      "F1 Score: 0.1951\n",
      "Accuracy: 0.9469\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97   8741049\n",
      "           1       1.00      0.11      0.20    553207\n",
      "\n",
      "    accuracy                           0.95   9294256\n",
      "   macro avg       0.97      0.55      0.58   9294256\n",
      "weighted avg       0.95      0.95      0.93   9294256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# --- Generar predicciones para conjunto de validación ---\n",
    "print(\"\\n--- Evaluando el modelo ---\")\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Convertir las probabilidades en etiquetas binarias para métricas de clasificación\n",
    "y_val_pred_binary = np.where(y_val_pred >= 0.5, 1, 0)\n",
    "\n",
    "# --- Cálculo de métricas adicionales ---\n",
    "roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred_binary)\n",
    "recall = recall_score(y_val, y_val_pred_binary)\n",
    "f1 = f1_score(y_val, y_val_pred_binary)\n",
    "accuracy = accuracy_score(y_val, y_val_pred_binary)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\n--- Métricas de Evaluación ---\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Reporte completo\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_val, y_val_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando el conjunto de prueba ---\n",
      "\n",
      "--- Preprocesando conjunto de prueba ---\n",
      "\n",
      "--- Generando predicciones ---\n",
      "\n",
      "--- Guardando predicciones en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched.json ---\n",
      "Predicciones guardadas con éxito.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "\n",
    "# Cargar modelo entrenado\n",
    "print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_enriched_model.txt'\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# Cargar conjunto de prueba\n",
    "print(\"\\n--- Cargando el conjunto de prueba ---\")\n",
    "test_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl')\n",
    "\n",
    "# Preprocesar conjunto de prueba\n",
    "print(\"\\n--- Preprocesando conjunto de prueba ---\")\n",
    "test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "test_df['hour'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.hour\n",
    "test_df['day_of_week'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "\n",
    "# Generar predicciones\n",
    "print(\"\\n--- Generando predicciones ---\")\n",
    "session_ids = test_df['session_id'].unique()\n",
    "predictions = {}\n",
    "\n",
    "popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "for session_id in session_ids:\n",
    "    session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "    if session_data.empty:\n",
    "        predictions[str(session_id)] = []\n",
    "        continue\n",
    "\n",
    "    session_features = session_data.drop(['session_id'], axis=1)\n",
    "    if session_features.shape[1] != model.num_feature():\n",
    "        raise ValueError(f\"El número de características no coincide con el modelo. \"\n",
    "                         f\"Esperado: {model.num_feature()}, Actual: {session_features.shape[1]}\")\n",
    "    \n",
    "    session_data['score'] = model.predict(session_features)\n",
    "    recommended_products = (\n",
    "        session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "        .drop_duplicates()\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    # Completar con productos populares si faltan\n",
    "    for product in popular_products:\n",
    "        if len(recommended_products) >= 5:\n",
    "            break\n",
    "        if product not in recommended_products:\n",
    "            recommended_products.append(product)\n",
    "    \n",
    "    predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "# Guardar predicciones\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_enriched.json'\n",
    "print(f\"\\n--- Guardando predicciones en {output_path} ---\")\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump({\"target\": predictions}, f, indent=4)\n",
    "print(\"Predicciones guardadas con éxito.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocesando datos ---\n",
      "\n",
      "--- Calculado scale_pos_weight: 15.95 ---\n",
      "\n",
      "--- Dividiendo datos ---\n",
      "\n",
      "--- Configurando y entrenando el modelo LambdaMART ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's ndcg@1: 0.842865\ttrain's ndcg@3: 0.857555\ttrain's ndcg@5: 0.868467\tval's ndcg@1: 0.842335\tval's ndcg@3: 0.857066\tval's ndcg@5: 0.867949\n",
      "[20]\ttrain's ndcg@1: 0.842795\ttrain's ndcg@3: 0.857643\ttrain's ndcg@5: 0.868573\tval's ndcg@1: 0.842271\tval's ndcg@3: 0.857012\tval's ndcg@5: 0.86807\n",
      "[30]\ttrain's ndcg@1: 0.842801\ttrain's ndcg@3: 0.857622\ttrain's ndcg@5: 0.868538\tval's ndcg@1: 0.842333\tval's ndcg@3: 0.857153\tval's ndcg@5: 0.868048\n",
      "[40]\ttrain's ndcg@1: 0.842782\ttrain's ndcg@3: 0.857621\ttrain's ndcg@5: 0.868574\tval's ndcg@1: 0.842069\tval's ndcg@3: 0.856895\tval's ndcg@5: 0.867818\n",
      "[50]\ttrain's ndcg@1: 0.84276\ttrain's ndcg@3: 0.857571\ttrain's ndcg@5: 0.868533\tval's ndcg@1: 0.842264\tval's ndcg@3: 0.857073\tval's ndcg@5: 0.868044\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttrain's ndcg@1: 0.843152\ttrain's ndcg@3: 0.858084\ttrain's ndcg@5: 0.869018\tval's ndcg@1: 0.84269\tval's ndcg@3: 0.857657\tval's ndcg@5: 0.868589\n",
      "\n",
      "--- Guardando el modelo ---\n",
      "\n",
      "--- Modelo guardado en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_weighted_model.txt ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "# --- Preprocesamiento de datos ---\n",
    "def preprocess_data(df):\n",
    "    print(\"\\n--- Preprocesando datos ---\")\n",
    "    df['date'] = pd.to_datetime(df['date']).astype(int) / 10**9\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local']).astype(int) / 10**9\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp_local'], unit='s').dt.dayofweek\n",
    "    return df\n",
    "\n",
    "# Cargar y preprocesar datos\n",
    "train_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data.pkl')\n",
    "train_df = preprocess_data(train_df)\n",
    "\n",
    "# Separar características, etiquetas y grupos\n",
    "X = train_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "y = train_df['add_to_cart']\n",
    "groups = train_df['session_id']\n",
    "\n",
    "# Calcular scale_pos_weight\n",
    "pos_weight = len(y[y == 0]) / len(y[y == 1])\n",
    "print(f\"\\n--- Calculado scale_pos_weight: {pos_weight:.2f} ---\")\n",
    "\n",
    "# Dividir datos\n",
    "print(\"\\n--- Dividiendo datos ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "groups_train = groups.iloc[train_idx].value_counts().values\n",
    "groups_val = groups.iloc[val_idx].value_counts().values\n",
    "\n",
    "# Configurar y entrenar modelo\n",
    "print(\"\\n--- Configurando y entrenando el modelo LambdaMART ---\")\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=groups_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, group=groups_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [1, 3, 5],\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 40,\n",
    "    'min_data_in_leaf': 15,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu',\n",
    "    'scale_pos_weight': pos_weight  # Balancear las clases\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping(stopping_rounds=50, verbose=True),\n",
    "    log_evaluation(period=10)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=200,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Guardar modelo\n",
    "print(\"\\n--- Guardando el modelo ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_weighted_model.txt'\n",
    "model.save_model(model_path)\n",
    "print(f\"\\n--- Modelo guardado en {model_path} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando datos de validación ---\n",
      "\n",
      "--- Preprocesando datos ---\n",
      "\n",
      "--- Generando predicciones ---\n",
      "\n",
      "--- Calculando métricas ---\n",
      "ROC AUC Score: 0.6517\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Accuracy: 0.9410\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97  43805662\n",
      "           1       0.00      0.00      0.00   2745783\n",
      "\n",
      "    accuracy                           0.94  46551445\n",
      "   macro avg       0.47      0.50      0.48  46551445\n",
      "weighted avg       0.89      0.94      0.91  46551445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "# --- Cargar el modelo entrenado ---\n",
    "print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_weighted_model.txt'\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# --- Cargar los datos de validación ---\n",
    "print(\"\\n--- Cargando datos de validación ---\")\n",
    "val_data_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data.pkl'\n",
    "val_df = pd.read_pickle(val_data_path)\n",
    "\n",
    "# --- Preprocesar datos ---\n",
    "print(\"\\n--- Preprocesando datos ---\")\n",
    "val_df['date'] = pd.to_datetime(val_df['date']).astype(int) / 10**9\n",
    "val_df['timestamp_local'] = pd.to_datetime(val_df['timestamp_local']).astype(int) / 10**9\n",
    "val_df['hour'] = pd.to_datetime(val_df['timestamp_local'], unit='s').dt.hour\n",
    "val_df['day_of_week'] = pd.to_datetime(val_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "\n",
    "# Separar las características y etiquetas\n",
    "X_val = val_df.drop(['add_to_cart', 'session_id'], axis=1)\n",
    "y_val = val_df['add_to_cart']\n",
    "\n",
    "# Predecir las probabilidades de las etiquetas positivas\n",
    "print(\"\\n--- Generando predicciones ---\")\n",
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# --- Calcular métricas ---\n",
    "print(\"\\n--- Calculando métricas ---\")\n",
    "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# --- Mostrar resultados ---\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# --- Informe de clasificación ---\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_val, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cargando el modelo entrenado ---\n",
      "\n",
      "--- Cargando el conjunto de prueba ---\n",
      "Columnas en test_df: Index(['session_id', 'date', 'timestamp_local', 'user_id', 'country',\n",
      "       'partnumber', 'device_type', 'pagetype'],\n",
      "      dtype='object')\n",
      "\n",
      "--- Calculando productos populares ---\n",
      "\n",
      "--- Preprocesando `test_df` ---\n",
      "Columnas procesadas en test_df: Index(['session_id', 'date', 'timestamp_local', 'user_id', 'country',\n",
      "       'partnumber', 'device_type', 'pagetype', 'hour', 'day_of_week'],\n",
      "      dtype='object')\n",
      "\n",
      "--- Generando predicciones ---\n",
      "\n",
      "--- Guardando el archivo predictions_3.json ---\n",
      "Archivo predictions_3.json generado con éxito en /home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_weighted.json.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "\n",
    "# --- Generar predicciones y guardar en JSON ---\n",
    "def json_generator(model_path, output_path):\n",
    "    print(\"\\n--- Cargando el modelo entrenado ---\")\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "    # --- Cargar el conjunto de prueba ---\n",
    "    print(\"\\n--- Cargando el conjunto de prueba ---\")\n",
    "    test_df = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_data.pkl')\n",
    "    print(\"Columnas en test_df:\", test_df.columns)\n",
    "\n",
    "    # --- Obtener productos populares ---\n",
    "    print(\"\\n--- Calculando productos populares ---\")\n",
    "    popular_products = test_df['partnumber'].value_counts().index.tolist()\n",
    "\n",
    "    # --- Preprocesar el conjunto de prueba ---\n",
    "    def preprocess_test_data(test_df):\n",
    "        print(\"\\n--- Preprocesando `test_df` ---\")\n",
    "        test_df['date'] = pd.to_datetime(test_df['date']).astype(int) / 10**9\n",
    "        test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local']).astype(int) / 10**9\n",
    "        test_df['hour'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.hour\n",
    "        test_df['day_of_week'] = pd.to_datetime(test_df['timestamp_local'], unit='s').dt.dayofweek\n",
    "        feature_columns = [\n",
    "            'session_id', 'date', 'timestamp_local', 'user_id', 'country',\n",
    "            'partnumber', 'device_type', 'pagetype', 'hour', 'day_of_week'\n",
    "        ]\n",
    "        return test_df[feature_columns]\n",
    "\n",
    "    test_df = preprocess_test_data(test_df)\n",
    "    print(\"Columnas procesadas en test_df:\", test_df.columns)\n",
    "\n",
    "    # --- Generar predicciones para cada session_id ---\n",
    "    print(\"\\n--- Generando predicciones ---\")\n",
    "    session_ids = test_df['session_id'].unique()\n",
    "    predictions = {}\n",
    "\n",
    "    for session_id in session_ids:\n",
    "        session_data = test_df[test_df['session_id'] == session_id].copy()\n",
    "\n",
    "        if session_data.empty:\n",
    "            predictions[str(session_id)] = []\n",
    "            continue\n",
    "\n",
    "        session_features = session_data.drop(['session_id'], axis=1)\n",
    "\n",
    "        if session_features.shape[1] != model.num_feature():\n",
    "            raise ValueError(f\"El número de características no coincide con el modelo. \"\n",
    "                             f\"Esperado: {model.num_feature()}, Actual: {session_features.shape[1]}\")\n",
    "\n",
    "        session_data['score'] = model.predict(session_features)\n",
    "\n",
    "        recommended_products = (\n",
    "            session_data.sort_values(by='score', ascending=False)['partnumber']\n",
    "            .drop_duplicates()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # Completar con productos populares si faltan\n",
    "        for product in popular_products:\n",
    "            if len(recommended_products) >= 5:\n",
    "                break\n",
    "            if product not in recommended_products:\n",
    "                recommended_products.append(product)\n",
    "\n",
    "        # Garantizar exactamente 5 valores únicos\n",
    "        predictions[str(session_id)] = recommended_products[:5]\n",
    "\n",
    "    # --- Guardar el archivo predictions_3.json ---\n",
    "    print(\"\\n--- Guardando el archivo predictions_3.json ---\")\n",
    "    output = {\"target\": predictions}\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(output, f, indent=4)\n",
    "\n",
    "    print(f\"Archivo predictions_3.json generado con éxito en {output_path}.\")\n",
    "\n",
    "# Parámetros de entrada y salida\n",
    "model_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/models/lambdamart_weighted_model.txt'\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3_weighted.json'\n",
    "\n",
    "# Generar JSON\n",
    "json_generator(model_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
