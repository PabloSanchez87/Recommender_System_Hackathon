{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plan para Entrenar el Modelo de Recomendación**\n",
    "\n",
    "## **1. Estrategia del Modelo**\n",
    "Según el contexto, el modelo debe:\n",
    "- Recomendar 5 productos por sesión (`session_id`).\n",
    "- Considerar usuarios logueados y no logueados.\n",
    "- Incorporar datos de usuarios, productos e interacciones.\n",
    "\n",
    "Podemos optar por un enfoque híbrido:\n",
    "1. **Colaborativo:** Basado en interacciones previas entre usuarios y productos.\n",
    "2. **Basado en contenido:** Usando características como embeddings, clústeres y categorías.\n",
    "3. **Popularidad:** Para usuarios nuevos o sin historial.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Flujo del Entrenamiento**\n",
    "1. **Preparar los Datos:**\n",
    "   - Dividir `train_data` en entrenamiento y prueba.\n",
    "   - Asegurar que cada sesión tenga 5 recomendaciones.\n",
    "\n",
    "2. **Entrenamiento del Modelo:**\n",
    "   - Construir un pipeline que combine los enfoques colaborativo, basado en contenido y de popularidad.\n",
    "   - Usar embeddings (`reduced_embedding`) y datos de usuarios (`RFM_score`, `user_class`).\n",
    "\n",
    "3. **Evaluación del Modelo:**\n",
    "   - Usar la métrica **NDCG** para medir la calidad de las recomendaciones.\n",
    "\n",
    "4. **Predicción:**\n",
    "   - Generar recomendaciones para todas las sesiones en el conjunto de prueba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso 1: Preparar los Datos**\n",
    "\n",
    "#### **1. Dividir `train_data`**\n",
    "Dividiremos el dataset en conjuntos de **entrenamiento** y **prueba**:\n",
    "- **Entrenamiento:** 80% de las sesiones.\n",
    "- **Prueba:** 20% de las sesiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sesiones de entrenamiento: 3659004\n",
      "Total sesiones de prueba: 914752\n",
      "Interacciones en conjunto de entrenamiento: 37217837\n",
      "Interacciones en conjunto de prueba: 9333608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset enriquecido\n",
    "train_data_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_data_enriched.pkl'\n",
    "train_data = pd.read_pickle(train_data_path)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "train_sessions, test_sessions = train_test_split(\n",
    "    train_data['session_id'].unique(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Filtrar las interacciones de entrenamiento y prueba\n",
    "train_set = train_data[train_data['session_id'].isin(train_sessions)]\n",
    "test_set = train_data[train_data['session_id'].isin(test_sessions)]\n",
    "\n",
    "# Verificar tamaños\n",
    "print(f\"Total sesiones de entrenamiento: {len(train_sessions)}\")\n",
    "print(f\"Total sesiones de prueba: {len(test_sessions)}\")\n",
    "print(f\"Interacciones en conjunto de entrenamiento: {len(train_set)}\")\n",
    "print(f\"Interacciones en conjunto de prueba: {len(test_set)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso 2: Verificar la División**\n",
    "Revisaremos las distribuciones de las columnas clave (`add_to_cart`, `user_id`, `partnumber`) para asegurarnos de que las sesiones están bien distribuidas entre los conjuntos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_to_cart\n",
      "0    0.941039\n",
      "1    0.058961\n",
      "Name: proportion, dtype: float64\n",
      "user_id\n",
      "-1         0.852830\n",
      " 436999    0.000092\n",
      " 304018    0.000065\n",
      " 88890     0.000057\n",
      " 31534     0.000051\n",
      "Name: proportion, dtype: float64\n",
      "add_to_cart\n",
      "0    0.940926\n",
      "1    0.059074\n",
      "Name: proportion, dtype: float64\n",
      "user_id\n",
      "-1         0.852214\n",
      " 184051    0.000222\n",
      " 540020    0.000145\n",
      " 43208     0.000142\n",
      " 124268    0.000138\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Distribución en el conjunto de entrenamiento\n",
    "print(train_set['add_to_cart'].value_counts(normalize=True))\n",
    "print(train_set['user_id'].value_counts(normalize=True).head())\n",
    "\n",
    "# Distribución en el conjunto de prueba\n",
    "print(test_set['add_to_cart'].value_counts(normalize=True))\n",
    "print(test_set['user_id'].value_counts(normalize=True).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Revisión de los Datos**\n",
    "\n",
    "#### **1. División en Conjuntos**\n",
    "- **Sesiones:**\n",
    "  - Entrenamiento: **3,659,004** sesiones.\n",
    "  - Prueba: **914,752** sesiones.\n",
    "- **Interacciones:**\n",
    "  - Entrenamiento: **37,217,837** interacciones.\n",
    "  - Prueba: **9,333,608** interacciones.\n",
    "\n",
    "La proporción entre los conjuntos de entrenamiento (80%) y prueba (20%) está correctamente balanceada.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Distribución de `add_to_cart`**\n",
    "- **Entrenamiento:** \n",
    "  - No añadido al carrito (`0`): **94.10%**.\n",
    "  - Añadido al carrito (`1`): **5.90%**.\n",
    "- **Prueba:**\n",
    "  - No añadido al carrito (`0`): **94.09%**.\n",
    "  - Añadido al carrito (`1`): **5.91%**.\n",
    "\n",
    "La distribución de la variable objetivo `add_to_cart` es consistente entre los conjuntos de entrenamiento y prueba.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Distribución de `user_id`**\n",
    "- La mayoría de las interacciones son de usuarios no logueados (`user_id = -1`), lo cual es consistente con el contexto del problema:\n",
    "  - **Entrenamiento:** 85.28% de las interacciones.\n",
    "  - **Prueba:** 85.22% de las interacciones.\n",
    "\n",
    "Los usuarios logueados representan una minoría, pero las proporciones son similares en ambos conjuntos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso 3: Guardar los conjuntos de entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en formato Pickle\n",
    "train_set.to_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_set.pkl')\n",
    "test_set.to_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_set.pkl')\n",
    "\n",
    "# Guardar en formato CSV\n",
    "train_set.to_csv('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_set.csv', index=False)\n",
    "test_set.to_csv('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/test_set.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan para el Modelo Híbrido\n",
    "\n",
    "## 1. Componentes del Modelo\n",
    "\n",
    "El modelo incluirá tres enfoques complementarios:\n",
    "\n",
    "### Basado en Popularidad:\n",
    "- Recomendaciones de los productos más populares (por ejemplo, en la misma categoría o clúster).\n",
    "- Ideal para usuarios no logueados o sin historial.\n",
    "\n",
    "### Basado en Contenido:\n",
    "- Utiliza las características del producto (`reduced_embedding`, `cluster`, `discount_category`, etc.) y del usuario (`RFM_score`, `region`, etc.).\n",
    "- Perfecto para recomendaciones personalizadas según preferencias.\n",
    "\n",
    "### Filtrado Colaborativo:\n",
    "- Basado en patrones de interacción entre usuarios y productos (`add_to_cart`).\n",
    "- Útil para capturar tendencias entre usuarios con comportamientos similares.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Arquitectura del Pipeline\n",
    "\n",
    "### Input:\n",
    "- **Usuario:**\n",
    "  - `user_id`, `RFM_score`, etc.\n",
    "- **Productos:**\n",
    "  - `partnumber`, `cluster`, `embedding`.\n",
    "- **Historial de interacciones:**\n",
    "  - `add_to_cart`, `session_id`.\n",
    "\n",
    "### Procesamiento:\n",
    "\n",
    "- **Para usuarios logueados:**\n",
    "  - Filtrado colaborativo + contenido + popularidad.\n",
    "- **Para usuarios no logueados:**\n",
    "  - Popularidad + contenido.\n",
    "\n",
    "### Output:\n",
    "- Recomendación de los 5 productos más relevantes por sesión.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Algoritmos Seleccionados\n",
    "\n",
    "### Basado en Popularidad:\n",
    "- Ranking simple por frecuencia (popularidad).\n",
    "\n",
    "### Basado en Contenido:\n",
    "- Similaridad coseno con embeddings (`reduced_embedding`).\n",
    "\n",
    "### Filtrado Colaborativo:\n",
    "- **Matrix Factorization (SVD):** Para capturar relaciones usuario-producto.\n",
    "- **Alternativa:** Modelos como **ALS (Alternating Least Squares)** si trabajamos con datos dispersos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1: Función para Popularidad**\n",
    "Esta función recomendará los productos más populares dentro de un clúster, categoría o en general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_popularity(train_data, cluster=None, top_n=5):\n",
    "    \"\"\"\n",
    "    Recomienda productos basados en popularidad.\n",
    "    Si se especifica un clúster, filtra por clúster; de lo contrario, usa la popularidad general.\n",
    "    \"\"\"\n",
    "    if cluster is not None:\n",
    "        # Filtrar por clúster\n",
    "        popular_products = (\n",
    "            train_data[train_data['cluster'] == cluster]\n",
    "            .groupby('partnumber')['popularity']\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "            .sort_values('popularity', ascending=False)\n",
    "        )\n",
    "    else:\n",
    "        # Popularidad general\n",
    "        popular_products = (\n",
    "            train_data.groupby('partnumber')['popularity']\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "            .sort_values('popularity', ascending=False)\n",
    "        )\n",
    "    \n",
    "    # Devolver los Top N productos\n",
    "    return popular_products['partnumber'].head(top_n).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2: Función para Similaridad Basada en Contenido**\n",
    "Esta función utiliza los embeddings de productos para calcular similitudes y recomendar productos similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def recommend_by_content(product_embeddings, target_product, top_n=5):\n",
    "    \"\"\"\n",
    "    Recomienda productos basados en similitud de embeddings.\n",
    "    \"\"\"\n",
    "    # Calcular similitudes\n",
    "    similarity_matrix = cosine_similarity(product_embeddings)\n",
    "    \n",
    "    # Buscar índice del producto objetivo\n",
    "    product_index = target_product\n",
    "    \n",
    "    # Obtener los índices más similares\n",
    "    similar_indices = np.argsort(similarity_matrix[product_index])[::-1][1:top_n + 1]\n",
    "    \n",
    "    # Devolver los productos similares\n",
    "    return similar_indices.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3: Filtrado Colaborativo**\n",
    "Esta función implementa un enfoque basado en factorización de matrices (SVD) para recomendaciones personalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Cargar train_data desde archivo guardado\n",
    "\n",
    "train_data_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/train_set.pkl'\n",
    "train_data = pd.read_pickle(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo con submuestra...\n",
      "Modelo entrenado.\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD, Dataset, Reader\n",
    "\n",
    "\n",
    "\n",
    "def train_collaborative_filtering(train_sample):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de filtrado colaborativo (SVD) utilizando una submuestra.\n",
    "    \"\"\"\n",
    "    # Preparar datos para Surprise\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    data = Dataset.load_from_df(\n",
    "        train_sample[['user_id', 'partnumber', 'add_to_cart']],\n",
    "        reader\n",
    "    )\n",
    "    trainset = data.build_full_trainset()\n",
    "\n",
    "    # Entrenar modelo SVD\n",
    "    model = SVD()\n",
    "    model.fit(trainset)\n",
    "    return model\n",
    "\n",
    "# Submuestrear el dataset (10% de las interacciones)\n",
    "train_sample = train_data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Entrenar el modelo con la submuestra\n",
    "print(\"Entrenando modelo con submuestra...\")\n",
    "cf_model = train_collaborative_filtering(train_sample)\n",
    "print(\"Modelo entrenado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_collaborative(model, user_id, product_list, top_n=5):\n",
    "    \"\"\"\n",
    "    Recomienda productos usando filtrado colaborativo entrenado.\n",
    "    \"\"\"\n",
    "    # Predecir puntuaciones para cada producto en la lista\n",
    "    predictions = [\n",
    "        (product, model.predict(user_id, product).est) for product in product_list\n",
    "    ]\n",
    "    # Ordenar productos por puntuación en orden descendente\n",
    "    predictions = sorted(predictions, key=lambda x: x[1], reverse=True)\n",
    "    # Retornar los Top N productos recomendados\n",
    "    return [pred[0] for pred in predictions[:top_n]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 4: Integrar el Pipeline Híbrido**\n",
    "- Una vez implementadas estas funciones, podemos integrarlas en un pipeline híbrido para combinar recomendaciones de popularidad, contenido y colaborativo.\n",
    "- Vamos a integrar las funciones en un pipeline híbrido que combine los enfoques de popularidad, contenido, y filtrado colaborativo. \n",
    "- Este pipeline priorizará las recomendaciones dependiendo de la información disponible para cada usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pipeline Híbrido**\n",
    "1. **Usuarios Logueados:**\n",
    "   - **Filtrado Colaborativo:** Si existe historial.\n",
    "   - **Basado en Contenido:** Si no hay historial suficiente.\n",
    "   - **Popularidad:** Para completar las recomendaciones.\n",
    "\n",
    "2. **Usuarios No Logueados:**\n",
    "   - **Basado en Popularidad:** Principal fuente de recomendaciones.\n",
    "   - **Basado en Contenido:** Como complemento, si hay información contextual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommender:\n",
    "    def __init__(self, train_data, products_data):\n",
    "        self.train_data = train_data\n",
    "        self.products_data = products_data\n",
    "        \n",
    "        # Entrenar modelo de filtrado colaborativo\n",
    "        print(\"Entrenando modelo de filtrado colaborativo...\")\n",
    "        self.cf_model = train_collaborative_filtering(train_data.sample(frac=0.1, random_state=42))\n",
    "        print(\"Modelo de filtrado colaborativo entrenado.\")\n",
    "        \n",
    "        # Obtener embeddings de productos\n",
    "        self.product_embeddings = np.stack(products_data['reduced_embedding'])\n",
    "    \n",
    "    def recommend(self, user_id, session_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Genera recomendaciones híbridas para un session_id.\n",
    "        \"\"\"\n",
    "        # Generar recomendaciones por usuario\n",
    "        if user_id != -1:\n",
    "            recommendations = recommend_by_collaborative(\n",
    "                self.cf_model,\n",
    "                user_id,\n",
    "                self.products_data['partnumber'].tolist(),\n",
    "                top_n=top_n\n",
    "            )\n",
    "        else:\n",
    "            # Usuario no logueado\n",
    "            recommendations = recommend_by_popularity(self.train_data, top_n=top_n)\n",
    "        \n",
    "        return recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probar el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   discount                                          embedding  partnumber  \\\n",
      "0         0  [-0.13401361, -0.1200429, -0.016117405, -0.167...      -32760   \n",
      "1         0  [-0.0949274, -0.107294075, -0.16559914, -0.174...      -24105   \n",
      "2         0  [-0.12904441, -0.07724628, -0.09799071, -0.164...      -26117   \n",
      "3         1  [-0.12783332, -0.133868, -0.10101265, -0.18888...      -29449   \n",
      "4         1  [-0.14092924, -0.1258284, -0.10809927, -0.1765...      -31404   \n",
      "\n",
      "   color_id  cod_section  family  popularity discount_category  \\\n",
      "0        85            4      73           0        Full Price   \n",
      "1       135            4      73           0        Full Price   \n",
      "2       339            4      73           0        Full Price   \n",
      "3       135            4      73           0        Discounted   \n",
      "4         3            4      73           0        Discounted   \n",
      "\n",
      "                                   reduced_embedding  cluster  \\\n",
      "0  [1.2056737, 4.4139566, 0.2004558, -1.7360026, ...       18   \n",
      "1  [0.7336297, 5.2660084, 0.45973969, -2.7015145,...       18   \n",
      "2  [1.5594732, 5.369463, 0.94879663, -1.6583909, ...       18   \n",
      "3  [0.96842283, 5.4791565, 0.29451263, -2.2336948...       18   \n",
      "4  [1.7068894, 5.317782, 0.5276894, -2.311784, -0...       18   \n",
      "\n",
      "                        similar_products  \n",
      "0    [14863, -30489, 9002, -26117, 9284]  \n",
      "1  [21839, -31616, 20083, 28665, -26548]  \n",
      "2       [12976, 6752, 20009, 9284, 7604]  \n",
      "3     [25108, 10165, -23872, 12219, 364]  \n",
      "4  [-27711, -29449, -27153, 12219, 3603]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo de products_data enriquecido\n",
    "products_data_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/products_data_enriched.pkl'\n",
    "\n",
    "# Cargar el dataset\n",
    "products_data = pd.read_pickle(products_data_path)\n",
    "\n",
    "# Verificar las primeras filas\n",
    "print(products_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomendaciones para usuario 436999: [25438, 25026, 14539, 6982, 20347]\n"
     ]
    }
   ],
   "source": [
    "# Lista de productos disponibles\n",
    "product_list = train_data['partnumber'].unique()\n",
    "\n",
    "# Probar recomendaciones para un usuario logueado\n",
    "user_id = 436999  # Usuario logueado de ejemplo\n",
    "recommendations = recommend_by_collaborative(cf_model, user_id, product_list, top_n=5)\n",
    "\n",
    "print(f\"Recomendaciones para usuario {user_id}: {recommendations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inicializar el pipeline híbrido\n",
    "# recommender = HybridRecommender(train_data, products_data)\n",
    "\n",
    "# # Probar recomendaciones para un usuario logueado\n",
    "# user_id = 436999  # Usuario logueado de ejemplo\n",
    "# print(\"Recomendaciones para usuario logueado:\")\n",
    "# print(recommender.recommend(user_id, top_n=5))\n",
    "\n",
    "# # Probar recomendaciones para un usuario no logueado\n",
    "# user_id = -1  # Usuario no logueado\n",
    "# print(\"\\nRecomendaciones para usuario no logueado:\")\n",
    "# print(recommender.recommend(user_id, top_n=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generar `predictions_3.json` -> 51447 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# # Inicializar el pipeline\n",
    "# recommender = HybridRecommender(train_data, products_data)\n",
    "\n",
    "# # Generar recomendaciones para todas las session_id\n",
    "# session_recommendations = {}\n",
    "\n",
    "# print(\"Generando recomendaciones para todas las session_id...\")\n",
    "\n",
    "# for session_id, group in test_set.groupby('session_id'):\n",
    "#     # Obtener el user_id asociado a la sesión\n",
    "#     user_id = group['user_id'].iloc[0]\n",
    "\n",
    "#     # Generar recomendaciones usando el pipeline híbrido\n",
    "#     recommendations = recommender.recommend(user_id, session_id, top_n=5)\n",
    "\n",
    "#     # Eliminar duplicados en las recomendaciones\n",
    "#     unique_recommendations = list(dict.fromkeys(recommendations))\n",
    "\n",
    "#     # Guardar recomendaciones para la sesión\n",
    "#     session_recommendations[str(session_id)] = unique_recommendations[:5]\n",
    "\n",
    "# # Verificar que todas las session_id estén cubiertas\n",
    "# assert len(session_recommendations) == test_set['session_id'].nunique(), (\n",
    "#     \"No todas las session_id tienen recomendaciones.\"\n",
    "# )\n",
    "\n",
    "# # Guardar en predictions_3.json con el formato correcto\n",
    "# output_data = {\"target\": session_recommendations}\n",
    "# output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3.json'\n",
    "# with open(output_path, 'w') as f:\n",
    "#     json.dump(output_data, f, indent=4)\n",
    "\n",
    "# print(f\"Archivo predictions_3.json generado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HybridRecommender' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsutil\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Inicializar el pipeline\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m recommender \u001b[38;5;241m=\u001b[39m \u001b[43mHybridRecommender\u001b[49m(train_data, products_data)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_sessions\u001b[39m(session_chunk, recommender):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Procesa un subconjunto de session_id y genera recomendaciones.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HybridRecommender' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "# Inicializar el pipeline\n",
    "recommender = HybridRecommender(train_data, products_data)\n",
    "\n",
    "def process_sessions(session_chunk, recommender):\n",
    "    \"\"\"\n",
    "    Procesa un subconjunto de session_id y genera recomendaciones.\n",
    "    \"\"\"\n",
    "    chunk_recommendations = {}\n",
    "    for session_id, group in session_chunk:\n",
    "        user_id = group['user_id'].iloc[0]\n",
    "        recommendations = recommender.recommend(user_id, session_id, top_n=5)\n",
    "        unique_recommendations = list(dict.fromkeys(recommendations))\n",
    "        chunk_recommendations[str(session_id)] = unique_recommendations[:5]\n",
    "    return chunk_recommendations\n",
    "\n",
    "# Dividir las sesiones en bloques\n",
    "def split_sessions(test_set, n_chunks):\n",
    "    \"\"\"\n",
    "    Divide las session_id en n_chunks.\n",
    "    \"\"\"\n",
    "    session_groups = list(test_set.groupby('session_id'))\n",
    "    chunk_size = max(len(session_groups) // n_chunks, 100)  # Bloques de al menos 100 sesiones\n",
    "    return [session_groups[i:i + chunk_size] for i in range(0, len(session_groups), chunk_size)]\n",
    "\n",
    "# Monitoreo del uso de CPU\n",
    "def monitor_cpu_usage(interval=5):\n",
    "    \"\"\"\n",
    "    Monitorea el uso del CPU y la memoria en intervalos de tiempo.\n",
    "    \"\"\"\n",
    "    print(\"Monitoreando uso del CPU y memoria...\")\n",
    "    while True:\n",
    "        cpu_usage = psutil.cpu_percent(interval=interval)\n",
    "        memory_usage = psutil.virtual_memory().percent\n",
    "        print(f\"Uso del CPU: {cpu_usage}% | Uso de Memoria: {memory_usage}%\")\n",
    "\n",
    "# Configurar paralelismo\n",
    "n_processes = mp.cpu_count()  # Usar todos los núcleos disponibles\n",
    "session_chunks = split_sessions(test_set, n_processes)\n",
    "\n",
    "print(f\"Dividiendo {len(test_set['session_id'].unique())} sesiones en {len(session_chunks)} bloques para procesamiento paralelo...\")\n",
    "\n",
    "# Procesar las sesiones en paralelo con monitoreo de progreso\n",
    "with mp.Pool(n_processes) as pool:\n",
    "    results = list(tqdm(pool.imap(partial(process_sessions, recommender=recommender), session_chunks), \n",
    "                        total=len(session_chunks), desc=\"Procesando bloques\"))\n",
    "\n",
    "# Combinar resultados\n",
    "session_recommendations = {k: v for chunk in results for k, v in chunk.items()}\n",
    "\n",
    "# Verificar que todas las session_id estén cubiertas\n",
    "assert len(session_recommendations) == test_set['session_id'].nunique(), (\n",
    "    \"No todas las session_id tienen recomendaciones.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en predictions_3.json con el formato correcto\n",
    "output_data = {\"target\": session_recommendations}\n",
    "output_path = '/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/predictions/predictions_3.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(f\"Archivo predictions_3.json generado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **1. Evaluar el Modelo de Filtrado Colaborativo**\n",
    "Podemos evaluar el modelo usando la métrica **NDCG** para medir la calidad de las recomendaciones en el conjunto de prueba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **2. Integrar Filtrado Colaborativo con Popularidad y Contenido**\n",
    "Ahora que tenemos un modelo funcional para filtrado colaborativo, podemos:\n",
    "- Combinarlo con las recomendaciones basadas en **popularidad** y **contenido**.\n",
    "- Probar el pipeline híbrido para usuarios logueados y no logueados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['discount', 'embedding', 'partnumber', 'color_id', 'cod_section',\n",
      "       'family'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "products_data = pd.read_pickle('/home/pablost/Hackathon_inditex_data_science/hackathon-inditex-data-recommender/data/processed/new_processed/products_data.pkl')\n",
    "\n",
    "print(products_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
